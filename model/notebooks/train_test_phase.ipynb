{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP06nDMbuk/HBklrqKEYpc0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/addinar/permafrost-modeling-convlstm/blob/main/model/notebooks/train_test_phase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Packages and Libraries**"
      ],
      "metadata": {
        "id": "tdu99N1VNCDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAzksnsrM8JA",
        "outputId": "ab00a20f-f468-4950-a016-18b233d2c4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Have `convlstm.py` and `dataset.py` loaded in colab files."
      ],
      "metadata": {
        "id": "lonxy5nINGIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content')"
      ],
      "metadata": {
        "id": "pHGh3yUFOMz7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "UtP6Ul_3PYDZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3b18WvROqi9",
        "outputId": "91af21bc-c5b2-4eb5-f30c-284fb815adb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Inspect DF**"
      ],
      "metadata": {
        "id": "3DbnOUThPsdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = userdata.get('preprocessed_path')\n",
        "df = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "oULEuyk_PKTd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN9oLVcEPouY",
        "outputId": "e952ab20-41a9-4965-a6c8-3ab19e8b1705"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'date', 'snow_albedo', 'snow_cover', 'snow_density',\n",
              "       'snow_depth', 'snowfall_sum', 'surface_latent_heat_flux_sum',\n",
              "       'surface_sensible_heat_flux_sum',\n",
              "       'surface_solar_radiation_downwards_sum',\n",
              "       'surface_thermal_radiation_downwards_sum', 'skin_temperature',\n",
              "       'total_precipitation_sum', 'avg_volumetric_water_content',\n",
              "       'TBFI_skin_temperature', 'TBFI_soil_temperature_level_1',\n",
              "       'TBFI_soil_temperature_level_2', 'TBFI_soil_temperature_level_3',\n",
              "       'band_1', 'band_2', 'band_3', 'band_4', 'band_5', 'band_6'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "DcS4anDpPzDO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRycbptvP7DO",
        "outputId": "5edd59de-d3e6-4c02-d8a8-c5ba14592b47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50262, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "1W2Nz0I5P8-p",
        "outputId": "058e5d1b-57c1-4df1-ef05-a07f67891455"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date                                       0\n",
              "snow_albedo                                0\n",
              "snow_cover                                 0\n",
              "snow_density                               0\n",
              "snow_depth                                 0\n",
              "snowfall_sum                               0\n",
              "surface_latent_heat_flux_sum               0\n",
              "surface_sensible_heat_flux_sum             0\n",
              "surface_solar_radiation_downwards_sum      0\n",
              "surface_thermal_radiation_downwards_sum    0\n",
              "skin_temperature                           0\n",
              "total_precipitation_sum                    0\n",
              "avg_volumetric_water_content               0\n",
              "TBFI_skin_temperature                      0\n",
              "TBFI_soil_temperature_level_1              0\n",
              "TBFI_soil_temperature_level_2              0\n",
              "TBFI_soil_temperature_level_3              0\n",
              "band_1                                     0\n",
              "band_2                                     0\n",
              "band_3                                     0\n",
              "band_4                                     0\n",
              "band_5                                     0\n",
              "band_6                                     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_albedo</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_cover</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_density</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_depth</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snowfall_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_latent_heat_flux_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_sensible_heat_flux_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_solar_radiation_downwards_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_thermal_radiation_downwards_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skin_temperature</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total_precipitation_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avg_volumetric_water_content</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_skin_temperature</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_soil_temperature_level_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_soil_temperature_level_2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_soil_temperature_level_3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add an additional col 'Year'."
      ],
      "metadata": {
        "id": "C-QCc2kfQTpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['Year'] = df['date'].dt.year"
      ],
      "metadata": {
        "id": "nAmaoRn_QXIu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Year'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_1N8tdDQrIM",
        "outputId": "1a32e822-9021-433a-f133-f6efe2cbd9ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
              "       2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022,\n",
              "       2023], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the size of each year\n",
        "band_1 = df[df['band_1'] == 1]\n",
        "band_2 = df[df['band_2'] == 1]\n",
        "band_3 = df[df['band_3'] == 1]\n",
        "band_4 = df[df['band_4'] == 1]\n",
        "band_5 = df[df['band_5'] == 1]\n",
        "band_6 = df[df['band_6'] == 1]"
      ],
      "metadata": {
        "id": "ieIMu-ldw0RM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bands = [band_1, band_2, band_3, band_4, band_5, band_6]\n",
        "days_in_year = set()\n",
        "for band in bands:\n",
        "  for year in band['Year'].unique():\n",
        "    size = band[band['Year'] == year].shape[0]\n",
        "    days_in_year.add(size)\n",
        "\n",
        "print(days_in_year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmTokVnxxClv",
        "outputId": "5cb8b5d5-3174-4ba6-ab62-6d7eb71e7420"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{364, 365}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we know that there is not much variation for number of days in a year. This will make sequencing more straightforward."
      ],
      "metadata": {
        "id": "-FjTs3ZIyIW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train-Test Split**"
      ],
      "metadata": {
        "id": "8L3dDralQApK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to be band-conscious during the split so band data does not leak.\n",
        "\n",
        "Create sliding-window sequences."
      ],
      "metadata": {
        "id": "7J6bJAi6QEq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "band_cols = ['band_1', 'band_2', 'band_3', 'band_4', 'band_5', 'band_6']"
      ],
      "metadata": {
        "id": "98cl5jLEQL0v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_list, y_train_list = [], []\n",
        "X_test_list, y_test_list = [], []"
      ],
      "metadata": {
        "id": "KJIXFjCiRAaz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_features = ['date', 'Year', 'TBFI_skin_temperature',\n",
        "                'TBFI_soil_temperature_level_1', 'TBFI_soil_temperature_level_2',\n",
        "                'TBFI_soil_temperature_level_3']\n",
        "\n",
        "targets = ['TBFI_skin_temperature', 'TBFI_soil_temperature_level_1',\n",
        "           'TBFI_soil_temperature_level_2', 'TBFI_soil_temperature_level_3']\n",
        "\n",
        "features = df.drop(columns=not_features, axis=1).columns"
      ],
      "metadata": {
        "id": "oP9p3se1Ruyf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2"
      ],
      "metadata": {
        "id": "gKgEt_2G72h3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in band_cols:\n",
        "  data = df[df[col] == 1].reset_index(drop=True) # filter by band\n",
        "  X_windows = np.lib.stride_tricks.sliding_window_view(data[features], (window_size, len(features)))\n",
        "  X_windows = X_windows.reshape(-1, window_size, len(features))\n",
        "\n",
        "  y_windows = data[targets].values[window_size:]\n",
        "\n",
        "  split_index = np.where(data['Year'].values[window_size:] <= 2018)[0]\n",
        "  test_index = np.where(data['Year'].values[window_size:] > 2018)[0]\n",
        "\n",
        "  X_train_list.append(X_windows[split_index])\n",
        "  y_train_list.append(y_windows[split_index])\n",
        "\n",
        "  X_test_list.append(X_windows[test_index])\n",
        "  y_test_list.append(y_windows[test_index])"
      ],
      "metadata": {
        "id": "LxorpZDORDQC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(np.concatenate(X_train_list), dtype=torch.float32)\n",
        "y_train = torch.tensor(np.concatenate(y_train_list), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate(X_test_list), dtype=torch.float32)\n",
        "y_test = torch.tensor(np.concatenate(y_test_list), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "uqrN1-d_SZLy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data Shapes:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing Data Shapes:\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngdKHahT8tmB",
        "outputId": "65f11658-62dc-4184-85bd-441de7b5d141"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Shapes: torch.Size([39324, 2, 18]) torch.Size([39324, 4])\n",
            "Testing Data Shapes: torch.Size([10926, 2, 18]) torch.Size([10926, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize Dataset and Dataloaders**"
      ],
      "metadata": {
        "id": "48N08OiTSliF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import PermafrostDataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "X6RX-Xo7SnNx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PermafrostDataset(X_train, y_train)\n",
        "test_dataset = PermafrostDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "SLVxLWBB9K-F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "ERTK-wrl9Z4y"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader) # num batches per epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0sblxNYZVJy",
        "outputId": "6bc62d93-6246-4544-daaf-2b91f6418003"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9831"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop**"
      ],
      "metadata": {
        "id": "Wfr3WHR1-KQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from convlstm import ConvLSTM\n",
        "from torch.nn import MSELoss\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "bzO492A6-Ued"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvLSTM(input_features=len(features))\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# old optimizer =  Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "loss_fn = MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)"
      ],
      "metadata": {
        "id": "XOvKyWrdZ_JG"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients(model, max_norm=1.0):\n",
        "    for param in model.parameters():\n",
        "        torch.nn.utils.clip_grad_norm_(param, max_norm)"
      ],
      "metadata": {
        "id": "01ARlw3BaQUP"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SioeDpSDe3Lh"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_losses = []\n",
        "\n",
        "def train_model(model, train_loader, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}\")\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = loss_fn(preds, y_batch)\n",
        "            loss.backward()\n",
        "\n",
        "            if batch_idx % 1000 == 0:\n",
        "              print(f\"\\n[Batch {batch_idx}] Gradient Norms:\")\n",
        "              for name, param in model.named_parameters():\n",
        "                  if param.grad is not None:\n",
        "                      print(f\"{name} grad norm: {param.grad.norm().item()}\")\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 1000 == 0:\n",
        "                print(f\"[Batch {batch_idx}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Total Epoch Loss: {epoch_loss:.4f}\")\n",
        "        epoch_losses.append(epoch_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(epoch_losses, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kDD76jRkdGeK"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D9fwM_xYaUGQ",
        "outputId": "55dac59e-c550-4f2e-b253-a54732b87826"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "h0 grad norm: 0.026819352060556412\n",
            "c0 grad norm: 0.029844041913747787\n",
            "conv1.weight grad norm: 0.5429704189300537\n",
            "conv1.bias grad norm: 1.1783522602115681e-08\n",
            "batch_norm.weight grad norm: 0.0625041201710701\n",
            "batch_norm.bias grad norm: 0.0533536821603775\n",
            "lstm.weight_ih_l0 grad norm: 0.3180775046348572\n",
            "lstm.weight_hh_l0 grad norm: 0.13201627135276794\n",
            "lstm.bias_ih_l0 grad norm: 0.03465455397963524\n",
            "lstm.bias_hh_l0 grad norm: 0.03465455397963524\n",
            "fc.weight grad norm: 0.8212843537330627\n",
            "fc.bias grad norm: 0.2831479609012604\n",
            "[Batch 3000] Loss: 0.3376\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.038437627255916595\n",
            "c0 grad norm: 0.03731486573815346\n",
            "conv1.weight grad norm: 0.6882894039154053\n",
            "conv1.bias grad norm: 1.0163770269855377e-08\n",
            "batch_norm.weight grad norm: 0.06616795808076859\n",
            "batch_norm.bias grad norm: 0.055367689579725266\n",
            "lstm.weight_ih_l0 grad norm: 0.31011390686035156\n",
            "lstm.weight_hh_l0 grad norm: 0.09454593807458878\n",
            "lstm.bias_ih_l0 grad norm: 0.03282354027032852\n",
            "lstm.bias_hh_l0 grad norm: 0.03282354027032852\n",
            "fc.weight grad norm: 0.6624428629875183\n",
            "fc.bias grad norm: 0.06536537408828735\n",
            "[Batch 4000] Loss: 0.1099\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.025988254696130753\n",
            "c0 grad norm: 0.024447482079267502\n",
            "conv1.weight grad norm: 0.6192134022712708\n",
            "conv1.bias grad norm: 9.287454361128766e-09\n",
            "batch_norm.weight grad norm: 0.050611577928066254\n",
            "batch_norm.bias grad norm: 0.04610959067940712\n",
            "lstm.weight_ih_l0 grad norm: 0.24785800278186798\n",
            "lstm.weight_hh_l0 grad norm: 0.08583026379346848\n",
            "lstm.bias_ih_l0 grad norm: 0.027451852336525917\n",
            "lstm.bias_hh_l0 grad norm: 0.027451852336525917\n",
            "fc.weight grad norm: 0.2546772062778473\n",
            "fc.bias grad norm: 0.12104864418506622\n",
            "[Batch 5000] Loss: 0.0576\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.010992355644702911\n",
            "c0 grad norm: 0.006135676987469196\n",
            "conv1.weight grad norm: 0.17205052077770233\n",
            "conv1.bias grad norm: 1.7733806645381378e-09\n",
            "batch_norm.weight grad norm: 0.021212304010987282\n",
            "batch_norm.bias grad norm: 0.011293203569948673\n",
            "lstm.weight_ih_l0 grad norm: 0.11442214250564575\n",
            "lstm.weight_hh_l0 grad norm: 0.027907956391572952\n",
            "lstm.bias_ih_l0 grad norm: 0.009330335073173046\n",
            "lstm.bias_hh_l0 grad norm: 0.009330335073173046\n",
            "fc.weight grad norm: 0.1361796259880066\n",
            "fc.bias grad norm: 0.012859608046710491\n",
            "[Batch 6000] Loss: 0.0274\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.017067277804017067\n",
            "c0 grad norm: 0.014360564760863781\n",
            "conv1.weight grad norm: 0.15088613331317902\n",
            "conv1.bias grad norm: 2.7930089352423693e-09\n",
            "batch_norm.weight grad norm: 0.019627617672085762\n",
            "batch_norm.bias grad norm: 0.013804060406982899\n",
            "lstm.weight_ih_l0 grad norm: 0.09853334724903107\n",
            "lstm.weight_hh_l0 grad norm: 0.0269212257117033\n",
            "lstm.bias_ih_l0 grad norm: 0.010747001506388187\n",
            "lstm.bias_hh_l0 grad norm: 0.010747001506388187\n",
            "fc.weight grad norm: 0.30014315247535706\n",
            "fc.bias grad norm: 0.07734885066747665\n",
            "[Batch 7000] Loss: 0.0260\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.02014024741947651\n",
            "c0 grad norm: 0.014986232854425907\n",
            "conv1.weight grad norm: 0.3274884819984436\n",
            "conv1.bias grad norm: 8.997706579805254e-09\n",
            "batch_norm.weight grad norm: 0.03699297457933426\n",
            "batch_norm.bias grad norm: 0.023938767611980438\n",
            "lstm.weight_ih_l0 grad norm: 0.17362719774246216\n",
            "lstm.weight_hh_l0 grad norm: 0.05628867447376251\n",
            "lstm.bias_ih_l0 grad norm: 0.01764020137488842\n",
            "lstm.bias_hh_l0 grad norm: 0.01764020137488842\n",
            "fc.weight grad norm: 0.37066957354545593\n",
            "fc.bias grad norm: 0.017477089539170265\n",
            "[Batch 8000] Loss: 0.0608\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.019908692687749863\n",
            "c0 grad norm: 0.015348846092820168\n",
            "conv1.weight grad norm: 0.20587500929832458\n",
            "conv1.bias grad norm: 6.432022914282243e-09\n",
            "batch_norm.weight grad norm: 0.023799559101462364\n",
            "batch_norm.bias grad norm: 0.01974373124539852\n",
            "lstm.weight_ih_l0 grad norm: 0.12135429680347443\n",
            "lstm.weight_hh_l0 grad norm: 0.048421915620565414\n",
            "lstm.bias_ih_l0 grad norm: 0.01321340911090374\n",
            "lstm.bias_hh_l0 grad norm: 0.01321340911090374\n",
            "fc.weight grad norm: 0.5940030217170715\n",
            "fc.bias grad norm: 0.24208731949329376\n",
            "[Batch 9000] Loss: 0.0808\n",
            "Total Epoch Loss: 883.4208\n",
            "\n",
            "Epoch 69\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.007132561411708593\n",
            "c0 grad norm: 0.005624465178698301\n",
            "conv1.weight grad norm: 0.11348661035299301\n",
            "conv1.bias grad norm: 2.159832757087088e-09\n",
            "batch_norm.weight grad norm: 0.010914169251918793\n",
            "batch_norm.bias grad norm: 0.016474727541208267\n",
            "lstm.weight_ih_l0 grad norm: 0.06961555033922195\n",
            "lstm.weight_hh_l0 grad norm: 0.02318437583744526\n",
            "lstm.bias_ih_l0 grad norm: 0.007948845624923706\n",
            "lstm.bias_hh_l0 grad norm: 0.007948845624923706\n",
            "fc.weight grad norm: 0.19144675135612488\n",
            "fc.bias grad norm: 0.09746948629617691\n",
            "[Batch 0] Loss: 0.0151\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.03876494988799095\n",
            "c0 grad norm: 0.027417166158556938\n",
            "conv1.weight grad norm: 0.5709087252616882\n",
            "conv1.bias grad norm: 1.0072850109565934e-08\n",
            "batch_norm.weight grad norm: 0.06499315053224564\n",
            "batch_norm.bias grad norm: 0.0570668950676918\n",
            "lstm.weight_ih_l0 grad norm: 0.30665266513824463\n",
            "lstm.weight_hh_l0 grad norm: 0.07890443503856659\n",
            "lstm.bias_ih_l0 grad norm: 0.03216784447431564\n",
            "lstm.bias_hh_l0 grad norm: 0.03216784447431564\n",
            "fc.weight grad norm: 0.3924180865287781\n",
            "fc.bias grad norm: 0.06247898191213608\n",
            "[Batch 1000] Loss: 0.0353\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.02617097645998001\n",
            "c0 grad norm: 0.02003888599574566\n",
            "conv1.weight grad norm: 0.3361435830593109\n",
            "conv1.bias grad norm: 4.133807518513777e-09\n",
            "batch_norm.weight grad norm: 0.03910544142127037\n",
            "batch_norm.bias grad norm: 0.029908353462815285\n",
            "lstm.weight_ih_l0 grad norm: 0.21118506789207458\n",
            "lstm.weight_hh_l0 grad norm: 0.06851562112569809\n",
            "lstm.bias_ih_l0 grad norm: 0.019525466486811638\n",
            "lstm.bias_hh_l0 grad norm: 0.019525466486811638\n",
            "fc.weight grad norm: 0.3058379292488098\n",
            "fc.bias grad norm: 0.044666413217782974\n",
            "[Batch 2000] Loss: 0.0667\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.0344051830470562\n",
            "c0 grad norm: 0.032081831246614456\n",
            "conv1.weight grad norm: 0.38967224955558777\n",
            "conv1.bias grad norm: 5.550933934728164e-09\n",
            "batch_norm.weight grad norm: 0.0657651349902153\n",
            "batch_norm.bias grad norm: 0.047215770930051804\n",
            "lstm.weight_ih_l0 grad norm: 0.3686754107475281\n",
            "lstm.weight_hh_l0 grad norm: 0.08734722435474396\n",
            "lstm.bias_ih_l0 grad norm: 0.0325826071202755\n",
            "lstm.bias_hh_l0 grad norm: 0.0325826071202755\n",
            "fc.weight grad norm: 0.5878579020500183\n",
            "fc.bias grad norm: 0.2568117678165436\n",
            "[Batch 3000] Loss: 0.1987\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.0408780612051487\n",
            "c0 grad norm: 0.029124082997441292\n",
            "conv1.weight grad norm: 0.39736321568489075\n",
            "conv1.bias grad norm: 7.818712788321136e-09\n",
            "batch_norm.weight grad norm: 0.04575563967227936\n",
            "batch_norm.bias grad norm: 0.04850936308503151\n",
            "lstm.weight_ih_l0 grad norm: 0.24578779935836792\n",
            "lstm.weight_hh_l0 grad norm: 0.095066137611866\n",
            "lstm.bias_ih_l0 grad norm: 0.031517479568719864\n",
            "lstm.bias_hh_l0 grad norm: 0.031517479568719864\n",
            "fc.weight grad norm: 0.38840779662132263\n",
            "fc.bias grad norm: 0.15494368970394135\n",
            "[Batch 4000] Loss: 0.0927\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.027499549090862274\n",
            "c0 grad norm: 0.019241103902459145\n",
            "conv1.weight grad norm: 0.30250445008277893\n",
            "conv1.bias grad norm: 4.804027842197911e-09\n",
            "batch_norm.weight grad norm: 0.03007856011390686\n",
            "batch_norm.bias grad norm: 0.029155917465686798\n",
            "lstm.weight_ih_l0 grad norm: 0.18222576379776\n",
            "lstm.weight_hh_l0 grad norm: 0.054024260491132736\n",
            "lstm.bias_ih_l0 grad norm: 0.018451238051056862\n",
            "lstm.bias_hh_l0 grad norm: 0.018451238051056862\n",
            "fc.weight grad norm: 0.3879435956478119\n",
            "fc.bias grad norm: 0.023070205003023148\n",
            "[Batch 5000] Loss: 0.0364\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.010851148515939713\n",
            "c0 grad norm: 0.005458860658109188\n",
            "conv1.weight grad norm: 0.11022601276636124\n",
            "conv1.bias grad norm: 2.4770128170814587e-09\n",
            "batch_norm.weight grad norm: 0.014822910539805889\n",
            "batch_norm.bias grad norm: 0.011796054430305958\n",
            "lstm.weight_ih_l0 grad norm: 0.07969197630882263\n",
            "lstm.weight_hh_l0 grad norm: 0.03143803030252457\n",
            "lstm.bias_ih_l0 grad norm: 0.00973422545939684\n",
            "lstm.bias_hh_l0 grad norm: 0.00973422545939684\n",
            "fc.weight grad norm: 0.12717650830745697\n",
            "fc.bias grad norm: 0.038381434977054596\n",
            "[Batch 6000] Loss: 0.0084\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.034740958362817764\n",
            "c0 grad norm: 0.012130284681916237\n",
            "conv1.weight grad norm: 0.4537348747253418\n",
            "conv1.bias grad norm: 4.745409842854542e-09\n",
            "batch_norm.weight grad norm: 0.0341835580766201\n",
            "batch_norm.bias grad norm: 0.034356120973825455\n",
            "lstm.weight_ih_l0 grad norm: 0.16061674058437347\n",
            "lstm.weight_hh_l0 grad norm: 0.05551606044173241\n",
            "lstm.bias_ih_l0 grad norm: 0.018086593598127365\n",
            "lstm.bias_hh_l0 grad norm: 0.018086593598127365\n",
            "fc.weight grad norm: 0.2614494264125824\n",
            "fc.bias grad norm: 0.09476263076066971\n",
            "[Batch 7000] Loss: 0.0377\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.008015725761651993\n",
            "c0 grad norm: 0.010408385656774044\n",
            "conv1.weight grad norm: 0.09546594321727753\n",
            "conv1.bias grad norm: 2.165830625955323e-09\n",
            "batch_norm.weight grad norm: 0.01387063879519701\n",
            "batch_norm.bias grad norm: 0.008709860034286976\n",
            "lstm.weight_ih_l0 grad norm: 0.07956134527921677\n",
            "lstm.weight_hh_l0 grad norm: 0.030694060027599335\n",
            "lstm.bias_ih_l0 grad norm: 0.007236594799906015\n",
            "lstm.bias_hh_l0 grad norm: 0.007236594799906015\n",
            "fc.weight grad norm: 0.2891353964805603\n",
            "fc.bias grad norm: 0.13858889043331146\n",
            "[Batch 8000] Loss: 0.0379\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.02471184730529785\n",
            "c0 grad norm: 0.024688368663191795\n",
            "conv1.weight grad norm: 0.5512505769729614\n",
            "conv1.bias grad norm: 8.700342668532812e-09\n",
            "batch_norm.weight grad norm: 0.05175693705677986\n",
            "batch_norm.bias grad norm: 0.038602687418460846\n",
            "lstm.weight_ih_l0 grad norm: 0.2346949279308319\n",
            "lstm.weight_hh_l0 grad norm: 0.05440937727689743\n",
            "lstm.bias_ih_l0 grad norm: 0.02150265872478485\n",
            "lstm.bias_hh_l0 grad norm: 0.02150265872478485\n",
            "fc.weight grad norm: 0.2397194355726242\n",
            "fc.bias grad norm: 0.028444435447454453\n",
            "[Batch 9000] Loss: 0.0271\n",
            "Total Epoch Loss: 877.6497\n",
            "\n",
            "Epoch 70\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.06746567040681839\n",
            "c0 grad norm: 0.03676649183034897\n",
            "conv1.weight grad norm: 0.5702404379844666\n",
            "conv1.bias grad norm: 1.022154716423529e-08\n",
            "batch_norm.weight grad norm: 0.06416712701320648\n",
            "batch_norm.bias grad norm: 0.060318224132061005\n",
            "lstm.weight_ih_l0 grad norm: 0.33271703124046326\n",
            "lstm.weight_hh_l0 grad norm: 0.11397786438465118\n",
            "lstm.bias_ih_l0 grad norm: 0.04243343695998192\n",
            "lstm.bias_hh_l0 grad norm: 0.04243343695998192\n",
            "fc.weight grad norm: 0.5044952034950256\n",
            "fc.bias grad norm: 0.2126208245754242\n",
            "[Batch 0] Loss: 0.1119\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.02332761511206627\n",
            "c0 grad norm: 0.025371504947543144\n",
            "conv1.weight grad norm: 0.6486169695854187\n",
            "conv1.bias grad norm: 1.3566202383685777e-08\n",
            "batch_norm.weight grad norm: 0.04617561772465706\n",
            "batch_norm.bias grad norm: 0.04503747448325157\n",
            "lstm.weight_ih_l0 grad norm: 0.26001474261283875\n",
            "lstm.weight_hh_l0 grad norm: 0.08894459158182144\n",
            "lstm.bias_ih_l0 grad norm: 0.024723729118704796\n",
            "lstm.bias_hh_l0 grad norm: 0.024723729118704796\n",
            "fc.weight grad norm: 0.3440571129322052\n",
            "fc.bias grad norm: 0.028388338163495064\n",
            "[Batch 1000] Loss: 0.0480\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.043760087341070175\n",
            "c0 grad norm: 0.01927441544830799\n",
            "conv1.weight grad norm: 0.40925130248069763\n",
            "conv1.bias grad norm: 9.344530482735536e-09\n",
            "batch_norm.weight grad norm: 0.043195322155952454\n",
            "batch_norm.bias grad norm: 0.0581648163497448\n",
            "lstm.weight_ih_l0 grad norm: 0.21991124749183655\n",
            "lstm.weight_hh_l0 grad norm: 0.08272427320480347\n",
            "lstm.bias_ih_l0 grad norm: 0.031500089913606644\n",
            "lstm.bias_hh_l0 grad norm: 0.031500089913606644\n",
            "fc.weight grad norm: 0.3529219627380371\n",
            "fc.bias grad norm: 0.11004548519849777\n",
            "[Batch 2000] Loss: 0.0670\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.0890599712729454\n",
            "c0 grad norm: 0.06028017774224281\n",
            "conv1.weight grad norm: 1.1651296615600586\n",
            "conv1.bias grad norm: 1.606594857150867e-08\n",
            "batch_norm.weight grad norm: 0.12460058927536011\n",
            "batch_norm.bias grad norm: 0.13598531484603882\n",
            "lstm.weight_ih_l0 grad norm: 0.5133146047592163\n",
            "lstm.weight_hh_l0 grad norm: 0.1505652368068695\n",
            "lstm.bias_ih_l0 grad norm: 0.0672716274857521\n",
            "lstm.bias_hh_l0 grad norm: 0.0672716274857521\n",
            "fc.weight grad norm: 0.610771119594574\n",
            "fc.bias grad norm: 0.24009470641613007\n",
            "[Batch 3000] Loss: 0.1796\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.00854396726936102\n",
            "c0 grad norm: 0.008748725987970829\n",
            "conv1.weight grad norm: 0.15050160884857178\n",
            "conv1.bias grad norm: 2.4273727472490236e-09\n",
            "batch_norm.weight grad norm: 0.016636956483125687\n",
            "batch_norm.bias grad norm: 0.016562318429350853\n",
            "lstm.weight_ih_l0 grad norm: 0.10475612431764603\n",
            "lstm.weight_hh_l0 grad norm: 0.031006336212158203\n",
            "lstm.bias_ih_l0 grad norm: 0.009435427375137806\n",
            "lstm.bias_hh_l0 grad norm: 0.009435427375137806\n",
            "fc.weight grad norm: 0.25122931599617004\n",
            "fc.bias grad norm: 0.10031440854072571\n",
            "[Batch 4000] Loss: 0.0190\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.04296192526817322\n",
            "c0 grad norm: 0.021800830960273743\n",
            "conv1.weight grad norm: 0.27456724643707275\n",
            "conv1.bias grad norm: 6.450974865401804e-09\n",
            "batch_norm.weight grad norm: 0.02984781004488468\n",
            "batch_norm.bias grad norm: 0.045887574553489685\n",
            "lstm.weight_ih_l0 grad norm: 0.20903828740119934\n",
            "lstm.weight_hh_l0 grad norm: 0.06300249695777893\n",
            "lstm.bias_ih_l0 grad norm: 0.026495778933167458\n",
            "lstm.bias_hh_l0 grad norm: 0.026495778933167458\n",
            "fc.weight grad norm: 0.32405924797058105\n",
            "fc.bias grad norm: 0.06387172639369965\n",
            "[Batch 5000] Loss: 0.0527\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.01804209128022194\n",
            "c0 grad norm: 0.026964901015162468\n",
            "conv1.weight grad norm: 0.3694961965084076\n",
            "conv1.bias grad norm: 9.103711562374883e-09\n",
            "batch_norm.weight grad norm: 0.0404299721121788\n",
            "batch_norm.bias grad norm: 0.026379236951470375\n",
            "lstm.weight_ih_l0 grad norm: 0.1574641466140747\n",
            "lstm.weight_hh_l0 grad norm: 0.04207273945212364\n",
            "lstm.bias_ih_l0 grad norm: 0.01421418134123087\n",
            "lstm.bias_hh_l0 grad norm: 0.01421418134123087\n",
            "fc.weight grad norm: 0.2890532612800598\n",
            "fc.bias grad norm: 0.042831044644117355\n",
            "[Batch 6000] Loss: 0.0331\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.04143407568335533\n",
            "c0 grad norm: 0.03974393010139465\n",
            "conv1.weight grad norm: 0.5248708128929138\n",
            "conv1.bias grad norm: 1.4677816295716184e-08\n",
            "batch_norm.weight grad norm: 0.06308560073375702\n",
            "batch_norm.bias grad norm: 0.049645472317934036\n",
            "lstm.weight_ih_l0 grad norm: 0.30201268196105957\n",
            "lstm.weight_hh_l0 grad norm: 0.1023029163479805\n",
            "lstm.bias_ih_l0 grad norm: 0.033506013453006744\n",
            "lstm.bias_hh_l0 grad norm: 0.033506013453006744\n",
            "fc.weight grad norm: 0.44306275248527527\n",
            "fc.bias grad norm: 0.1341417282819748\n",
            "[Batch 7000] Loss: 0.1004\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.007896956987679005\n",
            "c0 grad norm: 0.010198114439845085\n",
            "conv1.weight grad norm: 0.23893985152244568\n",
            "conv1.bias grad norm: 2.3061714760075347e-09\n",
            "batch_norm.weight grad norm: 0.02209063246846199\n",
            "batch_norm.bias grad norm: 0.016604512929916382\n",
            "lstm.weight_ih_l0 grad norm: 0.10326866060495377\n",
            "lstm.weight_hh_l0 grad norm: 0.03388196602463722\n",
            "lstm.bias_ih_l0 grad norm: 0.009805405512452126\n",
            "lstm.bias_hh_l0 grad norm: 0.009805405512452126\n",
            "fc.weight grad norm: 0.3314579427242279\n",
            "fc.bias grad norm: 0.06886278092861176\n",
            "[Batch 8000] Loss: 0.0422\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.014828902669250965\n",
            "c0 grad norm: 0.012767293490469456\n",
            "conv1.weight grad norm: 0.15862274169921875\n",
            "conv1.bias grad norm: 2.4689863487026287e-09\n",
            "batch_norm.weight grad norm: 0.021326815709471703\n",
            "batch_norm.bias grad norm: 0.022868966683745384\n",
            "lstm.weight_ih_l0 grad norm: 0.13193228840827942\n",
            "lstm.weight_hh_l0 grad norm: 0.05474534258246422\n",
            "lstm.bias_ih_l0 grad norm: 0.01707177609205246\n",
            "lstm.bias_hh_l0 grad norm: 0.01707177609205246\n",
            "fc.weight grad norm: 0.3104008138179779\n",
            "fc.bias grad norm: 0.0711049735546112\n",
            "[Batch 9000] Loss: 0.0466\n",
            "Total Epoch Loss: 877.4414\n",
            "\n",
            "Epoch 71\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.00864329468458891\n",
            "c0 grad norm: 0.010255158878862858\n",
            "conv1.weight grad norm: 0.128182515501976\n",
            "conv1.bias grad norm: 2.55869680998444e-09\n",
            "batch_norm.weight grad norm: 0.018023986369371414\n",
            "batch_norm.bias grad norm: 0.011893386952579021\n",
            "lstm.weight_ih_l0 grad norm: 0.08461389690637589\n",
            "lstm.weight_hh_l0 grad norm: 0.028712477535009384\n",
            "lstm.bias_ih_l0 grad norm: 0.008382461965084076\n",
            "lstm.bias_hh_l0 grad norm: 0.008382461965084076\n",
            "fc.weight grad norm: 0.08604936301708221\n",
            "fc.bias grad norm: 0.017371945083141327\n",
            "[Batch 0] Loss: 0.0129\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.021999111399054527\n",
            "c0 grad norm: 0.017033599317073822\n",
            "conv1.weight grad norm: 0.3765060603618622\n",
            "conv1.bias grad norm: 6.335342472851835e-09\n",
            "batch_norm.weight grad norm: 0.04710647091269493\n",
            "batch_norm.bias grad norm: 0.03868802636861801\n",
            "lstm.weight_ih_l0 grad norm: 0.21295399963855743\n",
            "lstm.weight_hh_l0 grad norm: 0.056173354387283325\n",
            "lstm.bias_ih_l0 grad norm: 0.01598777435719967\n",
            "lstm.bias_hh_l0 grad norm: 0.01598777435719967\n",
            "fc.weight grad norm: 0.27189913392066956\n",
            "fc.bias grad norm: 0.08239348232746124\n",
            "[Batch 1000] Loss: 0.0483\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.05285618081688881\n",
            "c0 grad norm: 0.025021519511938095\n",
            "conv1.weight grad norm: 0.46755602955818176\n",
            "conv1.bias grad norm: 8.627010217310271e-09\n",
            "batch_norm.weight grad norm: 0.0687284842133522\n",
            "batch_norm.bias grad norm: 0.05488261207938194\n",
            "lstm.weight_ih_l0 grad norm: 0.3085806369781494\n",
            "lstm.weight_hh_l0 grad norm: 0.0775405690073967\n",
            "lstm.bias_ih_l0 grad norm: 0.028132418170571327\n",
            "lstm.bias_hh_l0 grad norm: 0.028132418170571327\n",
            "fc.weight grad norm: 0.5086560845375061\n",
            "fc.bias grad norm: 0.24657848477363586\n",
            "[Batch 2000] Loss: 0.1135\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.07640445232391357\n",
            "c0 grad norm: 0.041713591665029526\n",
            "conv1.weight grad norm: 0.4771738350391388\n",
            "conv1.bias grad norm: 8.092206904564136e-09\n",
            "batch_norm.weight grad norm: 0.046947769820690155\n",
            "batch_norm.bias grad norm: 0.07587075233459473\n",
            "lstm.weight_ih_l0 grad norm: 0.2817355692386627\n",
            "lstm.weight_hh_l0 grad norm: 0.09100896120071411\n",
            "lstm.bias_ih_l0 grad norm: 0.04287493973970413\n",
            "lstm.bias_hh_l0 grad norm: 0.04287493973970413\n",
            "fc.weight grad norm: 0.4517519772052765\n",
            "fc.bias grad norm: 0.06035599485039711\n",
            "[Batch 3000] Loss: 0.1082\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.02048834040760994\n",
            "c0 grad norm: 0.00805592443794012\n",
            "conv1.weight grad norm: 0.1669262796640396\n",
            "conv1.bias grad norm: 3.167658135794227e-09\n",
            "batch_norm.weight grad norm: 0.020412681624293327\n",
            "batch_norm.bias grad norm: 0.02421794831752777\n",
            "lstm.weight_ih_l0 grad norm: 0.13390116393566132\n",
            "lstm.weight_hh_l0 grad norm: 0.03684696927666664\n",
            "lstm.bias_ih_l0 grad norm: 0.01300757471472025\n",
            "lstm.bias_hh_l0 grad norm: 0.01300757471472025\n",
            "fc.weight grad norm: 0.3451070189476013\n",
            "fc.bias grad norm: 0.07555937767028809\n",
            "[Batch 4000] Loss: 0.0408\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.14256222546100616\n",
            "c0 grad norm: 0.04589295759797096\n",
            "conv1.weight grad norm: 0.935367226600647\n",
            "conv1.bias grad norm: 1.3880693927603716e-08\n",
            "batch_norm.weight grad norm: 0.12835723161697388\n",
            "batch_norm.bias grad norm: 0.09841480106115341\n",
            "lstm.weight_ih_l0 grad norm: 0.5956697463989258\n",
            "lstm.weight_hh_l0 grad norm: 0.13217858970165253\n",
            "lstm.bias_ih_l0 grad norm: 0.05661383271217346\n",
            "lstm.bias_hh_l0 grad norm: 0.05661383271217346\n",
            "fc.weight grad norm: 0.5969757437705994\n",
            "fc.bias grad norm: 0.16325253248214722\n",
            "[Batch 5000] Loss: 0.1420\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.007824632339179516\n",
            "c0 grad norm: 0.0048974137753248215\n",
            "conv1.weight grad norm: 0.0808187872171402\n",
            "conv1.bias grad norm: 1.4514897062056775e-09\n",
            "batch_norm.weight grad norm: 0.008851606398820877\n",
            "batch_norm.bias grad norm: 0.00830309558659792\n",
            "lstm.weight_ih_l0 grad norm: 0.05680542439222336\n",
            "lstm.weight_hh_l0 grad norm: 0.017587153241038322\n",
            "lstm.bias_ih_l0 grad norm: 0.006452334113419056\n",
            "lstm.bias_hh_l0 grad norm: 0.006452334113419056\n",
            "fc.weight grad norm: 0.11412254720926285\n",
            "fc.bias grad norm: 0.04179415479302406\n",
            "[Batch 6000] Loss: 0.0059\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.01906522549688816\n",
            "c0 grad norm: 0.018099293112754822\n",
            "conv1.weight grad norm: 0.3145190477371216\n",
            "conv1.bias grad norm: 4.337406434018476e-09\n",
            "batch_norm.weight grad norm: 0.028206756338477135\n",
            "batch_norm.bias grad norm: 0.01904206909239292\n",
            "lstm.weight_ih_l0 grad norm: 0.13507868349552155\n",
            "lstm.weight_hh_l0 grad norm: 0.045029476284980774\n",
            "lstm.bias_ih_l0 grad norm: 0.013385252095758915\n",
            "lstm.bias_hh_l0 grad norm: 0.013385252095758915\n",
            "fc.weight grad norm: 0.2247418612241745\n",
            "fc.bias grad norm: 0.04245590418577194\n",
            "[Batch 7000] Loss: 0.0333\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.036209218204021454\n",
            "c0 grad norm: 0.042351134121418\n",
            "conv1.weight grad norm: 0.5379933714866638\n",
            "conv1.bias grad norm: 1.18809113658358e-08\n",
            "batch_norm.weight grad norm: 0.05381868779659271\n",
            "batch_norm.bias grad norm: 0.06172588840126991\n",
            "lstm.weight_ih_l0 grad norm: 0.3323848247528076\n",
            "lstm.weight_hh_l0 grad norm: 0.14192146062850952\n",
            "lstm.bias_ih_l0 grad norm: 0.03854570537805557\n",
            "lstm.bias_hh_l0 grad norm: 0.03854570537805557\n",
            "fc.weight grad norm: 0.5744336247444153\n",
            "fc.bias grad norm: 0.16216856241226196\n",
            "[Batch 8000] Loss: 0.1617\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.034524429589509964\n",
            "c0 grad norm: 0.019200801849365234\n",
            "conv1.weight grad norm: 0.20959749817848206\n",
            "conv1.bias grad norm: 7.842142935032825e-09\n",
            "batch_norm.weight grad norm: 0.03937350958585739\n",
            "batch_norm.bias grad norm: 0.028670409694314003\n",
            "lstm.weight_ih_l0 grad norm: 0.15270096063613892\n",
            "lstm.weight_hh_l0 grad norm: 0.03469451144337654\n",
            "lstm.bias_ih_l0 grad norm: 0.016206614673137665\n",
            "lstm.bias_hh_l0 grad norm: 0.016206614673137665\n",
            "fc.weight grad norm: 0.24953188002109528\n",
            "fc.bias grad norm: 0.10506035387516022\n",
            "[Batch 9000] Loss: 0.0205\n",
            "Total Epoch Loss: 880.6785\n",
            "\n",
            "Epoch 72\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.08274399489164352\n",
            "c0 grad norm: 0.06777080148458481\n",
            "conv1.weight grad norm: 1.4283198118209839\n",
            "conv1.bias grad norm: 2.9735375051131996e-08\n",
            "batch_norm.weight grad norm: 0.1019529476761818\n",
            "batch_norm.bias grad norm: 0.1781771034002304\n",
            "lstm.weight_ih_l0 grad norm: 0.7445864677429199\n",
            "lstm.weight_hh_l0 grad norm: 0.20905180275440216\n",
            "lstm.bias_ih_l0 grad norm: 0.07961607724428177\n",
            "lstm.bias_hh_l0 grad norm: 0.07961607724428177\n",
            "fc.weight grad norm: 0.5636723041534424\n",
            "fc.bias grad norm: 0.02924484945833683\n",
            "[Batch 0] Loss: 0.2474\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.05027985945343971\n",
            "c0 grad norm: 0.020035134628415108\n",
            "conv1.weight grad norm: 0.44126561284065247\n",
            "conv1.bias grad norm: 7.452328532053798e-09\n",
            "batch_norm.weight grad norm: 0.04668251797556877\n",
            "batch_norm.bias grad norm: 0.07443688064813614\n",
            "lstm.weight_ih_l0 grad norm: 0.1954028159379959\n",
            "lstm.weight_hh_l0 grad norm: 0.08233508467674255\n",
            "lstm.bias_ih_l0 grad norm: 0.03208640217781067\n",
            "lstm.bias_hh_l0 grad norm: 0.03208640217781067\n",
            "fc.weight grad norm: 0.34684932231903076\n",
            "fc.bias grad norm: 0.049426451325416565\n",
            "[Batch 1000] Loss: 0.0893\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.01125204749405384\n",
            "c0 grad norm: 0.007626524660736322\n",
            "conv1.weight grad norm: 0.122999407351017\n",
            "conv1.bias grad norm: 1.980051900218882e-09\n",
            "batch_norm.weight grad norm: 0.011679984629154205\n",
            "batch_norm.bias grad norm: 0.010323659516870975\n",
            "lstm.weight_ih_l0 grad norm: 0.06719431281089783\n",
            "lstm.weight_hh_l0 grad norm: 0.02337224967777729\n",
            "lstm.bias_ih_l0 grad norm: 0.0082060806453228\n",
            "lstm.bias_hh_l0 grad norm: 0.0082060806453228\n",
            "fc.weight grad norm: 0.12728282809257507\n",
            "fc.bias grad norm: 0.0741775706410408\n",
            "[Batch 2000] Loss: 0.0089\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.03162530064582825\n",
            "c0 grad norm: 0.030339868739247322\n",
            "conv1.weight grad norm: 0.34044575691223145\n",
            "conv1.bias grad norm: 5.476807007909201e-09\n",
            "batch_norm.weight grad norm: 0.041737984865903854\n",
            "batch_norm.bias grad norm: 0.05124244838953018\n",
            "lstm.weight_ih_l0 grad norm: 0.2877362072467804\n",
            "lstm.weight_hh_l0 grad norm: 0.08241315931081772\n",
            "lstm.bias_ih_l0 grad norm: 0.025545546784996986\n",
            "lstm.bias_hh_l0 grad norm: 0.025545546784996986\n",
            "fc.weight grad norm: 0.40226075053215027\n",
            "fc.bias grad norm: 0.08143670111894608\n",
            "[Batch 3000] Loss: 0.0605\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.07754556089639664\n",
            "c0 grad norm: 0.05961104482412338\n",
            "conv1.weight grad norm: 0.9531519412994385\n",
            "conv1.bias grad norm: 3.084086230842331e-08\n",
            "batch_norm.weight grad norm: 0.10764879733324051\n",
            "batch_norm.bias grad norm: 0.15276998281478882\n",
            "lstm.weight_ih_l0 grad norm: 0.6836040019989014\n",
            "lstm.weight_hh_l0 grad norm: 0.2710321247577667\n",
            "lstm.bias_ih_l0 grad norm: 0.07877682894468307\n",
            "lstm.bias_hh_l0 grad norm: 0.07877682894468307\n",
            "fc.weight grad norm: 0.5086909532546997\n",
            "fc.bias grad norm: 0.1387537121772766\n",
            "[Batch 4000] Loss: 0.2293\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.0856039822101593\n",
            "c0 grad norm: 0.05132385715842247\n",
            "conv1.weight grad norm: 2.038360118865967\n",
            "conv1.bias grad norm: 2.0381671461677797e-08\n",
            "batch_norm.weight grad norm: 0.18116094172000885\n",
            "batch_norm.bias grad norm: 0.10439052432775497\n",
            "lstm.weight_ih_l0 grad norm: 0.6519996523857117\n",
            "lstm.weight_hh_l0 grad norm: 0.15017637610435486\n",
            "lstm.bias_ih_l0 grad norm: 0.06440258026123047\n",
            "lstm.bias_hh_l0 grad norm: 0.06440258026123047\n",
            "fc.weight grad norm: 0.41357290744781494\n",
            "fc.bias grad norm: 0.05259156599640846\n",
            "[Batch 5000] Loss: 0.1689\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.017765115946531296\n",
            "c0 grad norm: 0.014993436634540558\n",
            "conv1.weight grad norm: 0.20310448110103607\n",
            "conv1.bias grad norm: 2.7411255487663766e-09\n",
            "batch_norm.weight grad norm: 0.0228658989071846\n",
            "batch_norm.bias grad norm: 0.019673967733979225\n",
            "lstm.weight_ih_l0 grad norm: 0.1305411010980606\n",
            "lstm.weight_hh_l0 grad norm: 0.03471686691045761\n",
            "lstm.bias_ih_l0 grad norm: 0.012520764954388142\n",
            "lstm.bias_hh_l0 grad norm: 0.012520764954388142\n",
            "fc.weight grad norm: 0.2221403568983078\n",
            "fc.bias grad norm: 0.08455650508403778\n",
            "[Batch 6000] Loss: 0.0280\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.01783149130642414\n",
            "c0 grad norm: 0.011392702348530293\n",
            "conv1.weight grad norm: 0.2422102838754654\n",
            "conv1.bias grad norm: 4.8464103841183714e-09\n",
            "batch_norm.weight grad norm: 0.03230808675289154\n",
            "batch_norm.bias grad norm: 0.02873644419014454\n",
            "lstm.weight_ih_l0 grad norm: 0.17421723902225494\n",
            "lstm.weight_hh_l0 grad norm: 0.04486410319805145\n",
            "lstm.bias_ih_l0 grad norm: 0.016918208450078964\n",
            "lstm.bias_hh_l0 grad norm: 0.016918208450078964\n",
            "fc.weight grad norm: 0.3327541649341583\n",
            "fc.bias grad norm: 0.08786330372095108\n",
            "[Batch 7000] Loss: 0.0351\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.017190786078572273\n",
            "c0 grad norm: 0.01264024805277586\n",
            "conv1.weight grad norm: 0.298440545797348\n",
            "conv1.bias grad norm: 9.097036901550837e-09\n",
            "batch_norm.weight grad norm: 0.033297162503004074\n",
            "batch_norm.bias grad norm: 0.024223968386650085\n",
            "lstm.weight_ih_l0 grad norm: 0.21953395009040833\n",
            "lstm.weight_hh_l0 grad norm: 0.06422581523656845\n",
            "lstm.bias_ih_l0 grad norm: 0.01846238411962986\n",
            "lstm.bias_hh_l0 grad norm: 0.01846238411962986\n",
            "fc.weight grad norm: 0.2768732011318207\n",
            "fc.bias grad norm: 0.11695575714111328\n",
            "[Batch 8000] Loss: 0.0879\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.01417833473533392\n",
            "c0 grad norm: 0.010055236518383026\n",
            "conv1.weight grad norm: 0.19884628057479858\n",
            "conv1.bias grad norm: 3.68530295169478e-09\n",
            "batch_norm.weight grad norm: 0.02138739824295044\n",
            "batch_norm.bias grad norm: 0.01448783464729786\n",
            "lstm.weight_ih_l0 grad norm: 0.10157176852226257\n",
            "lstm.weight_hh_l0 grad norm: 0.025797614827752113\n",
            "lstm.bias_ih_l0 grad norm: 0.00930568017065525\n",
            "lstm.bias_hh_l0 grad norm: 0.00930568017065525\n",
            "fc.weight grad norm: 0.20700418949127197\n",
            "fc.bias grad norm: 0.06410067528486252\n",
            "[Batch 9000] Loss: 0.0227\n",
            "Total Epoch Loss: 879.8956\n",
            "\n",
            "Epoch 73\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03414178639650345\n",
            "c0 grad norm: 0.026591654866933823\n",
            "conv1.weight grad norm: 0.3721536099910736\n",
            "conv1.bias grad norm: 1.1176060077389138e-08\n",
            "batch_norm.weight grad norm: 0.05936452001333237\n",
            "batch_norm.bias grad norm: 0.05137740820646286\n",
            "lstm.weight_ih_l0 grad norm: 0.3087635934352875\n",
            "lstm.weight_hh_l0 grad norm: 0.09380687773227692\n",
            "lstm.bias_ih_l0 grad norm: 0.029114460572600365\n",
            "lstm.bias_hh_l0 grad norm: 0.029114460572600365\n",
            "fc.weight grad norm: 0.39363300800323486\n",
            "fc.bias grad norm: 0.08791840076446533\n",
            "[Batch 0] Loss: 0.1278\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.010623211041092873\n",
            "c0 grad norm: 0.009134461171925068\n",
            "conv1.weight grad norm: 0.13121439516544342\n",
            "conv1.bias grad norm: 3.375400403626827e-09\n",
            "batch_norm.weight grad norm: 0.016228750348091125\n",
            "batch_norm.bias grad norm: 0.010036986321210861\n",
            "lstm.weight_ih_l0 grad norm: 0.09527898579835892\n",
            "lstm.weight_hh_l0 grad norm: 0.03161965683102608\n",
            "lstm.bias_ih_l0 grad norm: 0.009469518437981606\n",
            "lstm.bias_hh_l0 grad norm: 0.009469518437981606\n",
            "fc.weight grad norm: 0.20186389982700348\n",
            "fc.bias grad norm: 0.031683340668678284\n",
            "[Batch 1000] Loss: 0.0298\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.022803770378232002\n",
            "c0 grad norm: 0.01943838968873024\n",
            "conv1.weight grad norm: 0.26898127794265747\n",
            "conv1.bias grad norm: 6.721082801419698e-09\n",
            "batch_norm.weight grad norm: 0.03229554370045662\n",
            "batch_norm.bias grad norm: 0.02022169902920723\n",
            "lstm.weight_ih_l0 grad norm: 0.14691126346588135\n",
            "lstm.weight_hh_l0 grad norm: 0.04161845147609711\n",
            "lstm.bias_ih_l0 grad norm: 0.013330717571079731\n",
            "lstm.bias_hh_l0 grad norm: 0.013330717571079731\n",
            "fc.weight grad norm: 0.254503458738327\n",
            "fc.bias grad norm: 0.08699106425046921\n",
            "[Batch 2000] Loss: 0.0225\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.013828831724822521\n",
            "c0 grad norm: 0.008590904995799065\n",
            "conv1.weight grad norm: 0.14033684134483337\n",
            "conv1.bias grad norm: 2.2146187106386606e-09\n",
            "batch_norm.weight grad norm: 0.013459543697535992\n",
            "batch_norm.bias grad norm: 0.014728719368577003\n",
            "lstm.weight_ih_l0 grad norm: 0.07923471182584763\n",
            "lstm.weight_hh_l0 grad norm: 0.022286001592874527\n",
            "lstm.bias_ih_l0 grad norm: 0.007472196128219366\n",
            "lstm.bias_hh_l0 grad norm: 0.007472196128219366\n",
            "fc.weight grad norm: 0.18914836645126343\n",
            "fc.bias grad norm: 0.04313156381249428\n",
            "[Batch 3000] Loss: 0.0168\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.020778529345989227\n",
            "c0 grad norm: 0.02844257466495037\n",
            "conv1.weight grad norm: 0.40221476554870605\n",
            "conv1.bias grad norm: 7.535001955716325e-09\n",
            "batch_norm.weight grad norm: 0.04339286684989929\n",
            "batch_norm.bias grad norm: 0.029664691537618637\n",
            "lstm.weight_ih_l0 grad norm: 0.2158292531967163\n",
            "lstm.weight_hh_l0 grad norm: 0.07874087244272232\n",
            "lstm.bias_ih_l0 grad norm: 0.021415144205093384\n",
            "lstm.bias_hh_l0 grad norm: 0.021415144205093384\n",
            "fc.weight grad norm: 0.45855972170829773\n",
            "fc.bias grad norm: 0.20879831910133362\n",
            "[Batch 4000] Loss: 0.0681\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.013592037372291088\n",
            "c0 grad norm: 0.011177809908986092\n",
            "conv1.weight grad norm: 0.256173312664032\n",
            "conv1.bias grad norm: 4.3564285512331935e-09\n",
            "batch_norm.weight grad norm: 0.022469960153102875\n",
            "batch_norm.bias grad norm: 0.018224474042654037\n",
            "lstm.weight_ih_l0 grad norm: 0.1301083266735077\n",
            "lstm.weight_hh_l0 grad norm: 0.03709844499826431\n",
            "lstm.bias_ih_l0 grad norm: 0.013072214089334011\n",
            "lstm.bias_hh_l0 grad norm: 0.013072214089334011\n",
            "fc.weight grad norm: 0.08061621338129044\n",
            "fc.bias grad norm: 0.004328939598053694\n",
            "[Batch 5000] Loss: 0.0099\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.035597898066043854\n",
            "c0 grad norm: 0.024262869730591774\n",
            "conv1.weight grad norm: 0.32319727540016174\n",
            "conv1.bias grad norm: 8.791178451872383e-09\n",
            "batch_norm.weight grad norm: 0.035187721252441406\n",
            "batch_norm.bias grad norm: 0.036737654358148575\n",
            "lstm.weight_ih_l0 grad norm: 0.20090055465698242\n",
            "lstm.weight_hh_l0 grad norm: 0.08336709439754486\n",
            "lstm.bias_ih_l0 grad norm: 0.024233372882008553\n",
            "lstm.bias_hh_l0 grad norm: 0.024233372882008553\n",
            "fc.weight grad norm: 0.4137551784515381\n",
            "fc.bias grad norm: 0.190359428524971\n",
            "[Batch 6000] Loss: 0.0640\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.024700505658984184\n",
            "c0 grad norm: 0.01724039763212204\n",
            "conv1.weight grad norm: 0.31735196709632874\n",
            "conv1.bias grad norm: 5.676404679633151e-09\n",
            "batch_norm.weight grad norm: 0.036712273955345154\n",
            "batch_norm.bias grad norm: 0.031996771693229675\n",
            "lstm.weight_ih_l0 grad norm: 0.1799117773771286\n",
            "lstm.weight_hh_l0 grad norm: 0.04810461029410362\n",
            "lstm.bias_ih_l0 grad norm: 0.018133193254470825\n",
            "lstm.bias_hh_l0 grad norm: 0.018133193254470825\n",
            "fc.weight grad norm: 0.28475242853164673\n",
            "fc.bias grad norm: 0.08183391392230988\n",
            "[Batch 7000] Loss: 0.0537\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.024037176743149757\n",
            "c0 grad norm: 0.015676843002438545\n",
            "conv1.weight grad norm: 0.409178227186203\n",
            "conv1.bias grad norm: 4.4358738904293205e-09\n",
            "batch_norm.weight grad norm: 0.029806561768054962\n",
            "batch_norm.bias grad norm: 0.021207867190241814\n",
            "lstm.weight_ih_l0 grad norm: 0.17704375088214874\n",
            "lstm.weight_hh_l0 grad norm: 0.04725628346204758\n",
            "lstm.bias_ih_l0 grad norm: 0.01493128202855587\n",
            "lstm.bias_hh_l0 grad norm: 0.01493128202855587\n",
            "fc.weight grad norm: 0.4236483573913574\n",
            "fc.bias grad norm: 0.19104981422424316\n",
            "[Batch 8000] Loss: 0.0551\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.01761019602417946\n",
            "c0 grad norm: 0.018245667219161987\n",
            "conv1.weight grad norm: 0.2541309893131256\n",
            "conv1.bias grad norm: 4.8138035779743404e-09\n",
            "batch_norm.weight grad norm: 0.04304033890366554\n",
            "batch_norm.bias grad norm: 0.04108763113617897\n",
            "lstm.weight_ih_l0 grad norm: 0.24926096200942993\n",
            "lstm.weight_hh_l0 grad norm: 0.10717198252677917\n",
            "lstm.bias_ih_l0 grad norm: 0.024947289377450943\n",
            "lstm.bias_hh_l0 grad norm: 0.024947289377450943\n",
            "fc.weight grad norm: 0.2460004985332489\n",
            "fc.bias grad norm: 0.05248827859759331\n",
            "[Batch 9000] Loss: 0.0611\n",
            "Total Epoch Loss: 883.6303\n",
            "\n",
            "Epoch 74\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.012926604598760605\n",
            "c0 grad norm: 0.008440239354968071\n",
            "conv1.weight grad norm: 0.12363819032907486\n",
            "conv1.bias grad norm: 1.9445949295260334e-09\n",
            "batch_norm.weight grad norm: 0.0151340551674366\n",
            "batch_norm.bias grad norm: 0.010288133285939693\n",
            "lstm.weight_ih_l0 grad norm: 0.10807063430547714\n",
            "lstm.weight_hh_l0 grad norm: 0.04021589830517769\n",
            "lstm.bias_ih_l0 grad norm: 0.01085983868688345\n",
            "lstm.bias_hh_l0 grad norm: 0.01085983868688345\n",
            "fc.weight grad norm: 0.17622800171375275\n",
            "fc.bias grad norm: 0.059375569224357605\n",
            "[Batch 0] Loss: 0.0293\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.010678430087864399\n",
            "c0 grad norm: 0.009528580121695995\n",
            "conv1.weight grad norm: 0.2014314830303192\n",
            "conv1.bias grad norm: 2.3015449546193167e-09\n",
            "batch_norm.weight grad norm: 0.020303351804614067\n",
            "batch_norm.bias grad norm: 0.01375587284564972\n",
            "lstm.weight_ih_l0 grad norm: 0.10549755394458771\n",
            "lstm.weight_hh_l0 grad norm: 0.02754487469792366\n",
            "lstm.bias_ih_l0 grad norm: 0.008754140697419643\n",
            "lstm.bias_hh_l0 grad norm: 0.008754140697419643\n",
            "fc.weight grad norm: 0.21378837525844574\n",
            "fc.bias grad norm: 0.08951280266046524\n",
            "[Batch 1000] Loss: 0.0150\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.01216290146112442\n",
            "c0 grad norm: 0.010073494166135788\n",
            "conv1.weight grad norm: 0.1523425430059433\n",
            "conv1.bias grad norm: 2.989906322525826e-09\n",
            "batch_norm.weight grad norm: 0.01613548956811428\n",
            "batch_norm.bias grad norm: 0.02113366685807705\n",
            "lstm.weight_ih_l0 grad norm: 0.08456229418516159\n",
            "lstm.weight_hh_l0 grad norm: 0.032461293041706085\n",
            "lstm.bias_ih_l0 grad norm: 0.009752182289958\n",
            "lstm.bias_hh_l0 grad norm: 0.009752182289958\n",
            "fc.weight grad norm: 0.1045704185962677\n",
            "fc.bias grad norm: 0.032252710312604904\n",
            "[Batch 2000] Loss: 0.0058\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.11972375959157944\n",
            "c0 grad norm: 0.07656095176935196\n",
            "conv1.weight grad norm: 0.9101091027259827\n",
            "conv1.bias grad norm: 2.2123524345829537e-08\n",
            "batch_norm.weight grad norm: 0.12841513752937317\n",
            "batch_norm.bias grad norm: 0.13401179015636444\n",
            "lstm.weight_ih_l0 grad norm: 0.7800512909889221\n",
            "lstm.weight_hh_l0 grad norm: 0.255176842212677\n",
            "lstm.bias_ih_l0 grad norm: 0.07350016385316849\n",
            "lstm.bias_hh_l0 grad norm: 0.07350016385316849\n",
            "fc.weight grad norm: 0.74518883228302\n",
            "fc.bias grad norm: 0.1980910301208496\n",
            "[Batch 3000] Loss: 0.3038\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.03267619386315346\n",
            "c0 grad norm: 0.009512118063867092\n",
            "conv1.weight grad norm: 0.27479591965675354\n",
            "conv1.bias grad norm: 5.364886312975159e-09\n",
            "batch_norm.weight grad norm: 0.026255665346980095\n",
            "batch_norm.bias grad norm: 0.036178670823574066\n",
            "lstm.weight_ih_l0 grad norm: 0.1421978920698166\n",
            "lstm.weight_hh_l0 grad norm: 0.05261366814374924\n",
            "lstm.bias_ih_l0 grad norm: 0.02340707927942276\n",
            "lstm.bias_hh_l0 grad norm: 0.02340707927942276\n",
            "fc.weight grad norm: 0.19285278022289276\n",
            "fc.bias grad norm: 0.04405589401721954\n",
            "[Batch 4000] Loss: 0.0328\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.02689787745475769\n",
            "c0 grad norm: 0.017747066915035248\n",
            "conv1.weight grad norm: 0.36629748344421387\n",
            "conv1.bias grad norm: 5.443011374950402e-09\n",
            "batch_norm.weight grad norm: 0.03502798080444336\n",
            "batch_norm.bias grad norm: 0.03159549459815025\n",
            "lstm.weight_ih_l0 grad norm: 0.15845027565956116\n",
            "lstm.weight_hh_l0 grad norm: 0.04773574694991112\n",
            "lstm.bias_ih_l0 grad norm: 0.016747944056987762\n",
            "lstm.bias_hh_l0 grad norm: 0.016747944056987762\n",
            "fc.weight grad norm: 0.28719815611839294\n",
            "fc.bias grad norm: 0.11274099349975586\n",
            "[Batch 5000] Loss: 0.0483\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.020062103867530823\n",
            "c0 grad norm: 0.03137646242976189\n",
            "conv1.weight grad norm: 0.4698781371116638\n",
            "conv1.bias grad norm: 8.278058238886388e-09\n",
            "batch_norm.weight grad norm: 0.0348535031080246\n",
            "batch_norm.bias grad norm: 0.047186899930238724\n",
            "lstm.weight_ih_l0 grad norm: 0.22785726189613342\n",
            "lstm.weight_hh_l0 grad norm: 0.08200028538703918\n",
            "lstm.bias_ih_l0 grad norm: 0.02512366510927677\n",
            "lstm.bias_hh_l0 grad norm: 0.02512366510927677\n",
            "fc.weight grad norm: 0.3375198245048523\n",
            "fc.bias grad norm: 0.15250632166862488\n",
            "[Batch 6000] Loss: 0.0625\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.00896544847637415\n",
            "c0 grad norm: 0.008304123766720295\n",
            "conv1.weight grad norm: 0.1886274367570877\n",
            "conv1.bias grad norm: 2.796028075735535e-09\n",
            "batch_norm.weight grad norm: 0.016873672604560852\n",
            "batch_norm.bias grad norm: 0.01601104810833931\n",
            "lstm.weight_ih_l0 grad norm: 0.0786047875881195\n",
            "lstm.weight_hh_l0 grad norm: 0.02959032543003559\n",
            "lstm.bias_ih_l0 grad norm: 0.008856987580657005\n",
            "lstm.bias_hh_l0 grad norm: 0.008856987580657005\n",
            "fc.weight grad norm: 0.18868917226791382\n",
            "fc.bias grad norm: 0.10574132949113846\n",
            "[Batch 7000] Loss: 0.0139\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.007136194035410881\n",
            "c0 grad norm: 0.009575135074555874\n",
            "conv1.weight grad norm: 0.3059482276439667\n",
            "conv1.bias grad norm: 4.109585560740925e-09\n",
            "batch_norm.weight grad norm: 0.017820822075009346\n",
            "batch_norm.bias grad norm: 0.01532664429396391\n",
            "lstm.weight_ih_l0 grad norm: 0.13047774136066437\n",
            "lstm.weight_hh_l0 grad norm: 0.04281435161828995\n",
            "lstm.bias_ih_l0 grad norm: 0.012203472666442394\n",
            "lstm.bias_hh_l0 grad norm: 0.012203472666442394\n",
            "fc.weight grad norm: 0.18059344589710236\n",
            "fc.bias grad norm: 0.05541222169995308\n",
            "[Batch 8000] Loss: 0.0276\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.042705848813056946\n",
            "c0 grad norm: 0.03217419236898422\n",
            "conv1.weight grad norm: 0.6794652938842773\n",
            "conv1.bias grad norm: 1.1069894334525543e-08\n",
            "batch_norm.weight grad norm: 0.08327212929725647\n",
            "batch_norm.bias grad norm: 0.03856148198246956\n",
            "lstm.weight_ih_l0 grad norm: 0.4307187497615814\n",
            "lstm.weight_hh_l0 grad norm: 0.08364745229482651\n",
            "lstm.bias_ih_l0 grad norm: 0.03295130282640457\n",
            "lstm.bias_hh_l0 grad norm: 0.03295130282640457\n",
            "fc.weight grad norm: 0.4832867980003357\n",
            "fc.bias grad norm: 0.18785256147384644\n",
            "[Batch 9000] Loss: 0.1250\n",
            "Total Epoch Loss: 877.0772\n",
            "\n",
            "Epoch 75\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.02632998302578926\n",
            "c0 grad norm: 0.016816573217511177\n",
            "conv1.weight grad norm: 0.2706491947174072\n",
            "conv1.bias grad norm: 5.5645119623193295e-09\n",
            "batch_norm.weight grad norm: 0.02925366722047329\n",
            "batch_norm.bias grad norm: 0.02409970946609974\n",
            "lstm.weight_ih_l0 grad norm: 0.1622980833053589\n",
            "lstm.weight_hh_l0 grad norm: 0.04463093727827072\n",
            "lstm.bias_ih_l0 grad norm: 0.016788847744464874\n",
            "lstm.bias_hh_l0 grad norm: 0.016788847744464874\n",
            "fc.weight grad norm: 0.3106153905391693\n",
            "fc.bias grad norm: 0.07584208250045776\n",
            "[Batch 0] Loss: 0.0389\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.010524842888116837\n",
            "c0 grad norm: 0.014588803984224796\n",
            "conv1.weight grad norm: 0.19026482105255127\n",
            "conv1.bias grad norm: 3.886373001193988e-09\n",
            "batch_norm.weight grad norm: 0.02424916811287403\n",
            "batch_norm.bias grad norm: 0.019389690831303596\n",
            "lstm.weight_ih_l0 grad norm: 0.13892510533332825\n",
            "lstm.weight_hh_l0 grad norm: 0.045596811920404434\n",
            "lstm.bias_ih_l0 grad norm: 0.012735920958220959\n",
            "lstm.bias_hh_l0 grad norm: 0.012735920958220959\n",
            "fc.weight grad norm: 0.33382752537727356\n",
            "fc.bias grad norm: 0.14690937101840973\n",
            "[Batch 1000] Loss: 0.0457\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.038086842745542526\n",
            "c0 grad norm: 0.019830550998449326\n",
            "conv1.weight grad norm: 0.4359488785266876\n",
            "conv1.bias grad norm: 7.811245872346717e-09\n",
            "batch_norm.weight grad norm: 0.05479901283979416\n",
            "batch_norm.bias grad norm: 0.05085877701640129\n",
            "lstm.weight_ih_l0 grad norm: 0.3213392496109009\n",
            "lstm.weight_hh_l0 grad norm: 0.07614859938621521\n",
            "lstm.bias_ih_l0 grad norm: 0.02825290523469448\n",
            "lstm.bias_hh_l0 grad norm: 0.02825290523469448\n",
            "fc.weight grad norm: 0.5585641264915466\n",
            "fc.bias grad norm: 0.2054799497127533\n",
            "[Batch 2000] Loss: 0.1196\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.024549877271056175\n",
            "c0 grad norm: 0.016845133155584335\n",
            "conv1.weight grad norm: 0.31971654295921326\n",
            "conv1.bias grad norm: 3.4778280255665095e-09\n",
            "batch_norm.weight grad norm: 0.035937536507844925\n",
            "batch_norm.bias grad norm: 0.023821529000997543\n",
            "lstm.weight_ih_l0 grad norm: 0.21073710918426514\n",
            "lstm.weight_hh_l0 grad norm: 0.04880654439330101\n",
            "lstm.bias_ih_l0 grad norm: 0.017506515607237816\n",
            "lstm.bias_hh_l0 grad norm: 0.017506515607237816\n",
            "fc.weight grad norm: 0.38300320506095886\n",
            "fc.bias grad norm: 0.04672306776046753\n",
            "[Batch 3000] Loss: 0.0538\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.3517894744873047\n",
            "c0 grad norm: 0.07160937786102295\n",
            "conv1.weight grad norm: 3.344630241394043\n",
            "conv1.bias grad norm: 5.173405881464532e-08\n",
            "batch_norm.weight grad norm: 0.24481706321239471\n",
            "batch_norm.bias grad norm: 0.28595659136772156\n",
            "lstm.weight_ih_l0 grad norm: 1.1393791437149048\n",
            "lstm.weight_hh_l0 grad norm: 0.36055442690849304\n",
            "lstm.bias_ih_l0 grad norm: 0.1538572758436203\n",
            "lstm.bias_hh_l0 grad norm: 0.1538572758436203\n",
            "fc.weight grad norm: 0.8390733003616333\n",
            "fc.bias grad norm: 0.32682526111602783\n",
            "[Batch 4000] Loss: 0.1589\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.03704989328980446\n",
            "c0 grad norm: 0.011223780922591686\n",
            "conv1.weight grad norm: 0.3034404516220093\n",
            "conv1.bias grad norm: 7.133110546675425e-09\n",
            "batch_norm.weight grad norm: 0.031076593324542046\n",
            "batch_norm.bias grad norm: 0.02979714795947075\n",
            "lstm.weight_ih_l0 grad norm: 0.1300654262304306\n",
            "lstm.weight_hh_l0 grad norm: 0.04689706861972809\n",
            "lstm.bias_ih_l0 grad norm: 0.01507625263184309\n",
            "lstm.bias_hh_l0 grad norm: 0.01507625263184309\n",
            "fc.weight grad norm: 0.21093134582042694\n",
            "fc.bias grad norm: 0.04187791049480438\n",
            "[Batch 5000] Loss: 0.0269\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.02262132056057453\n",
            "c0 grad norm: 0.020420191809535027\n",
            "conv1.weight grad norm: 0.503332793712616\n",
            "conv1.bias grad norm: 7.2171815190813504e-09\n",
            "batch_norm.weight grad norm: 0.038912951946258545\n",
            "batch_norm.bias grad norm: 0.025800533592700958\n",
            "lstm.weight_ih_l0 grad norm: 0.19876015186309814\n",
            "lstm.weight_hh_l0 grad norm: 0.059885188937187195\n",
            "lstm.bias_ih_l0 grad norm: 0.018196117132902145\n",
            "lstm.bias_hh_l0 grad norm: 0.018196117132902145\n",
            "fc.weight grad norm: 0.3265104591846466\n",
            "fc.bias grad norm: 0.02673642709851265\n",
            "[Batch 6000] Loss: 0.0564\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.10497406125068665\n",
            "c0 grad norm: 0.052702952176332474\n",
            "conv1.weight grad norm: 1.163037657737732\n",
            "conv1.bias grad norm: 2.5496706967942373e-08\n",
            "batch_norm.weight grad norm: 0.11668122559785843\n",
            "batch_norm.bias grad norm: 0.10880988091230392\n",
            "lstm.weight_ih_l0 grad norm: 0.4693237543106079\n",
            "lstm.weight_hh_l0 grad norm: 0.1620437502861023\n",
            "lstm.bias_ih_l0 grad norm: 0.061818819493055344\n",
            "lstm.bias_hh_l0 grad norm: 0.061818819493055344\n",
            "fc.weight grad norm: 0.5052760243415833\n",
            "fc.bias grad norm: 0.16460368037223816\n",
            "[Batch 7000] Loss: 0.1254\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.010280175134539604\n",
            "c0 grad norm: 0.007772139273583889\n",
            "conv1.weight grad norm: 0.16125397384166718\n",
            "conv1.bias grad norm: 2.65374167085497e-09\n",
            "batch_norm.weight grad norm: 0.018816549330949783\n",
            "batch_norm.bias grad norm: 0.015970146283507347\n",
            "lstm.weight_ih_l0 grad norm: 0.09412480890750885\n",
            "lstm.weight_hh_l0 grad norm: 0.02934463880956173\n",
            "lstm.bias_ih_l0 grad norm: 0.008433060720562935\n",
            "lstm.bias_hh_l0 grad norm: 0.008433060720562935\n",
            "fc.weight grad norm: 0.23923829197883606\n",
            "fc.bias grad norm: 0.052953705191612244\n",
            "[Batch 8000] Loss: 0.0254\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.11351217329502106\n",
            "c0 grad norm: 0.057900942862033844\n",
            "conv1.weight grad norm: 1.5027800798416138\n",
            "conv1.bias grad norm: 2.2890477069381632e-08\n",
            "batch_norm.weight grad norm: 0.15408773720264435\n",
            "batch_norm.bias grad norm: 0.13216617703437805\n",
            "lstm.weight_ih_l0 grad norm: 0.6672866344451904\n",
            "lstm.weight_hh_l0 grad norm: 0.1940976083278656\n",
            "lstm.bias_ih_l0 grad norm: 0.068817138671875\n",
            "lstm.bias_hh_l0 grad norm: 0.068817138671875\n",
            "fc.weight grad norm: 0.9165773987770081\n",
            "fc.bias grad norm: 0.42833346128463745\n",
            "[Batch 9000] Loss: 0.2861\n",
            "Total Epoch Loss: 879.5261\n",
            "\n",
            "Epoch 76\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03914663940668106\n",
            "c0 grad norm: 0.034534551203250885\n",
            "conv1.weight grad norm: 0.8147362470626831\n",
            "conv1.bias grad norm: 9.960325009217286e-09\n",
            "batch_norm.weight grad norm: 0.07518894970417023\n",
            "batch_norm.bias grad norm: 0.08348003774881363\n",
            "lstm.weight_ih_l0 grad norm: 0.41330158710479736\n",
            "lstm.weight_hh_l0 grad norm: 0.1394403874874115\n",
            "lstm.bias_ih_l0 grad norm: 0.042196087539196014\n",
            "lstm.bias_hh_l0 grad norm: 0.042196087539196014\n",
            "fc.weight grad norm: 0.9224692583084106\n",
            "fc.bias grad norm: 0.29757389426231384\n",
            "[Batch 0] Loss: 0.4362\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.011593461968004704\n",
            "c0 grad norm: 0.011623503640294075\n",
            "conv1.weight grad norm: 0.24205660820007324\n",
            "conv1.bias grad norm: 4.415954713010706e-09\n",
            "batch_norm.weight grad norm: 0.025127209722995758\n",
            "batch_norm.bias grad norm: 0.022006496787071228\n",
            "lstm.weight_ih_l0 grad norm: 0.13200388848781586\n",
            "lstm.weight_hh_l0 grad norm: 0.05224623903632164\n",
            "lstm.bias_ih_l0 grad norm: 0.014537145383656025\n",
            "lstm.bias_hh_l0 grad norm: 0.014537145383656025\n",
            "fc.weight grad norm: 0.23176388442516327\n",
            "fc.bias grad norm: 0.12100367993116379\n",
            "[Batch 1000] Loss: 0.0312\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.007344817277044058\n",
            "c0 grad norm: 0.013267861679196358\n",
            "conv1.weight grad norm: 0.2570251226425171\n",
            "conv1.bias grad norm: 3.0861411204341493e-09\n",
            "batch_norm.weight grad norm: 0.023479076102375984\n",
            "batch_norm.bias grad norm: 0.02513384073972702\n",
            "lstm.weight_ih_l0 grad norm: 0.12042879313230515\n",
            "lstm.weight_hh_l0 grad norm: 0.04273500293493271\n",
            "lstm.bias_ih_l0 grad norm: 0.011184939183294773\n",
            "lstm.bias_hh_l0 grad norm: 0.011184939183294773\n",
            "fc.weight grad norm: 0.34027382731437683\n",
            "fc.bias grad norm: 0.12635716795921326\n",
            "[Batch 2000] Loss: 0.0507\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.01880493387579918\n",
            "c0 grad norm: 0.01583574153482914\n",
            "conv1.weight grad norm: 0.2958827018737793\n",
            "conv1.bias grad norm: 3.981368568162225e-09\n",
            "batch_norm.weight grad norm: 0.035122573375701904\n",
            "batch_norm.bias grad norm: 0.026822170242667198\n",
            "lstm.weight_ih_l0 grad norm: 0.1850665658712387\n",
            "lstm.weight_hh_l0 grad norm: 0.056386206299066544\n",
            "lstm.bias_ih_l0 grad norm: 0.015157432295382023\n",
            "lstm.bias_hh_l0 grad norm: 0.015157432295382023\n",
            "fc.weight grad norm: 0.307606965303421\n",
            "fc.bias grad norm: 0.06800515204668045\n",
            "[Batch 3000] Loss: 0.0639\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.013191459700465202\n",
            "c0 grad norm: 0.015285501256585121\n",
            "conv1.weight grad norm: 0.13222333788871765\n",
            "conv1.bias grad norm: 3.6173295470121047e-09\n",
            "batch_norm.weight grad norm: 0.01371173094958067\n",
            "batch_norm.bias grad norm: 0.013061177916824818\n",
            "lstm.weight_ih_l0 grad norm: 0.11833585053682327\n",
            "lstm.weight_hh_l0 grad norm: 0.03756632283329964\n",
            "lstm.bias_ih_l0 grad norm: 0.011265971697866917\n",
            "lstm.bias_hh_l0 grad norm: 0.011265971697866917\n",
            "fc.weight grad norm: 0.2594713568687439\n",
            "fc.bias grad norm: 0.12917235493659973\n",
            "[Batch 4000] Loss: 0.0462\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.03108365833759308\n",
            "c0 grad norm: 0.026340696960687637\n",
            "conv1.weight grad norm: 0.4255933165550232\n",
            "conv1.bias grad norm: 8.39868441460112e-09\n",
            "batch_norm.weight grad norm: 0.06102700158953667\n",
            "batch_norm.bias grad norm: 0.041860248893499374\n",
            "lstm.weight_ih_l0 grad norm: 0.3244551122188568\n",
            "lstm.weight_hh_l0 grad norm: 0.09506121277809143\n",
            "lstm.bias_ih_l0 grad norm: 0.03070397488772869\n",
            "lstm.bias_hh_l0 grad norm: 0.03070397488772869\n",
            "fc.weight grad norm: 0.3626101016998291\n",
            "fc.bias grad norm: 0.053306274116039276\n",
            "[Batch 5000] Loss: 0.0554\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.020776689052581787\n",
            "c0 grad norm: 0.0221941489726305\n",
            "conv1.weight grad norm: 0.46921712160110474\n",
            "conv1.bias grad norm: 7.2011876461886e-09\n",
            "batch_norm.weight grad norm: 0.03617139533162117\n",
            "batch_norm.bias grad norm: 0.029513442888855934\n",
            "lstm.weight_ih_l0 grad norm: 0.20990903675556183\n",
            "lstm.weight_hh_l0 grad norm: 0.06020600348711014\n",
            "lstm.bias_ih_l0 grad norm: 0.01749507524073124\n",
            "lstm.bias_hh_l0 grad norm: 0.01749507524073124\n",
            "fc.weight grad norm: 0.33866482973098755\n",
            "fc.bias grad norm: 0.05278301239013672\n",
            "[Batch 6000] Loss: 0.0517\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.011063193902373314\n",
            "c0 grad norm: 0.01171747874468565\n",
            "conv1.weight grad norm: 0.12334407866001129\n",
            "conv1.bias grad norm: 1.908446733978053e-09\n",
            "batch_norm.weight grad norm: 0.017101570963859558\n",
            "batch_norm.bias grad norm: 0.01232840958982706\n",
            "lstm.weight_ih_l0 grad norm: 0.13150715827941895\n",
            "lstm.weight_hh_l0 grad norm: 0.03819017484784126\n",
            "lstm.bias_ih_l0 grad norm: 0.013091444969177246\n",
            "lstm.bias_hh_l0 grad norm: 0.013091444969177246\n",
            "fc.weight grad norm: 0.3320019543170929\n",
            "fc.bias grad norm: 0.12970799207687378\n",
            "[Batch 7000] Loss: 0.0384\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.03517582640051842\n",
            "c0 grad norm: 0.02808302454650402\n",
            "conv1.weight grad norm: 0.49134308099746704\n",
            "conv1.bias grad norm: 9.16867648470543e-09\n",
            "batch_norm.weight grad norm: 0.053643934428691864\n",
            "batch_norm.bias grad norm: 0.0416988767683506\n",
            "lstm.weight_ih_l0 grad norm: 0.26642629504203796\n",
            "lstm.weight_hh_l0 grad norm: 0.08088896423578262\n",
            "lstm.bias_ih_l0 grad norm: 0.027379842475056648\n",
            "lstm.bias_hh_l0 grad norm: 0.027379842475056648\n",
            "fc.weight grad norm: 0.662222146987915\n",
            "fc.bias grad norm: 0.2651573419570923\n",
            "[Batch 8000] Loss: 0.1515\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.047832097858190536\n",
            "c0 grad norm: 0.029258953407406807\n",
            "conv1.weight grad norm: 0.37534603476524353\n",
            "conv1.bias grad norm: 4.701693256947692e-09\n",
            "batch_norm.weight grad norm: 0.041836898773908615\n",
            "batch_norm.bias grad norm: 0.05460439622402191\n",
            "lstm.weight_ih_l0 grad norm: 0.2571999728679657\n",
            "lstm.weight_hh_l0 grad norm: 0.0967026948928833\n",
            "lstm.bias_ih_l0 grad norm: 0.03830068185925484\n",
            "lstm.bias_hh_l0 grad norm: 0.03830068185925484\n",
            "fc.weight grad norm: 0.504477322101593\n",
            "fc.bias grad norm: 0.25688865780830383\n",
            "[Batch 9000] Loss: 0.1123\n",
            "Total Epoch Loss: 880.3282\n",
            "\n",
            "Epoch 77\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.04275476932525635\n",
            "c0 grad norm: 0.03452202305197716\n",
            "conv1.weight grad norm: 0.6374122500419617\n",
            "conv1.bias grad norm: 1.0650254900212985e-08\n",
            "batch_norm.weight grad norm: 0.0524308942258358\n",
            "batch_norm.bias grad norm: 0.0347861684858799\n",
            "lstm.weight_ih_l0 grad norm: 0.25166064500808716\n",
            "lstm.weight_hh_l0 grad norm: 0.09369062632322311\n",
            "lstm.bias_ih_l0 grad norm: 0.028562208637595177\n",
            "lstm.bias_hh_l0 grad norm: 0.028562208637595177\n",
            "fc.weight grad norm: 0.6881679892539978\n",
            "fc.bias grad norm: 0.24986524879932404\n",
            "[Batch 0] Loss: 0.0898\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.1677270233631134\n",
            "c0 grad norm: 0.09516690671443939\n",
            "conv1.weight grad norm: 1.3956384658813477\n",
            "conv1.bias grad norm: 3.7317040124662526e-08\n",
            "batch_norm.weight grad norm: 0.13302911818027496\n",
            "batch_norm.bias grad norm: 0.13955838978290558\n",
            "lstm.weight_ih_l0 grad norm: 0.6459544897079468\n",
            "lstm.weight_hh_l0 grad norm: 0.19676972925662994\n",
            "lstm.bias_ih_l0 grad norm: 0.07915455102920532\n",
            "lstm.bias_hh_l0 grad norm: 0.07915455102920532\n",
            "fc.weight grad norm: 0.8449719548225403\n",
            "fc.bias grad norm: 0.3907647430896759\n",
            "[Batch 1000] Loss: 0.4050\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.037336867302656174\n",
            "c0 grad norm: 0.017033830285072327\n",
            "conv1.weight grad norm: 0.4553491771221161\n",
            "conv1.bias grad norm: 1.0320542642716646e-08\n",
            "batch_norm.weight grad norm: 0.037499621510505676\n",
            "batch_norm.bias grad norm: 0.03444996476173401\n",
            "lstm.weight_ih_l0 grad norm: 0.17304158210754395\n",
            "lstm.weight_hh_l0 grad norm: 0.05773348733782768\n",
            "lstm.bias_ih_l0 grad norm: 0.018791424110531807\n",
            "lstm.bias_hh_l0 grad norm: 0.018791424110531807\n",
            "fc.weight grad norm: 0.22214917838573456\n",
            "fc.bias grad norm: 0.0400383435189724\n",
            "[Batch 2000] Loss: 0.0363\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.022501083090901375\n",
            "c0 grad norm: 0.016225339844822884\n",
            "conv1.weight grad norm: 0.2149038463830948\n",
            "conv1.bias grad norm: 4.395539932033898e-09\n",
            "batch_norm.weight grad norm: 0.01976954936981201\n",
            "batch_norm.bias grad norm: 0.01901581510901451\n",
            "lstm.weight_ih_l0 grad norm: 0.10681158304214478\n",
            "lstm.weight_hh_l0 grad norm: 0.037650857120752335\n",
            "lstm.bias_ih_l0 grad norm: 0.01409804169088602\n",
            "lstm.bias_hh_l0 grad norm: 0.01409804169088602\n",
            "fc.weight grad norm: 0.2764792740345001\n",
            "fc.bias grad norm: 0.09167551249265671\n",
            "[Batch 3000] Loss: 0.0220\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.01250678300857544\n",
            "c0 grad norm: 0.01635415107011795\n",
            "conv1.weight grad norm: 0.16238947212696075\n",
            "conv1.bias grad norm: 3.0300104647551507e-09\n",
            "batch_norm.weight grad norm: 0.0229396503418684\n",
            "batch_norm.bias grad norm: 0.026065567508339882\n",
            "lstm.weight_ih_l0 grad norm: 0.13507409393787384\n",
            "lstm.weight_hh_l0 grad norm: 0.04979561269283295\n",
            "lstm.bias_ih_l0 grad norm: 0.014898061752319336\n",
            "lstm.bias_hh_l0 grad norm: 0.014898061752319336\n",
            "fc.weight grad norm: 0.3015943169593811\n",
            "fc.bias grad norm: 0.13268353044986725\n",
            "[Batch 4000] Loss: 0.0365\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.03889770433306694\n",
            "c0 grad norm: 0.01791643165051937\n",
            "conv1.weight grad norm: 0.3868849575519562\n",
            "conv1.bias grad norm: 5.581939799270685e-09\n",
            "batch_norm.weight grad norm: 0.03532246872782707\n",
            "batch_norm.bias grad norm: 0.03447597101330757\n",
            "lstm.weight_ih_l0 grad norm: 0.1991199553012848\n",
            "lstm.weight_hh_l0 grad norm: 0.06515228748321533\n",
            "lstm.bias_ih_l0 grad norm: 0.025944583117961884\n",
            "lstm.bias_hh_l0 grad norm: 0.025944583117961884\n",
            "fc.weight grad norm: 0.3268982470035553\n",
            "fc.bias grad norm: 0.10354122519493103\n",
            "[Batch 5000] Loss: 0.0670\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.015325588174164295\n",
            "c0 grad norm: 0.01683753915131092\n",
            "conv1.weight grad norm: 0.20162595808506012\n",
            "conv1.bias grad norm: 4.790255303532831e-09\n",
            "batch_norm.weight grad norm: 0.020952027291059494\n",
            "batch_norm.bias grad norm: 0.016503656283020973\n",
            "lstm.weight_ih_l0 grad norm: 0.11943221092224121\n",
            "lstm.weight_hh_l0 grad norm: 0.03570472076535225\n",
            "lstm.bias_ih_l0 grad norm: 0.012286483310163021\n",
            "lstm.bias_hh_l0 grad norm: 0.012286483310163021\n",
            "fc.weight grad norm: 0.2573945224285126\n",
            "fc.bias grad norm: 0.10167718678712845\n",
            "[Batch 6000] Loss: 0.0378\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.04780438169836998\n",
            "c0 grad norm: 0.03699950501322746\n",
            "conv1.weight grad norm: 0.628937840461731\n",
            "conv1.bias grad norm: 1.2470210641879476e-08\n",
            "batch_norm.weight grad norm: 0.07683455944061279\n",
            "batch_norm.bias grad norm: 0.07975440472364426\n",
            "lstm.weight_ih_l0 grad norm: 0.3881397843360901\n",
            "lstm.weight_hh_l0 grad norm: 0.11502949893474579\n",
            "lstm.bias_ih_l0 grad norm: 0.03394141420722008\n",
            "lstm.bias_hh_l0 grad norm: 0.03394141420722008\n",
            "fc.weight grad norm: 0.5927271842956543\n",
            "fc.bias grad norm: 0.216892808675766\n",
            "[Batch 7000] Loss: 0.1955\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.016276400536298752\n",
            "c0 grad norm: 0.016904089599847794\n",
            "conv1.weight grad norm: 0.19174008071422577\n",
            "conv1.bias grad norm: 3.725734387671764e-09\n",
            "batch_norm.weight grad norm: 0.024831067770719528\n",
            "batch_norm.bias grad norm: 0.021292762830853462\n",
            "lstm.weight_ih_l0 grad norm: 0.1337619572877884\n",
            "lstm.weight_hh_l0 grad norm: 0.04215231165289879\n",
            "lstm.bias_ih_l0 grad norm: 0.013878275640308857\n",
            "lstm.bias_hh_l0 grad norm: 0.013878275640308857\n",
            "fc.weight grad norm: 0.20380418002605438\n",
            "fc.bias grad norm: 0.08879844844341278\n",
            "[Batch 8000] Loss: 0.0290\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.015984203666448593\n",
            "c0 grad norm: 0.009101351723074913\n",
            "conv1.weight grad norm: 0.2041970044374466\n",
            "conv1.bias grad norm: 3.030415030025324e-09\n",
            "batch_norm.weight grad norm: 0.02733452618122101\n",
            "batch_norm.bias grad norm: 0.021879704669117928\n",
            "lstm.weight_ih_l0 grad norm: 0.1429297775030136\n",
            "lstm.weight_hh_l0 grad norm: 0.032909080386161804\n",
            "lstm.bias_ih_l0 grad norm: 0.013100742362439632\n",
            "lstm.bias_hh_l0 grad norm: 0.013100742362439632\n",
            "fc.weight grad norm: 0.34448644518852234\n",
            "fc.bias grad norm: 0.12652283906936646\n",
            "[Batch 9000] Loss: 0.0277\n",
            "Total Epoch Loss: 881.3804\n",
            "\n",
            "Epoch 78\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.09336548298597336\n",
            "c0 grad norm: 0.09798433631658554\n",
            "conv1.weight grad norm: 1.2505580186843872\n",
            "conv1.bias grad norm: 2.8786224959276296e-08\n",
            "batch_norm.weight grad norm: 0.12328578531742096\n",
            "batch_norm.bias grad norm: 0.1421678364276886\n",
            "lstm.weight_ih_l0 grad norm: 0.6766332387924194\n",
            "lstm.weight_hh_l0 grad norm: 0.24370703101158142\n",
            "lstm.bias_ih_l0 grad norm: 0.08600001037120819\n",
            "lstm.bias_hh_l0 grad norm: 0.08600001037120819\n",
            "fc.weight grad norm: 0.866074800491333\n",
            "fc.bias grad norm: 0.44772472977638245\n",
            "[Batch 0] Loss: 0.4955\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.04631614312529564\n",
            "c0 grad norm: 0.029350316151976585\n",
            "conv1.weight grad norm: 0.4489107131958008\n",
            "conv1.bias grad norm: 1.0931597849150876e-08\n",
            "batch_norm.weight grad norm: 0.06087141111493111\n",
            "batch_norm.bias grad norm: 0.054602161049842834\n",
            "lstm.weight_ih_l0 grad norm: 0.3115123510360718\n",
            "lstm.weight_hh_l0 grad norm: 0.11346113681793213\n",
            "lstm.bias_ih_l0 grad norm: 0.03784850239753723\n",
            "lstm.bias_hh_l0 grad norm: 0.03784850239753723\n",
            "fc.weight grad norm: 0.24742341041564941\n",
            "fc.bias grad norm: 0.08268292248249054\n",
            "[Batch 1000] Loss: 0.0394\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.025010066106915474\n",
            "c0 grad norm: 0.01592557691037655\n",
            "conv1.weight grad norm: 0.2368038147687912\n",
            "conv1.bias grad norm: 4.836504974292666e-09\n",
            "batch_norm.weight grad norm: 0.02666548267006874\n",
            "batch_norm.bias grad norm: 0.02311107888817787\n",
            "lstm.weight_ih_l0 grad norm: 0.1373155117034912\n",
            "lstm.weight_hh_l0 grad norm: 0.04238268360495567\n",
            "lstm.bias_ih_l0 grad norm: 0.014992594718933105\n",
            "lstm.bias_hh_l0 grad norm: 0.014992594718933105\n",
            "fc.weight grad norm: 0.3119964003562927\n",
            "fc.bias grad norm: 0.15842223167419434\n",
            "[Batch 2000] Loss: 0.0465\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.04931158572435379\n",
            "c0 grad norm: 0.020961612462997437\n",
            "conv1.weight grad norm: 0.2815684974193573\n",
            "conv1.bias grad norm: 9.33859212182142e-09\n",
            "batch_norm.weight grad norm: 0.031708355993032455\n",
            "batch_norm.bias grad norm: 0.04323052987456322\n",
            "lstm.weight_ih_l0 grad norm: 0.2120065689086914\n",
            "lstm.weight_hh_l0 grad norm: 0.07358894497156143\n",
            "lstm.bias_ih_l0 grad norm: 0.030561016872525215\n",
            "lstm.bias_hh_l0 grad norm: 0.030561016872525215\n",
            "fc.weight grad norm: 0.48656246066093445\n",
            "fc.bias grad norm: 0.13539528846740723\n",
            "[Batch 3000] Loss: 0.0712\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.05716737359762192\n",
            "c0 grad norm: 0.019488107413053513\n",
            "conv1.weight grad norm: 0.4602775573730469\n",
            "conv1.bias grad norm: 1.3526875619618295e-08\n",
            "batch_norm.weight grad norm: 0.049149006605148315\n",
            "batch_norm.bias grad norm: 0.06470343470573425\n",
            "lstm.weight_ih_l0 grad norm: 0.2055332511663437\n",
            "lstm.weight_hh_l0 grad norm: 0.058983270078897476\n",
            "lstm.bias_ih_l0 grad norm: 0.02868741750717163\n",
            "lstm.bias_hh_l0 grad norm: 0.02868741750717163\n",
            "fc.weight grad norm: 0.32357949018478394\n",
            "fc.bias grad norm: 0.07782381772994995\n",
            "[Batch 4000] Loss: 0.0538\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.03913814201951027\n",
            "c0 grad norm: 0.04015122354030609\n",
            "conv1.weight grad norm: 0.4439888000488281\n",
            "conv1.bias grad norm: 1.0959592344761404e-08\n",
            "batch_norm.weight grad norm: 0.05084246024489403\n",
            "batch_norm.bias grad norm: 0.05898280069231987\n",
            "lstm.weight_ih_l0 grad norm: 0.3163699805736542\n",
            "lstm.weight_hh_l0 grad norm: 0.14878502488136292\n",
            "lstm.bias_ih_l0 grad norm: 0.037893299013376236\n",
            "lstm.bias_hh_l0 grad norm: 0.037893299013376236\n",
            "fc.weight grad norm: 0.365546315908432\n",
            "fc.bias grad norm: 0.11092251539230347\n",
            "[Batch 5000] Loss: 0.0713\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.017936168238520622\n",
            "c0 grad norm: 0.01866011880338192\n",
            "conv1.weight grad norm: 0.3415321111679077\n",
            "conv1.bias grad norm: 4.911831830156643e-09\n",
            "batch_norm.weight grad norm: 0.028843402862548828\n",
            "batch_norm.bias grad norm: 0.037342701107263565\n",
            "lstm.weight_ih_l0 grad norm: 0.21051567792892456\n",
            "lstm.weight_hh_l0 grad norm: 0.07888089120388031\n",
            "lstm.bias_ih_l0 grad norm: 0.022478364408016205\n",
            "lstm.bias_hh_l0 grad norm: 0.022478364408016205\n",
            "fc.weight grad norm: 0.19243714213371277\n",
            "fc.bias grad norm: 0.08946354687213898\n",
            "[Batch 6000] Loss: 0.0382\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.012136410921812057\n",
            "c0 grad norm: 0.011004460044205189\n",
            "conv1.weight grad norm: 0.1931450217962265\n",
            "conv1.bias grad norm: 5.2325468402614206e-09\n",
            "batch_norm.weight grad norm: 0.0264885351061821\n",
            "batch_norm.bias grad norm: 0.03258490934967995\n",
            "lstm.weight_ih_l0 grad norm: 0.1218363419175148\n",
            "lstm.weight_hh_l0 grad norm: 0.04489229992032051\n",
            "lstm.bias_ih_l0 grad norm: 0.014993084594607353\n",
            "lstm.bias_hh_l0 grad norm: 0.014993084594607353\n",
            "fc.weight grad norm: 0.14694951474666595\n",
            "fc.bias grad norm: 0.0823049321770668\n",
            "[Batch 7000] Loss: 0.0209\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.05220258608460426\n",
            "c0 grad norm: 0.028254864737391472\n",
            "conv1.weight grad norm: 0.6392294764518738\n",
            "conv1.bias grad norm: 1.2864613374574674e-08\n",
            "batch_norm.weight grad norm: 0.05872507393360138\n",
            "batch_norm.bias grad norm: 0.051712147891521454\n",
            "lstm.weight_ih_l0 grad norm: 0.3617279827594757\n",
            "lstm.weight_hh_l0 grad norm: 0.08723398298025131\n",
            "lstm.bias_ih_l0 grad norm: 0.030192924663424492\n",
            "lstm.bias_hh_l0 grad norm: 0.030192924663424492\n",
            "fc.weight grad norm: 0.33371487259864807\n",
            "fc.bias grad norm: 0.09374352544546127\n",
            "[Batch 8000] Loss: 0.0525\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.01137674693018198\n",
            "c0 grad norm: 0.011728852055966854\n",
            "conv1.weight grad norm: 0.15384885668754578\n",
            "conv1.bias grad norm: 3.2357077017763913e-09\n",
            "batch_norm.weight grad norm: 0.01842515729367733\n",
            "batch_norm.bias grad norm: 0.015939567238092422\n",
            "lstm.weight_ih_l0 grad norm: 0.09802962094545364\n",
            "lstm.weight_hh_l0 grad norm: 0.038398340344429016\n",
            "lstm.bias_ih_l0 grad norm: 0.01025931816548109\n",
            "lstm.bias_hh_l0 grad norm: 0.01025931816548109\n",
            "fc.weight grad norm: 0.3140154480934143\n",
            "fc.bias grad norm: 0.15448661148548126\n",
            "[Batch 9000] Loss: 0.0573\n",
            "Total Epoch Loss: 882.6283\n",
            "\n",
            "Epoch 79\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.005584167782217264\n",
            "c0 grad norm: 0.005893459543585777\n",
            "conv1.weight grad norm: 0.11827713251113892\n",
            "conv1.bias grad norm: 2.4423176814281078e-09\n",
            "batch_norm.weight grad norm: 0.011329138651490211\n",
            "batch_norm.bias grad norm: 0.007620963733643293\n",
            "lstm.weight_ih_l0 grad norm: 0.07270356267690659\n",
            "lstm.weight_hh_l0 grad norm: 0.025666287168860435\n",
            "lstm.bias_ih_l0 grad norm: 0.006876484490931034\n",
            "lstm.bias_hh_l0 grad norm: 0.006876484490931034\n",
            "fc.weight grad norm: 0.15512023866176605\n",
            "fc.bias grad norm: 0.027465637773275375\n",
            "[Batch 0] Loss: 0.0297\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.03261558338999748\n",
            "c0 grad norm: 0.0334642119705677\n",
            "conv1.weight grad norm: 0.3063530921936035\n",
            "conv1.bias grad norm: 5.6443303364517305e-09\n",
            "batch_norm.weight grad norm: 0.04424917697906494\n",
            "batch_norm.bias grad norm: 0.044717203825712204\n",
            "lstm.weight_ih_l0 grad norm: 0.2194272130727768\n",
            "lstm.weight_hh_l0 grad norm: 0.07331225275993347\n",
            "lstm.bias_ih_l0 grad norm: 0.023846084251999855\n",
            "lstm.bias_hh_l0 grad norm: 0.023846084251999855\n",
            "fc.weight grad norm: 0.4599296450614929\n",
            "fc.bias grad norm: 0.08828378468751907\n",
            "[Batch 1000] Loss: 0.1356\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.014452800154685974\n",
            "c0 grad norm: 0.025769192725419998\n",
            "conv1.weight grad norm: 0.2627972364425659\n",
            "conv1.bias grad norm: 5.377311929066764e-09\n",
            "batch_norm.weight grad norm: 0.03459698706865311\n",
            "batch_norm.bias grad norm: 0.03549931198358536\n",
            "lstm.weight_ih_l0 grad norm: 0.16319647431373596\n",
            "lstm.weight_hh_l0 grad norm: 0.056743185967206955\n",
            "lstm.bias_ih_l0 grad norm: 0.016770288348197937\n",
            "lstm.bias_hh_l0 grad norm: 0.016770288348197937\n",
            "fc.weight grad norm: 0.786291778087616\n",
            "fc.bias grad norm: 0.21977555751800537\n",
            "[Batch 2000] Loss: 0.1456\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.01898345910012722\n",
            "c0 grad norm: 0.020599963143467903\n",
            "conv1.weight grad norm: 0.2932850122451782\n",
            "conv1.bias grad norm: 6.985492628786005e-09\n",
            "batch_norm.weight grad norm: 0.031533364206552505\n",
            "batch_norm.bias grad norm: 0.04059743881225586\n",
            "lstm.weight_ih_l0 grad norm: 0.17703624069690704\n",
            "lstm.weight_hh_l0 grad norm: 0.06327080726623535\n",
            "lstm.bias_ih_l0 grad norm: 0.018914014101028442\n",
            "lstm.bias_hh_l0 grad norm: 0.018914014101028442\n",
            "fc.weight grad norm: 0.248195081949234\n",
            "fc.bias grad norm: 0.058485884219408035\n",
            "[Batch 3000] Loss: 0.0463\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.05964767560362816\n",
            "c0 grad norm: 0.04269401356577873\n",
            "conv1.weight grad norm: 0.9509606957435608\n",
            "conv1.bias grad norm: 1.4395174829928692e-08\n",
            "batch_norm.weight grad norm: 0.08125846087932587\n",
            "batch_norm.bias grad norm: 0.1063789576292038\n",
            "lstm.weight_ih_l0 grad norm: 0.3611486554145813\n",
            "lstm.weight_hh_l0 grad norm: 0.13689810037612915\n",
            "lstm.bias_ih_l0 grad norm: 0.0445983000099659\n",
            "lstm.bias_hh_l0 grad norm: 0.0445983000099659\n",
            "fc.weight grad norm: 0.4367905855178833\n",
            "fc.bias grad norm: 0.2031427025794983\n",
            "[Batch 4000] Loss: 0.1206\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.020675508305430412\n",
            "c0 grad norm: 0.020435193553566933\n",
            "conv1.weight grad norm: 0.29645952582359314\n",
            "conv1.bias grad norm: 3.864279563003947e-09\n",
            "batch_norm.weight grad norm: 0.03148987516760826\n",
            "batch_norm.bias grad norm: 0.03323523700237274\n",
            "lstm.weight_ih_l0 grad norm: 0.17721223831176758\n",
            "lstm.weight_hh_l0 grad norm: 0.06938158720731735\n",
            "lstm.bias_ih_l0 grad norm: 0.020840898156166077\n",
            "lstm.bias_hh_l0 grad norm: 0.020840898156166077\n",
            "fc.weight grad norm: 0.36156436800956726\n",
            "fc.bias grad norm: 0.10504695028066635\n",
            "[Batch 5000] Loss: 0.0507\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.048522211611270905\n",
            "c0 grad norm: 0.04249894246459007\n",
            "conv1.weight grad norm: 0.9935751557350159\n",
            "conv1.bias grad norm: 1.4090867139771035e-08\n",
            "batch_norm.weight grad norm: 0.07449047267436981\n",
            "batch_norm.bias grad norm: 0.06838872283697128\n",
            "lstm.weight_ih_l0 grad norm: 0.3807569444179535\n",
            "lstm.weight_hh_l0 grad norm: 0.11518926918506622\n",
            "lstm.bias_ih_l0 grad norm: 0.040355782955884933\n",
            "lstm.bias_hh_l0 grad norm: 0.040355782955884933\n",
            "fc.weight grad norm: 0.46499213576316833\n",
            "fc.bias grad norm: 0.07639798521995544\n",
            "[Batch 6000] Loss: 0.1202\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.006801700219511986\n",
            "c0 grad norm: 0.004025672096759081\n",
            "conv1.weight grad norm: 0.07533960789442062\n",
            "conv1.bias grad norm: 1.2711397490150489e-09\n",
            "batch_norm.weight grad norm: 0.007985645905137062\n",
            "batch_norm.bias grad norm: 0.005972632206976414\n",
            "lstm.weight_ih_l0 grad norm: 0.0418352410197258\n",
            "lstm.weight_hh_l0 grad norm: 0.013245858252048492\n",
            "lstm.bias_ih_l0 grad norm: 0.004987735766917467\n",
            "lstm.bias_hh_l0 grad norm: 0.004987735766917467\n",
            "fc.weight grad norm: 0.2065478265285492\n",
            "fc.bias grad norm: 0.06847821176052094\n",
            "[Batch 7000] Loss: 0.0156\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.061041127890348434\n",
            "c0 grad norm: 0.039214152842760086\n",
            "conv1.weight grad norm: 0.9140493273735046\n",
            "conv1.bias grad norm: 1.8150025482555066e-08\n",
            "batch_norm.weight grad norm: 0.07010344415903091\n",
            "batch_norm.bias grad norm: 0.05440398305654526\n",
            "lstm.weight_ih_l0 grad norm: 0.3638283610343933\n",
            "lstm.weight_hh_l0 grad norm: 0.09893237054347992\n",
            "lstm.bias_ih_l0 grad norm: 0.034212060272693634\n",
            "lstm.bias_hh_l0 grad norm: 0.034212060272693634\n",
            "fc.weight grad norm: 0.5890071988105774\n",
            "fc.bias grad norm: 0.07637523859739304\n",
            "[Batch 8000] Loss: 0.2463\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.012898426502943039\n",
            "c0 grad norm: 0.01064668782055378\n",
            "conv1.weight grad norm: 0.08836471289396286\n",
            "conv1.bias grad norm: 1.8509108690167864e-09\n",
            "batch_norm.weight grad norm: 0.011764215305447578\n",
            "batch_norm.bias grad norm: 0.01079016737639904\n",
            "lstm.weight_ih_l0 grad norm: 0.06829296797513962\n",
            "lstm.weight_hh_l0 grad norm: 0.02598239667713642\n",
            "lstm.bias_ih_l0 grad norm: 0.007980741560459137\n",
            "lstm.bias_hh_l0 grad norm: 0.007980741560459137\n",
            "fc.weight grad norm: 0.19337965548038483\n",
            "fc.bias grad norm: 0.05940322205424309\n",
            "[Batch 9000] Loss: 0.0149\n",
            "Total Epoch Loss: 882.2166\n",
            "\n",
            "Epoch 80\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.04235462471842766\n",
            "c0 grad norm: 0.03262346237897873\n",
            "conv1.weight grad norm: 0.35510504245758057\n",
            "conv1.bias grad norm: 7.721446593222936e-09\n",
            "batch_norm.weight grad norm: 0.051880065351724625\n",
            "batch_norm.bias grad norm: 0.04651157557964325\n",
            "lstm.weight_ih_l0 grad norm: 0.2588181495666504\n",
            "lstm.weight_hh_l0 grad norm: 0.08900167793035507\n",
            "lstm.bias_ih_l0 grad norm: 0.028047608211636543\n",
            "lstm.bias_hh_l0 grad norm: 0.028047608211636543\n",
            "fc.weight grad norm: 0.7398672699928284\n",
            "fc.bias grad norm: 0.21825267374515533\n",
            "[Batch 0] Loss: 0.1676\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.01087042223662138\n",
            "c0 grad norm: 0.009363053366541862\n",
            "conv1.weight grad norm: 0.15799680352210999\n",
            "conv1.bias grad norm: 3.1273303946477427e-09\n",
            "batch_norm.weight grad norm: 0.01949785090982914\n",
            "batch_norm.bias grad norm: 0.014025317505002022\n",
            "lstm.weight_ih_l0 grad norm: 0.10812155157327652\n",
            "lstm.weight_hh_l0 grad norm: 0.03450050950050354\n",
            "lstm.bias_ih_l0 grad norm: 0.009502324275672436\n",
            "lstm.bias_hh_l0 grad norm: 0.009502324275672436\n",
            "fc.weight grad norm: 0.09534025192260742\n",
            "fc.bias grad norm: 0.02441910095512867\n",
            "[Batch 1000] Loss: 0.0253\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.08928593248128891\n",
            "c0 grad norm: 0.054517585784196854\n",
            "conv1.weight grad norm: 0.8968337178230286\n",
            "conv1.bias grad norm: 1.4367110168223007e-08\n",
            "batch_norm.weight grad norm: 0.11147548258304596\n",
            "batch_norm.bias grad norm: 0.08126746863126755\n",
            "lstm.weight_ih_l0 grad norm: 0.6047026515007019\n",
            "lstm.weight_hh_l0 grad norm: 0.14225642383098602\n",
            "lstm.bias_ih_l0 grad norm: 0.05944559723138809\n",
            "lstm.bias_hh_l0 grad norm: 0.05944559723138809\n",
            "fc.weight grad norm: 0.7846145033836365\n",
            "fc.bias grad norm: 0.11151598393917084\n",
            "[Batch 2000] Loss: 0.1873\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.08969372510910034\n",
            "c0 grad norm: 0.020733999088406563\n",
            "conv1.weight grad norm: 0.718043327331543\n",
            "conv1.bias grad norm: 8.35967917112157e-09\n",
            "batch_norm.weight grad norm: 0.04422653466463089\n",
            "batch_norm.bias grad norm: 0.0939849391579628\n",
            "lstm.weight_ih_l0 grad norm: 0.2840971350669861\n",
            "lstm.weight_hh_l0 grad norm: 0.09615295380353928\n",
            "lstm.bias_ih_l0 grad norm: 0.04815921187400818\n",
            "lstm.bias_hh_l0 grad norm: 0.04815921187400818\n",
            "fc.weight grad norm: 0.4411148428916931\n",
            "fc.bias grad norm: 0.09055691212415695\n",
            "[Batch 3000] Loss: 0.0659\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.09604381769895554\n",
            "c0 grad norm: 0.03556499257683754\n",
            "conv1.weight grad norm: 0.8865005970001221\n",
            "conv1.bias grad norm: 1.3061501213940119e-08\n",
            "batch_norm.weight grad norm: 0.11498070508241653\n",
            "batch_norm.bias grad norm: 0.07918398082256317\n",
            "lstm.weight_ih_l0 grad norm: 0.4907190501689911\n",
            "lstm.weight_hh_l0 grad norm: 0.11878179758787155\n",
            "lstm.bias_ih_l0 grad norm: 0.04907293617725372\n",
            "lstm.bias_hh_l0 grad norm: 0.04907293617725372\n",
            "fc.weight grad norm: 0.6095914840698242\n",
            "fc.bias grad norm: 0.1822425276041031\n",
            "[Batch 4000] Loss: 0.1203\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.03099059872329235\n",
            "c0 grad norm: 0.01882306858897209\n",
            "conv1.weight grad norm: 0.42523136734962463\n",
            "conv1.bias grad norm: 5.394437341266212e-09\n",
            "batch_norm.weight grad norm: 0.043582189828157425\n",
            "batch_norm.bias grad norm: 0.03867931663990021\n",
            "lstm.weight_ih_l0 grad norm: 0.230154886841774\n",
            "lstm.weight_hh_l0 grad norm: 0.06158846616744995\n",
            "lstm.bias_ih_l0 grad norm: 0.02356790192425251\n",
            "lstm.bias_hh_l0 grad norm: 0.02356790192425251\n",
            "fc.weight grad norm: 0.3436098098754883\n",
            "fc.bias grad norm: 0.15438079833984375\n",
            "[Batch 5000] Loss: 0.0676\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.06234833970665932\n",
            "c0 grad norm: 0.03587057814002037\n",
            "conv1.weight grad norm: 0.43589112162590027\n",
            "conv1.bias grad norm: 8.713550769812173e-09\n",
            "batch_norm.weight grad norm: 0.046983908861875534\n",
            "batch_norm.bias grad norm: 0.06556259095668793\n",
            "lstm.weight_ih_l0 grad norm: 0.2974551320075989\n",
            "lstm.weight_hh_l0 grad norm: 0.11059721559286118\n",
            "lstm.bias_ih_l0 grad norm: 0.04310738295316696\n",
            "lstm.bias_hh_l0 grad norm: 0.04310738295316696\n",
            "fc.weight grad norm: 0.3315664529800415\n",
            "fc.bias grad norm: 0.12891899049282074\n",
            "[Batch 6000] Loss: 0.0692\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.05148204788565636\n",
            "c0 grad norm: 0.028364095836877823\n",
            "conv1.weight grad norm: 0.5519078969955444\n",
            "conv1.bias grad norm: 1.4397245173825013e-08\n",
            "batch_norm.weight grad norm: 0.06130626052618027\n",
            "batch_norm.bias grad norm: 0.04813319817185402\n",
            "lstm.weight_ih_l0 grad norm: 0.3027724325656891\n",
            "lstm.weight_hh_l0 grad norm: 0.08555681258440018\n",
            "lstm.bias_ih_l0 grad norm: 0.028590919449925423\n",
            "lstm.bias_hh_l0 grad norm: 0.028590919449925423\n",
            "fc.weight grad norm: 0.3377636671066284\n",
            "fc.bias grad norm: 0.07067078351974487\n",
            "[Batch 7000] Loss: 0.0714\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.01480065193027258\n",
            "c0 grad norm: 0.007965740747749805\n",
            "conv1.weight grad norm: 0.16550195217132568\n",
            "conv1.bias grad norm: 2.691987743830282e-09\n",
            "batch_norm.weight grad norm: 0.01753607951104641\n",
            "batch_norm.bias grad norm: 0.017337616533041\n",
            "lstm.weight_ih_l0 grad norm: 0.09906258434057236\n",
            "lstm.weight_hh_l0 grad norm: 0.04195456951856613\n",
            "lstm.bias_ih_l0 grad norm: 0.013527177274227142\n",
            "lstm.bias_hh_l0 grad norm: 0.013527177274227142\n",
            "fc.weight grad norm: 0.15164853632450104\n",
            "fc.bias grad norm: 0.035471152514219284\n",
            "[Batch 8000] Loss: 0.0139\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.16798530519008636\n",
            "c0 grad norm: 0.09150121361017227\n",
            "conv1.weight grad norm: 1.404417872428894\n",
            "conv1.bias grad norm: 2.4965997269532636e-08\n",
            "batch_norm.weight grad norm: 0.1662161499261856\n",
            "batch_norm.bias grad norm: 0.11656046658754349\n",
            "lstm.weight_ih_l0 grad norm: 0.6977174282073975\n",
            "lstm.weight_hh_l0 grad norm: 0.23460735380649567\n",
            "lstm.bias_ih_l0 grad norm: 0.08563783019781113\n",
            "lstm.bias_hh_l0 grad norm: 0.08563783019781113\n",
            "fc.weight grad norm: 0.879349410533905\n",
            "fc.bias grad norm: 0.10256606340408325\n",
            "[Batch 9000] Loss: 0.5399\n",
            "Total Epoch Loss: 880.2506\n",
            "\n",
            "Epoch 81\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03184615448117256\n",
            "c0 grad norm: 0.013935288414359093\n",
            "conv1.weight grad norm: 0.33492106199264526\n",
            "conv1.bias grad norm: 5.799929869709786e-09\n",
            "batch_norm.weight grad norm: 0.03621656447649002\n",
            "batch_norm.bias grad norm: 0.042001258581876755\n",
            "lstm.weight_ih_l0 grad norm: 0.1398654729127884\n",
            "lstm.weight_hh_l0 grad norm: 0.042323146015405655\n",
            "lstm.bias_ih_l0 grad norm: 0.016194051131606102\n",
            "lstm.bias_hh_l0 grad norm: 0.016194051131606102\n",
            "fc.weight grad norm: 0.19622579216957092\n",
            "fc.bias grad norm: 0.022006893530488014\n",
            "[Batch 0] Loss: 0.0205\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.05683525279164314\n",
            "c0 grad norm: 0.02387136034667492\n",
            "conv1.weight grad norm: 0.7274748086929321\n",
            "conv1.bias grad norm: 1.3511482599426472e-08\n",
            "batch_norm.weight grad norm: 0.07588455080986023\n",
            "batch_norm.bias grad norm: 0.07034439593553543\n",
            "lstm.weight_ih_l0 grad norm: 0.3109064996242523\n",
            "lstm.weight_hh_l0 grad norm: 0.10179588943719864\n",
            "lstm.bias_ih_l0 grad norm: 0.037002138793468475\n",
            "lstm.bias_hh_l0 grad norm: 0.037002138793468475\n",
            "fc.weight grad norm: 0.3833303153514862\n",
            "fc.bias grad norm: 0.09131565690040588\n",
            "[Batch 1000] Loss: 0.0976\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.0484110563993454\n",
            "c0 grad norm: 0.029367610812187195\n",
            "conv1.weight grad norm: 0.5233432054519653\n",
            "conv1.bias grad norm: 8.306916932099284e-09\n",
            "batch_norm.weight grad norm: 0.0613694041967392\n",
            "batch_norm.bias grad norm: 0.0455932654440403\n",
            "lstm.weight_ih_l0 grad norm: 0.3030799627304077\n",
            "lstm.weight_hh_l0 grad norm: 0.10065549612045288\n",
            "lstm.bias_ih_l0 grad norm: 0.027814649045467377\n",
            "lstm.bias_hh_l0 grad norm: 0.027814649045467377\n",
            "fc.weight grad norm: 0.3551665246486664\n",
            "fc.bias grad norm: 0.06600585579872131\n",
            "[Batch 2000] Loss: 0.1391\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.027663888409733772\n",
            "c0 grad norm: 0.022282063961029053\n",
            "conv1.weight grad norm: 0.42966336011886597\n",
            "conv1.bias grad norm: 9.121206900886136e-09\n",
            "batch_norm.weight grad norm: 0.0492880679666996\n",
            "batch_norm.bias grad norm: 0.04721132293343544\n",
            "lstm.weight_ih_l0 grad norm: 0.24875454604625702\n",
            "lstm.weight_hh_l0 grad norm: 0.08144967257976532\n",
            "lstm.bias_ih_l0 grad norm: 0.027817482128739357\n",
            "lstm.bias_hh_l0 grad norm: 0.027817482128739357\n",
            "fc.weight grad norm: 0.4710448384284973\n",
            "fc.bias grad norm: 0.16514161229133606\n",
            "[Batch 3000] Loss: 0.0637\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.010849671438336372\n",
            "c0 grad norm: 0.009362094104290009\n",
            "conv1.weight grad norm: 0.12634742259979248\n",
            "conv1.bias grad norm: 3.11129944030597e-09\n",
            "batch_norm.weight grad norm: 0.016462046653032303\n",
            "batch_norm.bias grad norm: 0.01054432988166809\n",
            "lstm.weight_ih_l0 grad norm: 0.07612709701061249\n",
            "lstm.weight_hh_l0 grad norm: 0.027193602174520493\n",
            "lstm.bias_ih_l0 grad norm: 0.007474222220480442\n",
            "lstm.bias_hh_l0 grad norm: 0.007474222220480442\n",
            "fc.weight grad norm: 0.17074288427829742\n",
            "fc.bias grad norm: 0.056600041687488556\n",
            "[Batch 4000] Loss: 0.0119\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.0325724296271801\n",
            "c0 grad norm: 0.03324566408991814\n",
            "conv1.weight grad norm: 0.34573712944984436\n",
            "conv1.bias grad norm: 9.826608859953012e-09\n",
            "batch_norm.weight grad norm: 0.03307779133319855\n",
            "batch_norm.bias grad norm: 0.06630078703165054\n",
            "lstm.weight_ih_l0 grad norm: 0.22435981035232544\n",
            "lstm.weight_hh_l0 grad norm: 0.10891744494438171\n",
            "lstm.bias_ih_l0 grad norm: 0.03737780451774597\n",
            "lstm.bias_hh_l0 grad norm: 0.03737780451774597\n",
            "fc.weight grad norm: 0.5049502849578857\n",
            "fc.bias grad norm: 0.21358555555343628\n",
            "[Batch 5000] Loss: 0.1176\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.13480766117572784\n",
            "c0 grad norm: 0.06695748120546341\n",
            "conv1.weight grad norm: 2.1711981296539307\n",
            "conv1.bias grad norm: 4.042935941583892e-08\n",
            "batch_norm.weight grad norm: 0.17176683247089386\n",
            "batch_norm.bias grad norm: 0.1364251673221588\n",
            "lstm.weight_ih_l0 grad norm: 0.7458193898200989\n",
            "lstm.weight_hh_l0 grad norm: 0.2010059654712677\n",
            "lstm.bias_ih_l0 grad norm: 0.0850336030125618\n",
            "lstm.bias_hh_l0 grad norm: 0.0850336030125618\n",
            "fc.weight grad norm: 0.5433357954025269\n",
            "fc.bias grad norm: 0.09315133094787598\n",
            "[Batch 6000] Loss: 0.1703\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.012539470568299294\n",
            "c0 grad norm: 0.011310510337352753\n",
            "conv1.weight grad norm: 0.23670630156993866\n",
            "conv1.bias grad norm: 3.5973430900781977e-09\n",
            "batch_norm.weight grad norm: 0.020631037652492523\n",
            "batch_norm.bias grad norm: 0.0218921210616827\n",
            "lstm.weight_ih_l0 grad norm: 0.11259674280881882\n",
            "lstm.weight_hh_l0 grad norm: 0.03994445875287056\n",
            "lstm.bias_ih_l0 grad norm: 0.011387568898499012\n",
            "lstm.bias_hh_l0 grad norm: 0.011387568898499012\n",
            "fc.weight grad norm: 0.2668950855731964\n",
            "fc.bias grad norm: 0.10218548774719238\n",
            "[Batch 7000] Loss: 0.0404\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.008062022738158703\n",
            "c0 grad norm: 0.01422122586518526\n",
            "conv1.weight grad norm: 0.20477312803268433\n",
            "conv1.bias grad norm: 2.4570796508527337e-09\n",
            "batch_norm.weight grad norm: 0.020507093518972397\n",
            "batch_norm.bias grad norm: 0.025510909035801888\n",
            "lstm.weight_ih_l0 grad norm: 0.12698961794376373\n",
            "lstm.weight_hh_l0 grad norm: 0.03898539021611214\n",
            "lstm.bias_ih_l0 grad norm: 0.013197100721299648\n",
            "lstm.bias_hh_l0 grad norm: 0.013197100721299648\n",
            "fc.weight grad norm: 0.3733554482460022\n",
            "fc.bias grad norm: 0.10350440442562103\n",
            "[Batch 8000] Loss: 0.0390\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.02136307582259178\n",
            "c0 grad norm: 0.015289438888430595\n",
            "conv1.weight grad norm: 0.3712383508682251\n",
            "conv1.bias grad norm: 6.308150446443506e-09\n",
            "batch_norm.weight grad norm: 0.02991214208304882\n",
            "batch_norm.bias grad norm: 0.02387867495417595\n",
            "lstm.weight_ih_l0 grad norm: 0.20157000422477722\n",
            "lstm.weight_hh_l0 grad norm: 0.0658753514289856\n",
            "lstm.bias_ih_l0 grad norm: 0.019205620512366295\n",
            "lstm.bias_hh_l0 grad norm: 0.019205620512366295\n",
            "fc.weight grad norm: 0.24903979897499084\n",
            "fc.bias grad norm: 0.03994568809866905\n",
            "[Batch 9000] Loss: 0.0265\n",
            "Total Epoch Loss: 877.8326\n",
            "\n",
            "Epoch 82\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.05182034894824028\n",
            "c0 grad norm: 0.02926982194185257\n",
            "conv1.weight grad norm: 0.869856595993042\n",
            "conv1.bias grad norm: 1.0998692623331863e-08\n",
            "batch_norm.weight grad norm: 0.07558091729879379\n",
            "batch_norm.bias grad norm: 0.03879344463348389\n",
            "lstm.weight_ih_l0 grad norm: 0.34750816226005554\n",
            "lstm.weight_hh_l0 grad norm: 0.08582673966884613\n",
            "lstm.bias_ih_l0 grad norm: 0.034477923065423965\n",
            "lstm.bias_hh_l0 grad norm: 0.034477923065423965\n",
            "fc.weight grad norm: 0.36729705333709717\n",
            "fc.bias grad norm: 0.09407902508974075\n",
            "[Batch 0] Loss: 0.0606\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.00785693060606718\n",
            "c0 grad norm: 0.007640864700078964\n",
            "conv1.weight grad norm: 0.10591282695531845\n",
            "conv1.bias grad norm: 2.8575020127874495e-09\n",
            "batch_norm.weight grad norm: 0.014552009291946888\n",
            "batch_norm.bias grad norm: 0.016412410885095596\n",
            "lstm.weight_ih_l0 grad norm: 0.0824643075466156\n",
            "lstm.weight_hh_l0 grad norm: 0.031563159078359604\n",
            "lstm.bias_ih_l0 grad norm: 0.008266978897154331\n",
            "lstm.bias_hh_l0 grad norm: 0.008266978897154331\n",
            "fc.weight grad norm: 0.2570876181125641\n",
            "fc.bias grad norm: 0.08222321420907974\n",
            "[Batch 1000] Loss: 0.0557\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.02017708122730255\n",
            "c0 grad norm: 0.011634883470833302\n",
            "conv1.weight grad norm: 0.21842390298843384\n",
            "conv1.bias grad norm: 3.617935506738945e-09\n",
            "batch_norm.weight grad norm: 0.031099680811166763\n",
            "batch_norm.bias grad norm: 0.03138045594096184\n",
            "lstm.weight_ih_l0 grad norm: 0.19780392944812775\n",
            "lstm.weight_hh_l0 grad norm: 0.04240460321307182\n",
            "lstm.bias_ih_l0 grad norm: 0.017121313139796257\n",
            "lstm.bias_hh_l0 grad norm: 0.017121313139796257\n",
            "fc.weight grad norm: 0.25663092732429504\n",
            "fc.bias grad norm: 0.06364938616752625\n",
            "[Batch 2000] Loss: 0.0348\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.030467260628938675\n",
            "c0 grad norm: 0.02764623612165451\n",
            "conv1.weight grad norm: 0.25122225284576416\n",
            "conv1.bias grad norm: 6.312319555945578e-09\n",
            "batch_norm.weight grad norm: 0.03224519267678261\n",
            "batch_norm.bias grad norm: 0.05298658087849617\n",
            "lstm.weight_ih_l0 grad norm: 0.16834983229637146\n",
            "lstm.weight_hh_l0 grad norm: 0.07570391148328781\n",
            "lstm.bias_ih_l0 grad norm: 0.028879722580313683\n",
            "lstm.bias_hh_l0 grad norm: 0.028879722580313683\n",
            "fc.weight grad norm: 0.3945308327674866\n",
            "fc.bias grad norm: 0.18407656252384186\n",
            "[Batch 3000] Loss: 0.0722\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.2716696262359619\n",
            "c0 grad norm: 0.04679855331778526\n",
            "conv1.weight grad norm: 2.879056215286255\n",
            "conv1.bias grad norm: 9.711273207813065e-08\n",
            "batch_norm.weight grad norm: 0.20683786273002625\n",
            "batch_norm.bias grad norm: 0.2656441926956177\n",
            "lstm.weight_ih_l0 grad norm: 1.1879360675811768\n",
            "lstm.weight_hh_l0 grad norm: 0.24767106771469116\n",
            "lstm.bias_ih_l0 grad norm: 0.12383870035409927\n",
            "lstm.bias_hh_l0 grad norm: 0.12383870035409927\n",
            "fc.weight grad norm: 0.48985862731933594\n",
            "fc.bias grad norm: 0.08513619005680084\n",
            "[Batch 4000] Loss: 0.2086\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.061901409178972244\n",
            "c0 grad norm: 0.032114360481500626\n",
            "conv1.weight grad norm: 0.5265742540359497\n",
            "conv1.bias grad norm: 1.4750149546216562e-08\n",
            "batch_norm.weight grad norm: 0.0877751037478447\n",
            "batch_norm.bias grad norm: 0.08859571069478989\n",
            "lstm.weight_ih_l0 grad norm: 0.3727196455001831\n",
            "lstm.weight_hh_l0 grad norm: 0.0852448120713234\n",
            "lstm.bias_ih_l0 grad norm: 0.03515714779496193\n",
            "lstm.bias_hh_l0 grad norm: 0.03515714779496193\n",
            "fc.weight grad norm: 0.7534341812133789\n",
            "fc.bias grad norm: 0.08786340057849884\n",
            "[Batch 5000] Loss: 0.1718\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.032171331346035004\n",
            "c0 grad norm: 0.02775363251566887\n",
            "conv1.weight grad norm: 0.5425987839698792\n",
            "conv1.bias grad norm: 8.928899397631085e-09\n",
            "batch_norm.weight grad norm: 0.036929644644260406\n",
            "batch_norm.bias grad norm: 0.04795808345079422\n",
            "lstm.weight_ih_l0 grad norm: 0.20330654084682465\n",
            "lstm.weight_hh_l0 grad norm: 0.09128623455762863\n",
            "lstm.bias_ih_l0 grad norm: 0.024872751906514168\n",
            "lstm.bias_hh_l0 grad norm: 0.024872751906514168\n",
            "fc.weight grad norm: 0.29595664143562317\n",
            "fc.bias grad norm: 0.14007562398910522\n",
            "[Batch 6000] Loss: 0.0675\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.044396836310625076\n",
            "c0 grad norm: 0.026751719415187836\n",
            "conv1.weight grad norm: 0.8773965835571289\n",
            "conv1.bias grad norm: 1.24232242271205e-08\n",
            "batch_norm.weight grad norm: 0.07421213388442993\n",
            "batch_norm.bias grad norm: 0.07701993733644485\n",
            "lstm.weight_ih_l0 grad norm: 0.49575018882751465\n",
            "lstm.weight_hh_l0 grad norm: 0.09229995310306549\n",
            "lstm.bias_ih_l0 grad norm: 0.045390620827674866\n",
            "lstm.bias_hh_l0 grad norm: 0.045390620827674866\n",
            "fc.weight grad norm: 0.5799438953399658\n",
            "fc.bias grad norm: 0.14456874132156372\n",
            "[Batch 7000] Loss: 0.1243\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.0069918860681355\n",
            "c0 grad norm: 0.012640761211514473\n",
            "conv1.weight grad norm: 0.11979317665100098\n",
            "conv1.bias grad norm: 1.787195724745061e-09\n",
            "batch_norm.weight grad norm: 0.011182768270373344\n",
            "batch_norm.bias grad norm: 0.016592396423220634\n",
            "lstm.weight_ih_l0 grad norm: 0.07651527971029282\n",
            "lstm.weight_hh_l0 grad norm: 0.03599712625145912\n",
            "lstm.bias_ih_l0 grad norm: 0.010500122793018818\n",
            "lstm.bias_hh_l0 grad norm: 0.010500122793018818\n",
            "fc.weight grad norm: 0.19189153611660004\n",
            "fc.bias grad norm: 0.0771474689245224\n",
            "[Batch 8000] Loss: 0.0211\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.07762300968170166\n",
            "c0 grad norm: 0.038243524730205536\n",
            "conv1.weight grad norm: 0.6370284557342529\n",
            "conv1.bias grad norm: 1.4453121366386767e-08\n",
            "batch_norm.weight grad norm: 0.06851581484079361\n",
            "batch_norm.bias grad norm: 0.1174916997551918\n",
            "lstm.weight_ih_l0 grad norm: 0.27916663885116577\n",
            "lstm.weight_hh_l0 grad norm: 0.14423057436943054\n",
            "lstm.bias_ih_l0 grad norm: 0.05668218806385994\n",
            "lstm.bias_hh_l0 grad norm: 0.05668218806385994\n",
            "fc.weight grad norm: 0.3633047044277191\n",
            "fc.bias grad norm: 0.18619917333126068\n",
            "[Batch 9000] Loss: 0.1289\n",
            "Total Epoch Loss: 886.2427\n",
            "\n",
            "Epoch 83\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.027018802240490913\n",
            "c0 grad norm: 0.018434718251228333\n",
            "conv1.weight grad norm: 0.40768253803253174\n",
            "conv1.bias grad norm: 8.479583257781087e-09\n",
            "batch_norm.weight grad norm: 0.03724466264247894\n",
            "batch_norm.bias grad norm: 0.042569391429424286\n",
            "lstm.weight_ih_l0 grad norm: 0.1927604228258133\n",
            "lstm.weight_hh_l0 grad norm: 0.07395631819963455\n",
            "lstm.bias_ih_l0 grad norm: 0.022851794958114624\n",
            "lstm.bias_hh_l0 grad norm: 0.022851794958114624\n",
            "fc.weight grad norm: 0.3032490909099579\n",
            "fc.bias grad norm: 0.09200528264045715\n",
            "[Batch 0] Loss: 0.0619\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.024163924157619476\n",
            "c0 grad norm: 0.049934469163417816\n",
            "conv1.weight grad norm: 0.9805619716644287\n",
            "conv1.bias grad norm: 9.557052038644542e-09\n",
            "batch_norm.weight grad norm: 0.08088424056768417\n",
            "batch_norm.bias grad norm: 0.05764990672469139\n",
            "lstm.weight_ih_l0 grad norm: 0.3489744961261749\n",
            "lstm.weight_hh_l0 grad norm: 0.1353423297405243\n",
            "lstm.bias_ih_l0 grad norm: 0.03637431189417839\n",
            "lstm.bias_hh_l0 grad norm: 0.03637431189417839\n",
            "fc.weight grad norm: 0.5078243017196655\n",
            "fc.bias grad norm: 0.12853559851646423\n",
            "[Batch 1000] Loss: 0.1728\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.04319002106785774\n",
            "c0 grad norm: 0.02741578407585621\n",
            "conv1.weight grad norm: 0.48386165499687195\n",
            "conv1.bias grad norm: 8.39752978265551e-09\n",
            "batch_norm.weight grad norm: 0.06072148308157921\n",
            "batch_norm.bias grad norm: 0.05872879922389984\n",
            "lstm.weight_ih_l0 grad norm: 0.2883891761302948\n",
            "lstm.weight_hh_l0 grad norm: 0.10639774054288864\n",
            "lstm.bias_ih_l0 grad norm: 0.030983977019786835\n",
            "lstm.bias_hh_l0 grad norm: 0.030983977019786835\n",
            "fc.weight grad norm: 0.36070549488067627\n",
            "fc.bias grad norm: 0.21777421236038208\n",
            "[Batch 2000] Loss: 0.0717\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.07178854942321777\n",
            "c0 grad norm: 0.04580286145210266\n",
            "conv1.weight grad norm: 1.6619977951049805\n",
            "conv1.bias grad norm: 3.081874666577278e-08\n",
            "batch_norm.weight grad norm: 0.13702113926410675\n",
            "batch_norm.bias grad norm: 0.09378290921449661\n",
            "lstm.weight_ih_l0 grad norm: 0.7804712057113647\n",
            "lstm.weight_hh_l0 grad norm: 0.20968790352344513\n",
            "lstm.bias_ih_l0 grad norm: 0.06320369988679886\n",
            "lstm.bias_hh_l0 grad norm: 0.06320369988679886\n",
            "fc.weight grad norm: 0.3402482867240906\n",
            "fc.bias grad norm: 0.0494200699031353\n",
            "[Batch 3000] Loss: 0.0702\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.01729971542954445\n",
            "c0 grad norm: 0.028746895492076874\n",
            "conv1.weight grad norm: 0.28409630060195923\n",
            "conv1.bias grad norm: 5.413320014469036e-09\n",
            "batch_norm.weight grad norm: 0.0270795077085495\n",
            "batch_norm.bias grad norm: 0.03194434940814972\n",
            "lstm.weight_ih_l0 grad norm: 0.1659751981496811\n",
            "lstm.weight_hh_l0 grad norm: 0.07025092095136642\n",
            "lstm.bias_ih_l0 grad norm: 0.021166272461414337\n",
            "lstm.bias_hh_l0 grad norm: 0.021166272461414337\n",
            "fc.weight grad norm: 0.3405311107635498\n",
            "fc.bias grad norm: 0.1421578824520111\n",
            "[Batch 4000] Loss: 0.0509\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.03478993475437164\n",
            "c0 grad norm: 0.020809421315789223\n",
            "conv1.weight grad norm: 0.18552418053150177\n",
            "conv1.bias grad norm: 4.456515156903151e-09\n",
            "batch_norm.weight grad norm: 0.025987403467297554\n",
            "batch_norm.bias grad norm: 0.025309449061751366\n",
            "lstm.weight_ih_l0 grad norm: 0.14280594885349274\n",
            "lstm.weight_hh_l0 grad norm: 0.04642613232135773\n",
            "lstm.bias_ih_l0 grad norm: 0.01938730850815773\n",
            "lstm.bias_hh_l0 grad norm: 0.01938730850815773\n",
            "fc.weight grad norm: 0.24491144716739655\n",
            "fc.bias grad norm: 0.10458267480134964\n",
            "[Batch 5000] Loss: 0.0282\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.03365655615925789\n",
            "c0 grad norm: 0.014932611957192421\n",
            "conv1.weight grad norm: 0.21868383884429932\n",
            "conv1.bias grad norm: 6.238458638563316e-09\n",
            "batch_norm.weight grad norm: 0.02000790275633335\n",
            "batch_norm.bias grad norm: 0.033392515033483505\n",
            "lstm.weight_ih_l0 grad norm: 0.09858950227499008\n",
            "lstm.weight_hh_l0 grad norm: 0.03900659829378128\n",
            "lstm.bias_ih_l0 grad norm: 0.017137018963694572\n",
            "lstm.bias_hh_l0 grad norm: 0.017137018963694572\n",
            "fc.weight grad norm: 0.2712223529815674\n",
            "fc.bias grad norm: 0.10428594797849655\n",
            "[Batch 6000] Loss: 0.0310\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.016890864819288254\n",
            "c0 grad norm: 0.01281189639121294\n",
            "conv1.weight grad norm: 0.18265311419963837\n",
            "conv1.bias grad norm: 3.790230795885918e-09\n",
            "batch_norm.weight grad norm: 0.018323838710784912\n",
            "batch_norm.bias grad norm: 0.016088733449578285\n",
            "lstm.weight_ih_l0 grad norm: 0.11252745985984802\n",
            "lstm.weight_hh_l0 grad norm: 0.03315962105989456\n",
            "lstm.bias_ih_l0 grad norm: 0.010819010436534882\n",
            "lstm.bias_hh_l0 grad norm: 0.010819010436534882\n",
            "fc.weight grad norm: 0.19407398998737335\n",
            "fc.bias grad norm: 0.09586802870035172\n",
            "[Batch 7000] Loss: 0.0128\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.02430601604282856\n",
            "c0 grad norm: 0.02370845153927803\n",
            "conv1.weight grad norm: 0.49430373311042786\n",
            "conv1.bias grad norm: 8.227650560854727e-09\n",
            "batch_norm.weight grad norm: 0.04773138836026192\n",
            "batch_norm.bias grad norm: 0.035844359546899796\n",
            "lstm.weight_ih_l0 grad norm: 0.24182294309139252\n",
            "lstm.weight_hh_l0 grad norm: 0.07980232685804367\n",
            "lstm.bias_ih_l0 grad norm: 0.02211437188088894\n",
            "lstm.bias_hh_l0 grad norm: 0.02211437188088894\n",
            "fc.weight grad norm: 0.4415157437324524\n",
            "fc.bias grad norm: 0.17984329164028168\n",
            "[Batch 8000] Loss: 0.1151\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.029497293755412102\n",
            "c0 grad norm: 0.019924499094486237\n",
            "conv1.weight grad norm: 0.2739092707633972\n",
            "conv1.bias grad norm: 6.140403741028422e-09\n",
            "batch_norm.weight grad norm: 0.031505849212408066\n",
            "batch_norm.bias grad norm: 0.030153341591358185\n",
            "lstm.weight_ih_l0 grad norm: 0.20044393837451935\n",
            "lstm.weight_hh_l0 grad norm: 0.052376169711351395\n",
            "lstm.bias_ih_l0 grad norm: 0.021152297034859657\n",
            "lstm.bias_hh_l0 grad norm: 0.021152297034859657\n",
            "fc.weight grad norm: 0.3014473617076874\n",
            "fc.bias grad norm: 0.07972072064876556\n",
            "[Batch 9000] Loss: 0.0317\n",
            "Total Epoch Loss: 879.8267\n",
            "\n",
            "Epoch 84\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.015201013535261154\n",
            "c0 grad norm: 0.012941946275532246\n",
            "conv1.weight grad norm: 0.17289027571678162\n",
            "conv1.bias grad norm: 6.2724252458679075e-09\n",
            "batch_norm.weight grad norm: 0.02924215979874134\n",
            "batch_norm.bias grad norm: 0.018262717872858047\n",
            "lstm.weight_ih_l0 grad norm: 0.14292073249816895\n",
            "lstm.weight_hh_l0 grad norm: 0.04175201803445816\n",
            "lstm.bias_ih_l0 grad norm: 0.012937182560563087\n",
            "lstm.bias_hh_l0 grad norm: 0.012937182560563087\n",
            "fc.weight grad norm: 0.18971312046051025\n",
            "fc.bias grad norm: 0.083249032497406\n",
            "[Batch 0] Loss: 0.0376\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.01842283084988594\n",
            "c0 grad norm: 0.01330663450062275\n",
            "conv1.weight grad norm: 0.16383986175060272\n",
            "conv1.bias grad norm: 2.8161393217374098e-09\n",
            "batch_norm.weight grad norm: 0.021625259891152382\n",
            "batch_norm.bias grad norm: 0.01539430022239685\n",
            "lstm.weight_ih_l0 grad norm: 0.10423260927200317\n",
            "lstm.weight_hh_l0 grad norm: 0.03539564833045006\n",
            "lstm.bias_ih_l0 grad norm: 0.010622694157063961\n",
            "lstm.bias_hh_l0 grad norm: 0.010622694157063961\n",
            "fc.weight grad norm: 0.2046714574098587\n",
            "fc.bias grad norm: 0.05527803674340248\n",
            "[Batch 1000] Loss: 0.0289\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.02465618960559368\n",
            "c0 grad norm: 0.01220852043479681\n",
            "conv1.weight grad norm: 0.20927980542182922\n",
            "conv1.bias grad norm: 4.8850945510992005e-09\n",
            "batch_norm.weight grad norm: 0.030459769070148468\n",
            "batch_norm.bias grad norm: 0.024113455787301064\n",
            "lstm.weight_ih_l0 grad norm: 0.16523800790309906\n",
            "lstm.weight_hh_l0 grad norm: 0.03687945008277893\n",
            "lstm.bias_ih_l0 grad norm: 0.0153606366366148\n",
            "lstm.bias_hh_l0 grad norm: 0.0153606366366148\n",
            "fc.weight grad norm: 0.17952440679073334\n",
            "fc.bias grad norm: 0.017575716599822044\n",
            "[Batch 2000] Loss: 0.0267\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.00721852807328105\n",
            "c0 grad norm: 0.008834215812385082\n",
            "conv1.weight grad norm: 0.15279076993465424\n",
            "conv1.bias grad norm: 2.3218102995770096e-09\n",
            "batch_norm.weight grad norm: 0.016497381031513214\n",
            "batch_norm.bias grad norm: 0.014916285872459412\n",
            "lstm.weight_ih_l0 grad norm: 0.06491976231336594\n",
            "lstm.weight_hh_l0 grad norm: 0.017847031354904175\n",
            "lstm.bias_ih_l0 grad norm: 0.0066367932595312595\n",
            "lstm.bias_hh_l0 grad norm: 0.0066367932595312595\n",
            "fc.weight grad norm: 0.15765047073364258\n",
            "fc.bias grad norm: 0.041728075593709946\n",
            "[Batch 3000] Loss: 0.0147\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.004838180728256702\n",
            "c0 grad norm: 0.0038171233609318733\n",
            "conv1.weight grad norm: 0.0830012708902359\n",
            "conv1.bias grad norm: 1.3703747026028168e-09\n",
            "batch_norm.weight grad norm: 0.008999780751764774\n",
            "batch_norm.bias grad norm: 0.007585161831229925\n",
            "lstm.weight_ih_l0 grad norm: 0.043150562793016434\n",
            "lstm.weight_hh_l0 grad norm: 0.017988374456763268\n",
            "lstm.bias_ih_l0 grad norm: 0.00478437589481473\n",
            "lstm.bias_hh_l0 grad norm: 0.00478437589481473\n",
            "fc.weight grad norm: 0.15686574578285217\n",
            "fc.bias grad norm: 0.07823420315980911\n",
            "[Batch 4000] Loss: 0.0111\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.057900410145521164\n",
            "c0 grad norm: 0.03471417725086212\n",
            "conv1.weight grad norm: 0.7751457691192627\n",
            "conv1.bias grad norm: 8.196821887906935e-09\n",
            "batch_norm.weight grad norm: 0.07702776789665222\n",
            "batch_norm.bias grad norm: 0.05721519887447357\n",
            "lstm.weight_ih_l0 grad norm: 0.33714208006858826\n",
            "lstm.weight_hh_l0 grad norm: 0.0776066929101944\n",
            "lstm.bias_ih_l0 grad norm: 0.03572104498744011\n",
            "lstm.bias_hh_l0 grad norm: 0.03572104498744011\n",
            "fc.weight grad norm: 0.4190431535243988\n",
            "fc.bias grad norm: 0.17459000647068024\n",
            "[Batch 5000] Loss: 0.0815\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.01995883509516716\n",
            "c0 grad norm: 0.015467965044081211\n",
            "conv1.weight grad norm: 0.3597540557384491\n",
            "conv1.bias grad norm: 4.153802191098066e-09\n",
            "batch_norm.weight grad norm: 0.0325290746986866\n",
            "batch_norm.bias grad norm: 0.02176833339035511\n",
            "lstm.weight_ih_l0 grad norm: 0.17415779829025269\n",
            "lstm.weight_hh_l0 grad norm: 0.05157038941979408\n",
            "lstm.bias_ih_l0 grad norm: 0.01672024093568325\n",
            "lstm.bias_hh_l0 grad norm: 0.01672024093568325\n",
            "fc.weight grad norm: 0.2634942829608917\n",
            "fc.bias grad norm: 0.04138602688908577\n",
            "[Batch 6000] Loss: 0.1020\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.031080102548003197\n",
            "c0 grad norm: 0.013914724811911583\n",
            "conv1.weight grad norm: 0.46577611565589905\n",
            "conv1.bias grad norm: 4.735981828929425e-09\n",
            "batch_norm.weight grad norm: 0.03226153180003166\n",
            "batch_norm.bias grad norm: 0.025820733979344368\n",
            "lstm.weight_ih_l0 grad norm: 0.14940086007118225\n",
            "lstm.weight_hh_l0 grad norm: 0.03783787786960602\n",
            "lstm.bias_ih_l0 grad norm: 0.014965583570301533\n",
            "lstm.bias_hh_l0 grad norm: 0.014965583570301533\n",
            "fc.weight grad norm: 0.27225130796432495\n",
            "fc.bias grad norm: 0.101162388920784\n",
            "[Batch 7000] Loss: 0.0284\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.02851175330579281\n",
            "c0 grad norm: 0.019242463633418083\n",
            "conv1.weight grad norm: 0.3458245098590851\n",
            "conv1.bias grad norm: 5.627129429086608e-09\n",
            "batch_norm.weight grad norm: 0.03741985931992531\n",
            "batch_norm.bias grad norm: 0.03992839902639389\n",
            "lstm.weight_ih_l0 grad norm: 0.22440914809703827\n",
            "lstm.weight_hh_l0 grad norm: 0.05643286556005478\n",
            "lstm.bias_ih_l0 grad norm: 0.02177964523434639\n",
            "lstm.bias_hh_l0 grad norm: 0.02177964523434639\n",
            "fc.weight grad norm: 0.3185577988624573\n",
            "fc.bias grad norm: 0.08686856925487518\n",
            "[Batch 8000] Loss: 0.0969\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.01026721578091383\n",
            "c0 grad norm: 0.014784223400056362\n",
            "conv1.weight grad norm: 0.1807527095079422\n",
            "conv1.bias grad norm: 2.5889290711234025e-09\n",
            "batch_norm.weight grad norm: 0.02017712965607643\n",
            "batch_norm.bias grad norm: 0.02298673428595066\n",
            "lstm.weight_ih_l0 grad norm: 0.12842629849910736\n",
            "lstm.weight_hh_l0 grad norm: 0.040787357836961746\n",
            "lstm.bias_ih_l0 grad norm: 0.012368597090244293\n",
            "lstm.bias_hh_l0 grad norm: 0.012368597090244293\n",
            "fc.weight grad norm: 0.3988073170185089\n",
            "fc.bias grad norm: 0.08559536188840866\n",
            "[Batch 9000] Loss: 0.0595\n",
            "Total Epoch Loss: 880.8575\n",
            "\n",
            "Epoch 85\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.04564528539776802\n",
            "c0 grad norm: 0.03529060631990433\n",
            "conv1.weight grad norm: 0.3623935580253601\n",
            "conv1.bias grad norm: 7.755665443198723e-09\n",
            "batch_norm.weight grad norm: 0.04395605996251106\n",
            "batch_norm.bias grad norm: 0.03322703391313553\n",
            "lstm.weight_ih_l0 grad norm: 0.24962526559829712\n",
            "lstm.weight_hh_l0 grad norm: 0.08153899759054184\n",
            "lstm.bias_ih_l0 grad norm: 0.02382076159119606\n",
            "lstm.bias_hh_l0 grad norm: 0.02382076159119606\n",
            "fc.weight grad norm: 0.3342736065387726\n",
            "fc.bias grad norm: 0.05409431457519531\n",
            "[Batch 0] Loss: 0.0507\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.011807967908680439\n",
            "c0 grad norm: 0.010232833214104176\n",
            "conv1.weight grad norm: 0.12219071388244629\n",
            "conv1.bias grad norm: 4.281200727263013e-09\n",
            "batch_norm.weight grad norm: 0.015321734361350536\n",
            "batch_norm.bias grad norm: 0.016025684773921967\n",
            "lstm.weight_ih_l0 grad norm: 0.09512734413146973\n",
            "lstm.weight_hh_l0 grad norm: 0.030613917857408524\n",
            "lstm.bias_ih_l0 grad norm: 0.009691950865089893\n",
            "lstm.bias_hh_l0 grad norm: 0.009691950865089893\n",
            "fc.weight grad norm: 0.20809175074100494\n",
            "fc.bias grad norm: 0.09511600434780121\n",
            "[Batch 1000] Loss: 0.0218\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.06980816274881363\n",
            "c0 grad norm: 0.027035588398575783\n",
            "conv1.weight grad norm: 0.6586229205131531\n",
            "conv1.bias grad norm: 1.890859557818203e-08\n",
            "batch_norm.weight grad norm: 0.0822574645280838\n",
            "batch_norm.bias grad norm: 0.10036209225654602\n",
            "lstm.weight_ih_l0 grad norm: 0.3247390687465668\n",
            "lstm.weight_hh_l0 grad norm: 0.07868878543376923\n",
            "lstm.bias_ih_l0 grad norm: 0.03480713441967964\n",
            "lstm.bias_hh_l0 grad norm: 0.03480713441967964\n",
            "fc.weight grad norm: 0.35670149326324463\n",
            "fc.bias grad norm: 0.10193171352148056\n",
            "[Batch 2000] Loss: 0.0882\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.008488337509334087\n",
            "c0 grad norm: 0.011090019717812538\n",
            "conv1.weight grad norm: 0.11457189172506332\n",
            "conv1.bias grad norm: 2.8351616609967323e-09\n",
            "batch_norm.weight grad norm: 0.015727266669273376\n",
            "batch_norm.bias grad norm: 0.014335723593831062\n",
            "lstm.weight_ih_l0 grad norm: 0.0910801887512207\n",
            "lstm.weight_hh_l0 grad norm: 0.03314999118447304\n",
            "lstm.bias_ih_l0 grad norm: 0.009350667707622051\n",
            "lstm.bias_hh_l0 grad norm: 0.009350667707622051\n",
            "fc.weight grad norm: 0.23732587695121765\n",
            "fc.bias grad norm: 0.12854057550430298\n",
            "[Batch 3000] Loss: 0.0609\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.024090392515063286\n",
            "c0 grad norm: 0.021285584196448326\n",
            "conv1.weight grad norm: 0.4489912688732147\n",
            "conv1.bias grad norm: 8.445403487655767e-09\n",
            "batch_norm.weight grad norm: 0.04013464227318764\n",
            "batch_norm.bias grad norm: 0.036542922258377075\n",
            "lstm.weight_ih_l0 grad norm: 0.21336670219898224\n",
            "lstm.weight_hh_l0 grad norm: 0.0557742677628994\n",
            "lstm.bias_ih_l0 grad norm: 0.0228606965392828\n",
            "lstm.bias_hh_l0 grad norm: 0.0228606965392828\n",
            "fc.weight grad norm: 0.38847771286964417\n",
            "fc.bias grad norm: 0.1695677489042282\n",
            "[Batch 4000] Loss: 0.0568\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.01763339154422283\n",
            "c0 grad norm: 0.010868394747376442\n",
            "conv1.weight grad norm: 0.11101510375738144\n",
            "conv1.bias grad norm: 1.838243890439628e-09\n",
            "batch_norm.weight grad norm: 0.009783490560948849\n",
            "batch_norm.bias grad norm: 0.009361359290778637\n",
            "lstm.weight_ih_l0 grad norm: 0.07115987688302994\n",
            "lstm.weight_hh_l0 grad norm: 0.029693247750401497\n",
            "lstm.bias_ih_l0 grad norm: 0.009654846042394638\n",
            "lstm.bias_hh_l0 grad norm: 0.009654846042394638\n",
            "fc.weight grad norm: 0.21255093812942505\n",
            "fc.bias grad norm: 0.030794542282819748\n",
            "[Batch 5000] Loss: 0.0189\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.017933938652276993\n",
            "c0 grad norm: 0.012685264460742474\n",
            "conv1.weight grad norm: 0.16174110770225525\n",
            "conv1.bias grad norm: 2.0212476137260182e-09\n",
            "batch_norm.weight grad norm: 0.017681607976555824\n",
            "batch_norm.bias grad norm: 0.01688849925994873\n",
            "lstm.weight_ih_l0 grad norm: 0.07962092757225037\n",
            "lstm.weight_hh_l0 grad norm: 0.027474170550704002\n",
            "lstm.bias_ih_l0 grad norm: 0.009957344271242619\n",
            "lstm.bias_hh_l0 grad norm: 0.009957344271242619\n",
            "fc.weight grad norm: 0.19578424096107483\n",
            "fc.bias grad norm: 0.04808759316802025\n",
            "[Batch 6000] Loss: 0.0237\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.05297110602259636\n",
            "c0 grad norm: 0.0270003080368042\n",
            "conv1.weight grad norm: 0.8641384840011597\n",
            "conv1.bias grad norm: 1.2678042615732465e-08\n",
            "batch_norm.weight grad norm: 0.07590661197900772\n",
            "batch_norm.bias grad norm: 0.040491748601198196\n",
            "lstm.weight_ih_l0 grad norm: 0.26273486018180847\n",
            "lstm.weight_hh_l0 grad norm: 0.05596334487199783\n",
            "lstm.bias_ih_l0 grad norm: 0.023519687354564667\n",
            "lstm.bias_hh_l0 grad norm: 0.023519687354564667\n",
            "fc.weight grad norm: 0.3250449597835541\n",
            "fc.bias grad norm: 0.09281504154205322\n",
            "[Batch 7000] Loss: 0.0358\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.015850039198994637\n",
            "c0 grad norm: 0.008081045933067799\n",
            "conv1.weight grad norm: 0.1106981560587883\n",
            "conv1.bias grad norm: 1.8821066927188213e-09\n",
            "batch_norm.weight grad norm: 0.013387792743742466\n",
            "batch_norm.bias grad norm: 0.01764032617211342\n",
            "lstm.weight_ih_l0 grad norm: 0.06810836493968964\n",
            "lstm.weight_hh_l0 grad norm: 0.022224215790629387\n",
            "lstm.bias_ih_l0 grad norm: 0.009786631911993027\n",
            "lstm.bias_hh_l0 grad norm: 0.009786631911993027\n",
            "fc.weight grad norm: 0.1990644633769989\n",
            "fc.bias grad norm: 0.0679096207022667\n",
            "[Batch 8000] Loss: 0.0167\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.057406194508075714\n",
            "c0 grad norm: 0.04147351533174515\n",
            "conv1.weight grad norm: 0.8602311611175537\n",
            "conv1.bias grad norm: 1.4070996812165504e-08\n",
            "batch_norm.weight grad norm: 0.088569276034832\n",
            "batch_norm.bias grad norm: 0.09019462764263153\n",
            "lstm.weight_ih_l0 grad norm: 0.5215176939964294\n",
            "lstm.weight_hh_l0 grad norm: 0.17029985785484314\n",
            "lstm.bias_ih_l0 grad norm: 0.04964856058359146\n",
            "lstm.bias_hh_l0 grad norm: 0.04964856058359146\n",
            "fc.weight grad norm: 0.9251818060874939\n",
            "fc.bias grad norm: 0.23942473530769348\n",
            "[Batch 9000] Loss: 0.3413\n",
            "Total Epoch Loss: 879.4818\n",
            "\n",
            "Epoch 86\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.019395379349589348\n",
            "c0 grad norm: 0.023751290515065193\n",
            "conv1.weight grad norm: 0.415244996547699\n",
            "conv1.bias grad norm: 3.4945444316036856e-09\n",
            "batch_norm.weight grad norm: 0.04001383110880852\n",
            "batch_norm.bias grad norm: 0.051248449832201004\n",
            "lstm.weight_ih_l0 grad norm: 0.1994883120059967\n",
            "lstm.weight_hh_l0 grad norm: 0.07652754336595535\n",
            "lstm.bias_ih_l0 grad norm: 0.02349984645843506\n",
            "lstm.bias_hh_l0 grad norm: 0.02349984645843506\n",
            "fc.weight grad norm: 0.41224411129951477\n",
            "fc.bias grad norm: 0.05904781073331833\n",
            "[Batch 0] Loss: 0.1131\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.044078752398490906\n",
            "c0 grad norm: 0.03927866742014885\n",
            "conv1.weight grad norm: 0.8064765930175781\n",
            "conv1.bias grad norm: 1.0962220464705297e-08\n",
            "batch_norm.weight grad norm: 0.06861512362957001\n",
            "batch_norm.bias grad norm: 0.05643743649125099\n",
            "lstm.weight_ih_l0 grad norm: 0.37436699867248535\n",
            "lstm.weight_hh_l0 grad norm: 0.1356838047504425\n",
            "lstm.bias_ih_l0 grad norm: 0.03950250521302223\n",
            "lstm.bias_hh_l0 grad norm: 0.03950250521302223\n",
            "fc.weight grad norm: 0.40515241026878357\n",
            "fc.bias grad norm: 0.18829840421676636\n",
            "[Batch 1000] Loss: 0.1580\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.022941166535019875\n",
            "c0 grad norm: 0.023975620046257973\n",
            "conv1.weight grad norm: 0.2309122383594513\n",
            "conv1.bias grad norm: 3.370899115395787e-09\n",
            "batch_norm.weight grad norm: 0.025462167337536812\n",
            "batch_norm.bias grad norm: 0.024084582924842834\n",
            "lstm.weight_ih_l0 grad norm: 0.15693099796772003\n",
            "lstm.weight_hh_l0 grad norm: 0.049500077962875366\n",
            "lstm.bias_ih_l0 grad norm: 0.017215220257639885\n",
            "lstm.bias_hh_l0 grad norm: 0.017215220257639885\n",
            "fc.weight grad norm: 0.26022183895111084\n",
            "fc.bias grad norm: 0.033043280243873596\n",
            "[Batch 2000] Loss: 0.0702\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.03542483225464821\n",
            "c0 grad norm: 0.026529032737016678\n",
            "conv1.weight grad norm: 0.5266062021255493\n",
            "conv1.bias grad norm: 1.1401112942621694e-08\n",
            "batch_norm.weight grad norm: 0.04902658984065056\n",
            "batch_norm.bias grad norm: 0.03841981291770935\n",
            "lstm.weight_ih_l0 grad norm: 0.29071080684661865\n",
            "lstm.weight_hh_l0 grad norm: 0.08744601905345917\n",
            "lstm.bias_ih_l0 grad norm: 0.026670396327972412\n",
            "lstm.bias_hh_l0 grad norm: 0.026670396327972412\n",
            "fc.weight grad norm: 0.3134920001029968\n",
            "fc.bias grad norm: 0.13339586555957794\n",
            "[Batch 3000] Loss: 0.0508\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.054215312004089355\n",
            "c0 grad norm: 0.03493849188089371\n",
            "conv1.weight grad norm: 0.5993244647979736\n",
            "conv1.bias grad norm: 1.1875298966401715e-08\n",
            "batch_norm.weight grad norm: 0.0681435689330101\n",
            "batch_norm.bias grad norm: 0.06720218062400818\n",
            "lstm.weight_ih_l0 grad norm: 0.3175942599773407\n",
            "lstm.weight_hh_l0 grad norm: 0.12250135093927383\n",
            "lstm.bias_ih_l0 grad norm: 0.04012994095683098\n",
            "lstm.bias_hh_l0 grad norm: 0.04012994095683098\n",
            "fc.weight grad norm: 0.5553451180458069\n",
            "fc.bias grad norm: 0.23016619682312012\n",
            "[Batch 4000] Loss: 0.1317\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.024951623752713203\n",
            "c0 grad norm: 0.02269292064011097\n",
            "conv1.weight grad norm: 0.22927866876125336\n",
            "conv1.bias grad norm: 4.188971391982932e-09\n",
            "batch_norm.weight grad norm: 0.03601805120706558\n",
            "batch_norm.bias grad norm: 0.02559695579111576\n",
            "lstm.weight_ih_l0 grad norm: 0.16414889693260193\n",
            "lstm.weight_hh_l0 grad norm: 0.0572054386138916\n",
            "lstm.bias_ih_l0 grad norm: 0.01856924034655094\n",
            "lstm.bias_hh_l0 grad norm: 0.01856924034655094\n",
            "fc.weight grad norm: 0.4511188864707947\n",
            "fc.bias grad norm: 0.06180500611662865\n",
            "[Batch 5000] Loss: 0.0474\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.01797635480761528\n",
            "c0 grad norm: 0.018601341173052788\n",
            "conv1.weight grad norm: 0.27364277839660645\n",
            "conv1.bias grad norm: 4.1002716777427395e-09\n",
            "batch_norm.weight grad norm: 0.025126975029706955\n",
            "batch_norm.bias grad norm: 0.020909886807203293\n",
            "lstm.weight_ih_l0 grad norm: 0.1325841099023819\n",
            "lstm.weight_hh_l0 grad norm: 0.041626717895269394\n",
            "lstm.bias_ih_l0 grad norm: 0.01393591333180666\n",
            "lstm.bias_hh_l0 grad norm: 0.01393591333180666\n",
            "fc.weight grad norm: 0.15646657347679138\n",
            "fc.bias grad norm: 0.054710909724235535\n",
            "[Batch 6000] Loss: 0.0166\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.012961548753082752\n",
            "c0 grad norm: 0.012722041457891464\n",
            "conv1.weight grad norm: 0.19943569600582123\n",
            "conv1.bias grad norm: 3.744340837386062e-09\n",
            "batch_norm.weight grad norm: 0.01984054408967495\n",
            "batch_norm.bias grad norm: 0.022165408357977867\n",
            "lstm.weight_ih_l0 grad norm: 0.12231358140707016\n",
            "lstm.weight_hh_l0 grad norm: 0.042629703879356384\n",
            "lstm.bias_ih_l0 grad norm: 0.012647885829210281\n",
            "lstm.bias_hh_l0 grad norm: 0.012647885829210281\n",
            "fc.weight grad norm: 0.31484103202819824\n",
            "fc.bias grad norm: 0.12877710163593292\n",
            "[Batch 7000] Loss: 0.0382\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.023545073345303535\n",
            "c0 grad norm: 0.024878252297639847\n",
            "conv1.weight grad norm: 0.38552507758140564\n",
            "conv1.bias grad norm: 7.642202426438871e-09\n",
            "batch_norm.weight grad norm: 0.02744697779417038\n",
            "batch_norm.bias grad norm: 0.037311382591724396\n",
            "lstm.weight_ih_l0 grad norm: 0.19585469365119934\n",
            "lstm.weight_hh_l0 grad norm: 0.05331508070230484\n",
            "lstm.bias_ih_l0 grad norm: 0.01730109564960003\n",
            "lstm.bias_hh_l0 grad norm: 0.01730109564960003\n",
            "fc.weight grad norm: 0.45788469910621643\n",
            "fc.bias grad norm: 0.19468049705028534\n",
            "[Batch 8000] Loss: 0.0714\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.017704876139760017\n",
            "c0 grad norm: 0.017567772418260574\n",
            "conv1.weight grad norm: 0.36804574728012085\n",
            "conv1.bias grad norm: 4.923151664115721e-09\n",
            "batch_norm.weight grad norm: 0.032530076801776886\n",
            "batch_norm.bias grad norm: 0.028326403349637985\n",
            "lstm.weight_ih_l0 grad norm: 0.2379089891910553\n",
            "lstm.weight_hh_l0 grad norm: 0.09151922911405563\n",
            "lstm.bias_ih_l0 grad norm: 0.026248332113027573\n",
            "lstm.bias_hh_l0 grad norm: 0.026248332113027573\n",
            "fc.weight grad norm: 0.24864432215690613\n",
            "fc.bias grad norm: 0.0948040783405304\n",
            "[Batch 9000] Loss: 0.0503\n",
            "Total Epoch Loss: 879.8268\n",
            "\n",
            "Epoch 87\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.007759341038763523\n",
            "c0 grad norm: 0.004485985264182091\n",
            "conv1.weight grad norm: 0.07211780548095703\n",
            "conv1.bias grad norm: 1.097105184477698e-09\n",
            "batch_norm.weight grad norm: 0.007861512713134289\n",
            "batch_norm.bias grad norm: 0.0072824484668672085\n",
            "lstm.weight_ih_l0 grad norm: 0.05324464291334152\n",
            "lstm.weight_hh_l0 grad norm: 0.014613340608775616\n",
            "lstm.bias_ih_l0 grad norm: 0.0054583982564508915\n",
            "lstm.bias_hh_l0 grad norm: 0.0054583982564508915\n",
            "fc.weight grad norm: 0.23651863634586334\n",
            "fc.bias grad norm: 0.06850047409534454\n",
            "[Batch 0] Loss: 0.0216\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.05556560680270195\n",
            "c0 grad norm: 0.0253871101886034\n",
            "conv1.weight grad norm: 0.8685310482978821\n",
            "conv1.bias grad norm: 1.1476835481971648e-08\n",
            "batch_norm.weight grad norm: 0.07563833892345428\n",
            "batch_norm.bias grad norm: 0.06732018291950226\n",
            "lstm.weight_ih_l0 grad norm: 0.27662888169288635\n",
            "lstm.weight_hh_l0 grad norm: 0.06477634608745575\n",
            "lstm.bias_ih_l0 grad norm: 0.029286840930581093\n",
            "lstm.bias_hh_l0 grad norm: 0.029286840930581093\n",
            "fc.weight grad norm: 0.27238592505455017\n",
            "fc.bias grad norm: 0.05713044852018356\n",
            "[Batch 1000] Loss: 0.0362\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.027128195390105247\n",
            "c0 grad norm: 0.034254759550094604\n",
            "conv1.weight grad norm: 0.5254703164100647\n",
            "conv1.bias grad norm: 7.246809818894917e-09\n",
            "batch_norm.weight grad norm: 0.05575243756175041\n",
            "batch_norm.bias grad norm: 0.04621611908078194\n",
            "lstm.weight_ih_l0 grad norm: 0.26128965616226196\n",
            "lstm.weight_hh_l0 grad norm: 0.07602254301309586\n",
            "lstm.bias_ih_l0 grad norm: 0.02598927728831768\n",
            "lstm.bias_hh_l0 grad norm: 0.02598927728831768\n",
            "fc.weight grad norm: 0.271271675825119\n",
            "fc.bias grad norm: 0.059451907873153687\n",
            "[Batch 2000] Loss: 0.0682\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.04229525849223137\n",
            "c0 grad norm: 0.06688392162322998\n",
            "conv1.weight grad norm: 0.5763773322105408\n",
            "conv1.bias grad norm: 1.2460803056058012e-08\n",
            "batch_norm.weight grad norm: 0.09410928934812546\n",
            "batch_norm.bias grad norm: 0.08977629244327545\n",
            "lstm.weight_ih_l0 grad norm: 0.5535955429077148\n",
            "lstm.weight_hh_l0 grad norm: 0.16061069071292877\n",
            "lstm.bias_ih_l0 grad norm: 0.058802347630262375\n",
            "lstm.bias_hh_l0 grad norm: 0.058802347630262375\n",
            "fc.weight grad norm: 0.8492757678031921\n",
            "fc.bias grad norm: 0.30423036217689514\n",
            "[Batch 3000] Loss: 0.3167\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.16593554615974426\n",
            "c0 grad norm: 0.05925692617893219\n",
            "conv1.weight grad norm: 1.3782625198364258\n",
            "conv1.bias grad norm: 2.699430545760606e-08\n",
            "batch_norm.weight grad norm: 0.15187108516693115\n",
            "batch_norm.bias grad norm: 0.152471661567688\n",
            "lstm.weight_ih_l0 grad norm: 0.8029134273529053\n",
            "lstm.weight_hh_l0 grad norm: 0.19173407554626465\n",
            "lstm.bias_ih_l0 grad norm: 0.08125800639390945\n",
            "lstm.bias_hh_l0 grad norm: 0.08125800639390945\n",
            "fc.weight grad norm: 0.7574686408042908\n",
            "fc.bias grad norm: 0.1938016414642334\n",
            "[Batch 4000] Loss: 0.3781\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.02274549938738346\n",
            "c0 grad norm: 0.03168037161231041\n",
            "conv1.weight grad norm: 0.28450068831443787\n",
            "conv1.bias grad norm: 6.523253492929371e-09\n",
            "batch_norm.weight grad norm: 0.03709273785352707\n",
            "batch_norm.bias grad norm: 0.039256751537323\n",
            "lstm.weight_ih_l0 grad norm: 0.18137238919734955\n",
            "lstm.weight_hh_l0 grad norm: 0.0719970166683197\n",
            "lstm.bias_ih_l0 grad norm: 0.024494338780641556\n",
            "lstm.bias_hh_l0 grad norm: 0.024494338780641556\n",
            "fc.weight grad norm: 0.3526631295681\n",
            "fc.bias grad norm: 0.13213901221752167\n",
            "[Batch 5000] Loss: 0.0526\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.03563176840543747\n",
            "c0 grad norm: 0.026765871793031693\n",
            "conv1.weight grad norm: 0.3090774416923523\n",
            "conv1.bias grad norm: 6.446068123722171e-09\n",
            "batch_norm.weight grad norm: 0.040794745087623596\n",
            "batch_norm.bias grad norm: 0.02271684817969799\n",
            "lstm.weight_ih_l0 grad norm: 0.22817610204219818\n",
            "lstm.weight_hh_l0 grad norm: 0.06846463680267334\n",
            "lstm.bias_ih_l0 grad norm: 0.01798933371901512\n",
            "lstm.bias_hh_l0 grad norm: 0.01798933371901512\n",
            "fc.weight grad norm: 0.33772486448287964\n",
            "fc.bias grad norm: 0.06630247831344604\n",
            "[Batch 6000] Loss: 0.0506\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.013671480119228363\n",
            "c0 grad norm: 0.018736960366368294\n",
            "conv1.weight grad norm: 0.10948086529970169\n",
            "conv1.bias grad norm: 2.846616498075605e-09\n",
            "batch_norm.weight grad norm: 0.014605767093598843\n",
            "batch_norm.bias grad norm: 0.013504497706890106\n",
            "lstm.weight_ih_l0 grad norm: 0.08607873320579529\n",
            "lstm.weight_hh_l0 grad norm: 0.030126990750432014\n",
            "lstm.bias_ih_l0 grad norm: 0.008821327239274979\n",
            "lstm.bias_hh_l0 grad norm: 0.008821327239274979\n",
            "fc.weight grad norm: 0.20420213043689728\n",
            "fc.bias grad norm: 0.06582767516374588\n",
            "[Batch 7000] Loss: 0.0136\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.012341583147644997\n",
            "c0 grad norm: 0.008122066035866737\n",
            "conv1.weight grad norm: 0.17223121225833893\n",
            "conv1.bias grad norm: 3.3662019838232027e-09\n",
            "batch_norm.weight grad norm: 0.014908592216670513\n",
            "batch_norm.bias grad norm: 0.015668144449591637\n",
            "lstm.weight_ih_l0 grad norm: 0.0762578696012497\n",
            "lstm.weight_hh_l0 grad norm: 0.025250228121876717\n",
            "lstm.bias_ih_l0 grad norm: 0.007793205324560404\n",
            "lstm.bias_hh_l0 grad norm: 0.007793205324560404\n",
            "fc.weight grad norm: 0.1794823855161667\n",
            "fc.bias grad norm: 0.025514332577586174\n",
            "[Batch 8000] Loss: 0.0212\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.022304866462945938\n",
            "c0 grad norm: 0.02428707666695118\n",
            "conv1.weight grad norm: 0.2792406678199768\n",
            "conv1.bias grad norm: 1.5412760845379125e-08\n",
            "batch_norm.weight grad norm: 0.027897732332348824\n",
            "batch_norm.bias grad norm: 0.02754649519920349\n",
            "lstm.weight_ih_l0 grad norm: 0.12086984515190125\n",
            "lstm.weight_hh_l0 grad norm: 0.048542581498622894\n",
            "lstm.bias_ih_l0 grad norm: 0.015602239407598972\n",
            "lstm.bias_hh_l0 grad norm: 0.015602239407598972\n",
            "fc.weight grad norm: 0.24125301837921143\n",
            "fc.bias grad norm: 0.10057104378938675\n",
            "[Batch 9000] Loss: 0.1594\n",
            "Total Epoch Loss: 881.6232\n",
            "\n",
            "Epoch 88\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.02969225123524666\n",
            "c0 grad norm: 0.011744922958314419\n",
            "conv1.weight grad norm: 0.31898894906044006\n",
            "conv1.bias grad norm: 8.61430748955172e-09\n",
            "batch_norm.weight grad norm: 0.0511040985584259\n",
            "batch_norm.bias grad norm: 0.03633967414498329\n",
            "lstm.weight_ih_l0 grad norm: 0.3580601215362549\n",
            "lstm.weight_hh_l0 grad norm: 0.07599884271621704\n",
            "lstm.bias_ih_l0 grad norm: 0.026370666921138763\n",
            "lstm.bias_hh_l0 grad norm: 0.026370666921138763\n",
            "fc.weight grad norm: 0.3014158606529236\n",
            "fc.bias grad norm: 0.07279693335294724\n",
            "[Batch 0] Loss: 0.0365\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.03826368972659111\n",
            "c0 grad norm: 0.03417717665433884\n",
            "conv1.weight grad norm: 0.38428694009780884\n",
            "conv1.bias grad norm: 7.43727524010751e-09\n",
            "batch_norm.weight grad norm: 0.04292546212673187\n",
            "batch_norm.bias grad norm: 0.05038444697856903\n",
            "lstm.weight_ih_l0 grad norm: 0.26480403542518616\n",
            "lstm.weight_hh_l0 grad norm: 0.08906751871109009\n",
            "lstm.bias_ih_l0 grad norm: 0.03385505452752113\n",
            "lstm.bias_hh_l0 grad norm: 0.03385505452752113\n",
            "fc.weight grad norm: 0.6824353933334351\n",
            "fc.bias grad norm: 0.18936291337013245\n",
            "[Batch 1000] Loss: 0.1311\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.009778766892850399\n",
            "c0 grad norm: 0.012124838307499886\n",
            "conv1.weight grad norm: 0.1979178488254547\n",
            "conv1.bias grad norm: 5.591450413788834e-09\n",
            "batch_norm.weight grad norm: 0.026488877832889557\n",
            "batch_norm.bias grad norm: 0.01737956516444683\n",
            "lstm.weight_ih_l0 grad norm: 0.1260375827550888\n",
            "lstm.weight_hh_l0 grad norm: 0.041754189878702164\n",
            "lstm.bias_ih_l0 grad norm: 0.011968606151640415\n",
            "lstm.bias_hh_l0 grad norm: 0.011968606151640415\n",
            "fc.weight grad norm: 0.22937531769275665\n",
            "fc.bias grad norm: 0.0448925644159317\n",
            "[Batch 2000] Loss: 0.0386\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.06445909291505814\n",
            "c0 grad norm: 0.05664403364062309\n",
            "conv1.weight grad norm: 1.0021802186965942\n",
            "conv1.bias grad norm: 1.7804122620646012e-08\n",
            "batch_norm.weight grad norm: 0.10967148095369339\n",
            "batch_norm.bias grad norm: 0.07128933817148209\n",
            "lstm.weight_ih_l0 grad norm: 0.5242136120796204\n",
            "lstm.weight_hh_l0 grad norm: 0.15332521498203278\n",
            "lstm.bias_ih_l0 grad norm: 0.03859741985797882\n",
            "lstm.bias_hh_l0 grad norm: 0.03859741985797882\n",
            "fc.weight grad norm: 0.4342363476753235\n",
            "fc.bias grad norm: 0.0897279679775238\n",
            "[Batch 3000] Loss: 0.1847\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.05995512008666992\n",
            "c0 grad norm: 0.05933177471160889\n",
            "conv1.weight grad norm: 0.9463597536087036\n",
            "conv1.bias grad norm: 1.7223152681822285e-08\n",
            "batch_norm.weight grad norm: 0.08067872375249863\n",
            "batch_norm.bias grad norm: 0.07435525953769684\n",
            "lstm.weight_ih_l0 grad norm: 0.5683897733688354\n",
            "lstm.weight_hh_l0 grad norm: 0.1948372721672058\n",
            "lstm.bias_ih_l0 grad norm: 0.06332200765609741\n",
            "lstm.bias_hh_l0 grad norm: 0.06332200765609741\n",
            "fc.weight grad norm: 0.7465609312057495\n",
            "fc.bias grad norm: 0.2802668511867523\n",
            "[Batch 4000] Loss: 0.3606\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.013863789848983288\n",
            "c0 grad norm: 0.011673558503389359\n",
            "conv1.weight grad norm: 0.20040664076805115\n",
            "conv1.bias grad norm: 4.7964587857052265e-09\n",
            "batch_norm.weight grad norm: 0.026584334671497345\n",
            "batch_norm.bias grad norm: 0.028333274647593498\n",
            "lstm.weight_ih_l0 grad norm: 0.15009760856628418\n",
            "lstm.weight_hh_l0 grad norm: 0.046592265367507935\n",
            "lstm.bias_ih_l0 grad norm: 0.014974991790950298\n",
            "lstm.bias_hh_l0 grad norm: 0.014974991790950298\n",
            "fc.weight grad norm: 0.2894405722618103\n",
            "fc.bias grad norm: 0.10264193266630173\n",
            "[Batch 5000] Loss: 0.0309\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.045426297932863235\n",
            "c0 grad norm: 0.026915425434708595\n",
            "conv1.weight grad norm: 0.3220634460449219\n",
            "conv1.bias grad norm: 5.21391907426505e-09\n",
            "batch_norm.weight grad norm: 0.032576460391283035\n",
            "batch_norm.bias grad norm: 0.04081553593277931\n",
            "lstm.weight_ih_l0 grad norm: 0.18225358426570892\n",
            "lstm.weight_hh_l0 grad norm: 0.08237389475107193\n",
            "lstm.bias_ih_l0 grad norm: 0.026751868426799774\n",
            "lstm.bias_hh_l0 grad norm: 0.026751868426799774\n",
            "fc.weight grad norm: 0.49712446331977844\n",
            "fc.bias grad norm: 0.1949426531791687\n",
            "[Batch 6000] Loss: 0.0927\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.029720526188611984\n",
            "c0 grad norm: 0.017957959324121475\n",
            "conv1.weight grad norm: 0.24669790267944336\n",
            "conv1.bias grad norm: 5.645386380592754e-09\n",
            "batch_norm.weight grad norm: 0.03234593942761421\n",
            "batch_norm.bias grad norm: 0.032716650515794754\n",
            "lstm.weight_ih_l0 grad norm: 0.18119622766971588\n",
            "lstm.weight_hh_l0 grad norm: 0.05663103982806206\n",
            "lstm.bias_ih_l0 grad norm: 0.019166190177202225\n",
            "lstm.bias_hh_l0 grad norm: 0.019166190177202225\n",
            "fc.weight grad norm: 0.42271095514297485\n",
            "fc.bias grad norm: 0.12310674041509628\n",
            "[Batch 7000] Loss: 0.0407\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.014136691577732563\n",
            "c0 grad norm: 0.013127356767654419\n",
            "conv1.weight grad norm: 0.13263869285583496\n",
            "conv1.bias grad norm: 3.3068123794777193e-09\n",
            "batch_norm.weight grad norm: 0.025940673425793648\n",
            "batch_norm.bias grad norm: 0.016459986567497253\n",
            "lstm.weight_ih_l0 grad norm: 0.1215154305100441\n",
            "lstm.weight_hh_l0 grad norm: 0.04296577349305153\n",
            "lstm.bias_ih_l0 grad norm: 0.012995782308280468\n",
            "lstm.bias_hh_l0 grad norm: 0.012995782308280468\n",
            "fc.weight grad norm: 0.27024781703948975\n",
            "fc.bias grad norm: 0.14716827869415283\n",
            "[Batch 8000] Loss: 0.0421\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.02715192176401615\n",
            "c0 grad norm: 0.01876378431916237\n",
            "conv1.weight grad norm: 0.27535608410835266\n",
            "conv1.bias grad norm: 4.183065005491926e-09\n",
            "batch_norm.weight grad norm: 0.029957875609397888\n",
            "batch_norm.bias grad norm: 0.023194674402475357\n",
            "lstm.weight_ih_l0 grad norm: 0.15338951349258423\n",
            "lstm.weight_hh_l0 grad norm: 0.05536753311753273\n",
            "lstm.bias_ih_l0 grad norm: 0.016004201024770737\n",
            "lstm.bias_hh_l0 grad norm: 0.016004201024770737\n",
            "fc.weight grad norm: 0.3293452560901642\n",
            "fc.bias grad norm: 0.11812691390514374\n",
            "[Batch 9000] Loss: 0.0480\n",
            "Total Epoch Loss: 879.8409\n",
            "\n",
            "Epoch 89\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.0396481417119503\n",
            "c0 grad norm: 0.0248284712433815\n",
            "conv1.weight grad norm: 0.34970393776893616\n",
            "conv1.bias grad norm: 5.3429314306185915e-09\n",
            "batch_norm.weight grad norm: 0.0421423576772213\n",
            "batch_norm.bias grad norm: 0.036615416407585144\n",
            "lstm.weight_ih_l0 grad norm: 0.2495887279510498\n",
            "lstm.weight_hh_l0 grad norm: 0.07938860356807709\n",
            "lstm.bias_ih_l0 grad norm: 0.02306392788887024\n",
            "lstm.bias_hh_l0 grad norm: 0.02306392788887024\n",
            "fc.weight grad norm: 0.45995765924453735\n",
            "fc.bias grad norm: 0.1823522448539734\n",
            "[Batch 0] Loss: 0.0737\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.023723706603050232\n",
            "c0 grad norm: 0.013529264368116856\n",
            "conv1.weight grad norm: 0.2684636414051056\n",
            "conv1.bias grad norm: 5.062027685909243e-09\n",
            "batch_norm.weight grad norm: 0.023524818941950798\n",
            "batch_norm.bias grad norm: 0.03428197279572487\n",
            "lstm.weight_ih_l0 grad norm: 0.15446944534778595\n",
            "lstm.weight_hh_l0 grad norm: 0.05472636595368385\n",
            "lstm.bias_ih_l0 grad norm: 0.018648067489266396\n",
            "lstm.bias_hh_l0 grad norm: 0.018648067489266396\n",
            "fc.weight grad norm: 0.3790760338306427\n",
            "fc.bias grad norm: 0.12517213821411133\n",
            "[Batch 1000] Loss: 0.0628\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.028981966897845268\n",
            "c0 grad norm: 0.03247324377298355\n",
            "conv1.weight grad norm: 0.39571890234947205\n",
            "conv1.bias grad norm: 4.788519802900737e-09\n",
            "batch_norm.weight grad norm: 0.04292843118309975\n",
            "batch_norm.bias grad norm: 0.0341353565454483\n",
            "lstm.weight_ih_l0 grad norm: 0.24559639394283295\n",
            "lstm.weight_hh_l0 grad norm: 0.06972352415323257\n",
            "lstm.bias_ih_l0 grad norm: 0.023452386260032654\n",
            "lstm.bias_hh_l0 grad norm: 0.023452386260032654\n",
            "fc.weight grad norm: 0.5963521003723145\n",
            "fc.bias grad norm: 0.1570325791835785\n",
            "[Batch 2000] Loss: 0.1203\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.02740911766886711\n",
            "c0 grad norm: 0.02336791343986988\n",
            "conv1.weight grad norm: 0.31981360912323\n",
            "conv1.bias grad norm: 6.9980532479974045e-09\n",
            "batch_norm.weight grad norm: 0.038602039217948914\n",
            "batch_norm.bias grad norm: 0.04419158771634102\n",
            "lstm.weight_ih_l0 grad norm: 0.23560862243175507\n",
            "lstm.weight_hh_l0 grad norm: 0.06681858748197556\n",
            "lstm.bias_ih_l0 grad norm: 0.02318519353866577\n",
            "lstm.bias_hh_l0 grad norm: 0.02318519353866577\n",
            "fc.weight grad norm: 0.5152546763420105\n",
            "fc.bias grad norm: 0.13484753668308258\n",
            "[Batch 3000] Loss: 0.1296\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.027838783338665962\n",
            "c0 grad norm: 0.027965493500232697\n",
            "conv1.weight grad norm: 0.7396395802497864\n",
            "conv1.bias grad norm: 1.0490961876996607e-08\n",
            "batch_norm.weight grad norm: 0.09860824793577194\n",
            "batch_norm.bias grad norm: 0.07310241460800171\n",
            "lstm.weight_ih_l0 grad norm: 0.6155564188957214\n",
            "lstm.weight_hh_l0 grad norm: 0.17107617855072021\n",
            "lstm.bias_ih_l0 grad norm: 0.05296959728002548\n",
            "lstm.bias_hh_l0 grad norm: 0.05296959728002548\n",
            "fc.weight grad norm: 0.6692562699317932\n",
            "fc.bias grad norm: 0.2228720337152481\n",
            "[Batch 4000] Loss: 0.1122\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.009063564240932465\n",
            "c0 grad norm: 0.007066026795655489\n",
            "conv1.weight grad norm: 0.11064556241035461\n",
            "conv1.bias grad norm: 2.0507644471479125e-09\n",
            "batch_norm.weight grad norm: 0.01372882816940546\n",
            "batch_norm.bias grad norm: 0.010431496426463127\n",
            "lstm.weight_ih_l0 grad norm: 0.06374054402112961\n",
            "lstm.weight_hh_l0 grad norm: 0.022057196125388145\n",
            "lstm.bias_ih_l0 grad norm: 0.006133401300758123\n",
            "lstm.bias_hh_l0 grad norm: 0.006133401300758123\n",
            "fc.weight grad norm: 0.1418401002883911\n",
            "fc.bias grad norm: 0.047574885189533234\n",
            "[Batch 5000] Loss: 0.0119\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.036524076014757156\n",
            "c0 grad norm: 0.02917693555355072\n",
            "conv1.weight grad norm: 0.5043882131576538\n",
            "conv1.bias grad norm: 6.857743262145277e-09\n",
            "batch_norm.weight grad norm: 0.049086183309555054\n",
            "batch_norm.bias grad norm: 0.03905873000621796\n",
            "lstm.weight_ih_l0 grad norm: 0.28239014744758606\n",
            "lstm.weight_hh_l0 grad norm: 0.07734963297843933\n",
            "lstm.bias_ih_l0 grad norm: 0.02653459832072258\n",
            "lstm.bias_hh_l0 grad norm: 0.02653459832072258\n",
            "fc.weight grad norm: 0.4800789952278137\n",
            "fc.bias grad norm: 0.13562415540218353\n",
            "[Batch 6000] Loss: 0.1175\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.02042541839182377\n",
            "c0 grad norm: 0.01790989190340042\n",
            "conv1.weight grad norm: 0.34049493074417114\n",
            "conv1.bias grad norm: 3.125232961309621e-09\n",
            "batch_norm.weight grad norm: 0.03502216190099716\n",
            "batch_norm.bias grad norm: 0.03609953820705414\n",
            "lstm.weight_ih_l0 grad norm: 0.1512003391981125\n",
            "lstm.weight_hh_l0 grad norm: 0.053168732672929764\n",
            "lstm.bias_ih_l0 grad norm: 0.018780384212732315\n",
            "lstm.bias_hh_l0 grad norm: 0.018780384212732315\n",
            "fc.weight grad norm: 0.564141571521759\n",
            "fc.bias grad norm: 0.1506679803133011\n",
            "[Batch 7000] Loss: 0.1166\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.19683779776096344\n",
            "c0 grad norm: 0.08309194445610046\n",
            "conv1.weight grad norm: 2.697385311126709\n",
            "conv1.bias grad norm: 6.618219572374073e-08\n",
            "batch_norm.weight grad norm: 0.28893983364105225\n",
            "batch_norm.bias grad norm: 0.2749961316585541\n",
            "lstm.weight_ih_l0 grad norm: 1.0882289409637451\n",
            "lstm.weight_hh_l0 grad norm: 0.25253885984420776\n",
            "lstm.bias_ih_l0 grad norm: 0.10101165622472763\n",
            "lstm.bias_hh_l0 grad norm: 0.10101165622472763\n",
            "fc.weight grad norm: 0.9161883592605591\n",
            "fc.bias grad norm: 0.19526539742946625\n",
            "[Batch 8000] Loss: 0.5393\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.05522783473134041\n",
            "c0 grad norm: 0.04205694422125816\n",
            "conv1.weight grad norm: 0.7011693120002747\n",
            "conv1.bias grad norm: 1.6103765432262662e-08\n",
            "batch_norm.weight grad norm: 0.07471500337123871\n",
            "batch_norm.bias grad norm: 0.0873059630393982\n",
            "lstm.weight_ih_l0 grad norm: 0.3558250963687897\n",
            "lstm.weight_hh_l0 grad norm: 0.10724009573459625\n",
            "lstm.bias_ih_l0 grad norm: 0.04523792117834091\n",
            "lstm.bias_hh_l0 grad norm: 0.04523792117834091\n",
            "fc.weight grad norm: 0.628342866897583\n",
            "fc.bias grad norm: 0.2180156260728836\n",
            "[Batch 9000] Loss: 0.1390\n",
            "Total Epoch Loss: 881.1074\n",
            "\n",
            "Epoch 90\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.06395799666643143\n",
            "c0 grad norm: 0.02464570291340351\n",
            "conv1.weight grad norm: 0.7624291181564331\n",
            "conv1.bias grad norm: 1.4530674441459723e-08\n",
            "batch_norm.weight grad norm: 0.05990389361977577\n",
            "batch_norm.bias grad norm: 0.07532002776861191\n",
            "lstm.weight_ih_l0 grad norm: 0.2848120927810669\n",
            "lstm.weight_hh_l0 grad norm: 0.07879040390253067\n",
            "lstm.bias_ih_l0 grad norm: 0.030421249568462372\n",
            "lstm.bias_hh_l0 grad norm: 0.030421249568462372\n",
            "fc.weight grad norm: 0.2988778054714203\n",
            "fc.bias grad norm: 0.0932980552315712\n",
            "[Batch 0] Loss: 0.0861\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.10836279392242432\n",
            "c0 grad norm: 0.057274289429187775\n",
            "conv1.weight grad norm: 1.4313896894454956\n",
            "conv1.bias grad norm: 4.401629993822098e-08\n",
            "batch_norm.weight grad norm: 0.147505521774292\n",
            "batch_norm.bias grad norm: 0.12493309378623962\n",
            "lstm.weight_ih_l0 grad norm: 0.5759536623954773\n",
            "lstm.weight_hh_l0 grad norm: 0.1610206663608551\n",
            "lstm.bias_ih_l0 grad norm: 0.06854210793972015\n",
            "lstm.bias_hh_l0 grad norm: 0.06854210793972015\n",
            "fc.weight grad norm: 0.44183477759361267\n",
            "fc.bias grad norm: 0.0928569808602333\n",
            "[Batch 1000] Loss: 0.1116\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.038979414850473404\n",
            "c0 grad norm: 0.028733927756547928\n",
            "conv1.weight grad norm: 0.4973289370536804\n",
            "conv1.bias grad norm: 1.1546850586796609e-08\n",
            "batch_norm.weight grad norm: 0.05505182221531868\n",
            "batch_norm.bias grad norm: 0.044408585876226425\n",
            "lstm.weight_ih_l0 grad norm: 0.28838008642196655\n",
            "lstm.weight_hh_l0 grad norm: 0.12046368420124054\n",
            "lstm.bias_ih_l0 grad norm: 0.03251844272017479\n",
            "lstm.bias_hh_l0 grad norm: 0.03251844272017479\n",
            "fc.weight grad norm: 0.6078751087188721\n",
            "fc.bias grad norm: 0.3018239438533783\n",
            "[Batch 2000] Loss: 0.1528\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.0175803042948246\n",
            "c0 grad norm: 0.01743992418050766\n",
            "conv1.weight grad norm: 0.22550711035728455\n",
            "conv1.bias grad norm: 4.6524784025336885e-09\n",
            "batch_norm.weight grad norm: 0.04227357730269432\n",
            "batch_norm.bias grad norm: 0.04121255502104759\n",
            "lstm.weight_ih_l0 grad norm: 0.19572509825229645\n",
            "lstm.weight_hh_l0 grad norm: 0.05493488162755966\n",
            "lstm.bias_ih_l0 grad norm: 0.017422430217266083\n",
            "lstm.bias_hh_l0 grad norm: 0.017422430217266083\n",
            "fc.weight grad norm: 0.24753393232822418\n",
            "fc.bias grad norm: 0.036699939519166946\n",
            "[Batch 3000] Loss: 0.0481\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.024811819195747375\n",
            "c0 grad norm: 0.015622206963598728\n",
            "conv1.weight grad norm: 0.3494908809661865\n",
            "conv1.bias grad norm: 4.474525194808621e-09\n",
            "batch_norm.weight grad norm: 0.026772528886795044\n",
            "batch_norm.bias grad norm: 0.033497199416160583\n",
            "lstm.weight_ih_l0 grad norm: 0.1633347123861313\n",
            "lstm.weight_hh_l0 grad norm: 0.05444641411304474\n",
            "lstm.bias_ih_l0 grad norm: 0.020417384803295135\n",
            "lstm.bias_hh_l0 grad norm: 0.020417384803295135\n",
            "fc.weight grad norm: 0.3442293107509613\n",
            "fc.bias grad norm: 0.09180600196123123\n",
            "[Batch 4000] Loss: 0.0475\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.04260561987757683\n",
            "c0 grad norm: 0.019117416813969612\n",
            "conv1.weight grad norm: 0.31229424476623535\n",
            "conv1.bias grad norm: 6.666683205480695e-09\n",
            "batch_norm.weight grad norm: 0.03827016055583954\n",
            "batch_norm.bias grad norm: 0.032329823821783066\n",
            "lstm.weight_ih_l0 grad norm: 0.17956189811229706\n",
            "lstm.weight_hh_l0 grad norm: 0.05334274843335152\n",
            "lstm.bias_ih_l0 grad norm: 0.019695620983839035\n",
            "lstm.bias_hh_l0 grad norm: 0.019695620983839035\n",
            "fc.weight grad norm: 0.2656060457229614\n",
            "fc.bias grad norm: 0.07500579208135605\n",
            "[Batch 5000] Loss: 0.0351\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.030944984406232834\n",
            "c0 grad norm: 0.013802136294543743\n",
            "conv1.weight grad norm: 0.17490164935588837\n",
            "conv1.bias grad norm: 3.0025757435936384e-09\n",
            "batch_norm.weight grad norm: 0.019349707290530205\n",
            "batch_norm.bias grad norm: 0.031813424080610275\n",
            "lstm.weight_ih_l0 grad norm: 0.12886300683021545\n",
            "lstm.weight_hh_l0 grad norm: 0.044004641473293304\n",
            "lstm.bias_ih_l0 grad norm: 0.01652457006275654\n",
            "lstm.bias_hh_l0 grad norm: 0.01652457006275654\n",
            "fc.weight grad norm: 0.26906487345695496\n",
            "fc.bias grad norm: 0.09510263800621033\n",
            "[Batch 6000] Loss: 0.0291\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.017683543264865875\n",
            "c0 grad norm: 0.017633548006415367\n",
            "conv1.weight grad norm: 0.224168598651886\n",
            "conv1.bias grad norm: 4.357715521763339e-09\n",
            "batch_norm.weight grad norm: 0.03225434944033623\n",
            "batch_norm.bias grad norm: 0.025806192308664322\n",
            "lstm.weight_ih_l0 grad norm: 0.1932215839624405\n",
            "lstm.weight_hh_l0 grad norm: 0.06420262902975082\n",
            "lstm.bias_ih_l0 grad norm: 0.020560242235660553\n",
            "lstm.bias_hh_l0 grad norm: 0.020560242235660553\n",
            "fc.weight grad norm: 0.4213374853134155\n",
            "fc.bias grad norm: 0.14198946952819824\n",
            "[Batch 7000] Loss: 0.0573\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.04089801013469696\n",
            "c0 grad norm: 0.026704750955104828\n",
            "conv1.weight grad norm: 0.7185682058334351\n",
            "conv1.bias grad norm: 1.2473313937277908e-08\n",
            "batch_norm.weight grad norm: 0.07403638958930969\n",
            "batch_norm.bias grad norm: 0.040659062564373016\n",
            "lstm.weight_ih_l0 grad norm: 0.30265384912490845\n",
            "lstm.weight_hh_l0 grad norm: 0.10723946988582611\n",
            "lstm.bias_ih_l0 grad norm: 0.031058715656399727\n",
            "lstm.bias_hh_l0 grad norm: 0.031058715656399727\n",
            "fc.weight grad norm: 0.3975570797920227\n",
            "fc.bias grad norm: 0.11221329867839813\n",
            "[Batch 8000] Loss: 0.0970\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.015587504021823406\n",
            "c0 grad norm: 0.014496182091534138\n",
            "conv1.weight grad norm: 0.1606416255235672\n",
            "conv1.bias grad norm: 3.3373246388634925e-09\n",
            "batch_norm.weight grad norm: 0.016179705038666725\n",
            "batch_norm.bias grad norm: 0.022938789799809456\n",
            "lstm.weight_ih_l0 grad norm: 0.10566817224025726\n",
            "lstm.weight_hh_l0 grad norm: 0.03584296628832817\n",
            "lstm.bias_ih_l0 grad norm: 0.010579317808151245\n",
            "lstm.bias_hh_l0 grad norm: 0.010579317808151245\n",
            "fc.weight grad norm: 0.17044417560100555\n",
            "fc.bias grad norm: 0.009381144307553768\n",
            "[Batch 9000] Loss: 0.0272\n",
            "Total Epoch Loss: 883.8601\n",
            "\n",
            "Epoch 91\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.04142412543296814\n",
            "c0 grad norm: 0.03698372468352318\n",
            "conv1.weight grad norm: 0.575333833694458\n",
            "conv1.bias grad norm: 1.87467303902622e-08\n",
            "batch_norm.weight grad norm: 0.05651009827852249\n",
            "batch_norm.bias grad norm: 0.0628780871629715\n",
            "lstm.weight_ih_l0 grad norm: 0.3369618356227875\n",
            "lstm.weight_hh_l0 grad norm: 0.14123457670211792\n",
            "lstm.bias_ih_l0 grad norm: 0.040829118341207504\n",
            "lstm.bias_hh_l0 grad norm: 0.040829118341207504\n",
            "fc.weight grad norm: 0.3108154237270355\n",
            "fc.bias grad norm: 0.08376095443964005\n",
            "[Batch 0] Loss: 0.0770\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.014669409953057766\n",
            "c0 grad norm: 0.015600222162902355\n",
            "conv1.weight grad norm: 0.2585276961326599\n",
            "conv1.bias grad norm: 3.1863118810093738e-09\n",
            "batch_norm.weight grad norm: 0.023271555081009865\n",
            "batch_norm.bias grad norm: 0.025625605136156082\n",
            "lstm.weight_ih_l0 grad norm: 0.13938452303409576\n",
            "lstm.weight_hh_l0 grad norm: 0.04859907180070877\n",
            "lstm.bias_ih_l0 grad norm: 0.013956181704998016\n",
            "lstm.bias_hh_l0 grad norm: 0.013956181704998016\n",
            "fc.weight grad norm: 0.21093429625034332\n",
            "fc.bias grad norm: 0.05468463525176048\n",
            "[Batch 1000] Loss: 0.0317\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.01173668634146452\n",
            "c0 grad norm: 0.0077161528170108795\n",
            "conv1.weight grad norm: 0.12499251216650009\n",
            "conv1.bias grad norm: 3.922212776785727e-09\n",
            "batch_norm.weight grad norm: 0.016131117939949036\n",
            "batch_norm.bias grad norm: 0.013533839955925941\n",
            "lstm.weight_ih_l0 grad norm: 0.08766928315162659\n",
            "lstm.weight_hh_l0 grad norm: 0.021803593263030052\n",
            "lstm.bias_ih_l0 grad norm: 0.007809715811163187\n",
            "lstm.bias_hh_l0 grad norm: 0.007809715811163187\n",
            "fc.weight grad norm: 0.15935984253883362\n",
            "fc.bias grad norm: 0.06218613684177399\n",
            "[Batch 2000] Loss: 0.0129\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.014261790551245213\n",
            "c0 grad norm: 0.015316160395741463\n",
            "conv1.weight grad norm: 0.25085940957069397\n",
            "conv1.bias grad norm: 2.7627209409075704e-09\n",
            "batch_norm.weight grad norm: 0.0281731765717268\n",
            "batch_norm.bias grad norm: 0.02346891351044178\n",
            "lstm.weight_ih_l0 grad norm: 0.15899726748466492\n",
            "lstm.weight_hh_l0 grad norm: 0.058279141783714294\n",
            "lstm.bias_ih_l0 grad norm: 0.015845241025090218\n",
            "lstm.bias_hh_l0 grad norm: 0.015845241025090218\n",
            "fc.weight grad norm: 0.25260332226753235\n",
            "fc.bias grad norm: 0.12797217071056366\n",
            "[Batch 3000] Loss: 0.0443\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.04259898141026497\n",
            "c0 grad norm: 0.019648924469947815\n",
            "conv1.weight grad norm: 0.49075135588645935\n",
            "conv1.bias grad norm: 1.1036771496719666e-08\n",
            "batch_norm.weight grad norm: 0.06650238484144211\n",
            "batch_norm.bias grad norm: 0.06147939711809158\n",
            "lstm.weight_ih_l0 grad norm: 0.34975510835647583\n",
            "lstm.weight_hh_l0 grad norm: 0.07577574998140335\n",
            "lstm.bias_ih_l0 grad norm: 0.032323289662599564\n",
            "lstm.bias_hh_l0 grad norm: 0.032323289662599564\n",
            "fc.weight grad norm: 0.5045802593231201\n",
            "fc.bias grad norm: 0.17041176557540894\n",
            "[Batch 4000] Loss: 0.1557\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.026522869244217873\n",
            "c0 grad norm: 0.01270382385700941\n",
            "conv1.weight grad norm: 0.19792714715003967\n",
            "conv1.bias grad norm: 3.3405949118048284e-09\n",
            "batch_norm.weight grad norm: 0.02712673880159855\n",
            "batch_norm.bias grad norm: 0.02365812472999096\n",
            "lstm.weight_ih_l0 grad norm: 0.13354861736297607\n",
            "lstm.weight_hh_l0 grad norm: 0.042112916707992554\n",
            "lstm.bias_ih_l0 grad norm: 0.014974616467952728\n",
            "lstm.bias_hh_l0 grad norm: 0.014974616467952728\n",
            "fc.weight grad norm: 0.23943664133548737\n",
            "fc.bias grad norm: 0.057302650064229965\n",
            "[Batch 5000] Loss: 0.0222\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.038333579897880554\n",
            "c0 grad norm: 0.027734607458114624\n",
            "conv1.weight grad norm: 0.7437811493873596\n",
            "conv1.bias grad norm: 1.1105973918290601e-08\n",
            "batch_norm.weight grad norm: 0.08483408391475677\n",
            "batch_norm.bias grad norm: 0.03757445886731148\n",
            "lstm.weight_ih_l0 grad norm: 0.33582746982574463\n",
            "lstm.weight_hh_l0 grad norm: 0.08426351100206375\n",
            "lstm.bias_ih_l0 grad norm: 0.032653097063302994\n",
            "lstm.bias_hh_l0 grad norm: 0.032653097063302994\n",
            "fc.weight grad norm: 0.332303524017334\n",
            "fc.bias grad norm: 0.17480100691318512\n",
            "[Batch 6000] Loss: 0.0392\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.028233062475919724\n",
            "c0 grad norm: 0.010046538896858692\n",
            "conv1.weight grad norm: 0.2745155692100525\n",
            "conv1.bias grad norm: 6.41023278902253e-09\n",
            "batch_norm.weight grad norm: 0.026313479989767075\n",
            "batch_norm.bias grad norm: 0.02893657051026821\n",
            "lstm.weight_ih_l0 grad norm: 0.15555673837661743\n",
            "lstm.weight_hh_l0 grad norm: 0.043081264942884445\n",
            "lstm.bias_ih_l0 grad norm: 0.015171193517744541\n",
            "lstm.bias_hh_l0 grad norm: 0.015171193517744541\n",
            "fc.weight grad norm: 0.30535799264907837\n",
            "fc.bias grad norm: 0.06866266578435898\n",
            "[Batch 7000] Loss: 0.0547\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.037506911903619766\n",
            "c0 grad norm: 0.021763188764452934\n",
            "conv1.weight grad norm: 0.28865331411361694\n",
            "conv1.bias grad norm: 5.274384484721395e-09\n",
            "batch_norm.weight grad norm: 0.02670266106724739\n",
            "batch_norm.bias grad norm: 0.02775214985013008\n",
            "lstm.weight_ih_l0 grad norm: 0.1449945867061615\n",
            "lstm.weight_hh_l0 grad norm: 0.057725366204977036\n",
            "lstm.bias_ih_l0 grad norm: 0.019336314871907234\n",
            "lstm.bias_hh_l0 grad norm: 0.019336314871907234\n",
            "fc.weight grad norm: 0.388633131980896\n",
            "fc.bias grad norm: 0.11998811364173889\n",
            "[Batch 8000] Loss: 0.0408\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.017927493900060654\n",
            "c0 grad norm: 0.01732739433646202\n",
            "conv1.weight grad norm: 0.23191069066524506\n",
            "conv1.bias grad norm: 4.106099460443602e-09\n",
            "batch_norm.weight grad norm: 0.022587178274989128\n",
            "batch_norm.bias grad norm: 0.024169154465198517\n",
            "lstm.weight_ih_l0 grad norm: 0.16021119058132172\n",
            "lstm.weight_hh_l0 grad norm: 0.05117599666118622\n",
            "lstm.bias_ih_l0 grad norm: 0.016674567013978958\n",
            "lstm.bias_hh_l0 grad norm: 0.016674567013978958\n",
            "fc.weight grad norm: 0.28329747915267944\n",
            "fc.bias grad norm: 0.13156072795391083\n",
            "[Batch 9000] Loss: 0.0541\n",
            "Total Epoch Loss: 879.2278\n",
            "\n",
            "Epoch 92\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.010657536797225475\n",
            "c0 grad norm: 0.012343381531536579\n",
            "conv1.weight grad norm: 0.18765032291412354\n",
            "conv1.bias grad norm: 4.497965111482927e-09\n",
            "batch_norm.weight grad norm: 0.01755486987531185\n",
            "batch_norm.bias grad norm: 0.011863796040415764\n",
            "lstm.weight_ih_l0 grad norm: 0.0940612405538559\n",
            "lstm.weight_hh_l0 grad norm: 0.03183501586318016\n",
            "lstm.bias_ih_l0 grad norm: 0.00949147716164589\n",
            "lstm.bias_hh_l0 grad norm: 0.00949147716164589\n",
            "fc.weight grad norm: 0.19403819739818573\n",
            "fc.bias grad norm: 0.05530839413404465\n",
            "[Batch 0] Loss: 0.0223\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.03555154427886009\n",
            "c0 grad norm: 0.019914943724870682\n",
            "conv1.weight grad norm: 0.3344908654689789\n",
            "conv1.bias grad norm: 5.4688804596025875e-09\n",
            "batch_norm.weight grad norm: 0.0443839505314827\n",
            "batch_norm.bias grad norm: 0.04113329201936722\n",
            "lstm.weight_ih_l0 grad norm: 0.2787395715713501\n",
            "lstm.weight_hh_l0 grad norm: 0.06679095327854156\n",
            "lstm.bias_ih_l0 grad norm: 0.025481341406702995\n",
            "lstm.bias_hh_l0 grad norm: 0.025481341406702995\n",
            "fc.weight grad norm: 0.6951920986175537\n",
            "fc.bias grad norm: 0.09956663101911545\n",
            "[Batch 1000] Loss: 0.1285\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.0167278740555048\n",
            "c0 grad norm: 0.017319444566965103\n",
            "conv1.weight grad norm: 0.34181883931159973\n",
            "conv1.bias grad norm: 4.715022594581342e-09\n",
            "batch_norm.weight grad norm: 0.02642911486327648\n",
            "batch_norm.bias grad norm: 0.023308953270316124\n",
            "lstm.weight_ih_l0 grad norm: 0.16846837103366852\n",
            "lstm.weight_hh_l0 grad norm: 0.058749906718730927\n",
            "lstm.bias_ih_l0 grad norm: 0.018715854734182358\n",
            "lstm.bias_hh_l0 grad norm: 0.018715854734182358\n",
            "fc.weight grad norm: 0.2810915410518646\n",
            "fc.bias grad norm: 0.11276280879974365\n",
            "[Batch 2000] Loss: 0.0252\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.06451285630464554\n",
            "c0 grad norm: 0.04476543515920639\n",
            "conv1.weight grad norm: 0.5543939471244812\n",
            "conv1.bias grad norm: 1.787326198154915e-08\n",
            "batch_norm.weight grad norm: 0.06898365169763565\n",
            "batch_norm.bias grad norm: 0.06932903826236725\n",
            "lstm.weight_ih_l0 grad norm: 0.30973342061042786\n",
            "lstm.weight_hh_l0 grad norm: 0.10366888344287872\n",
            "lstm.bias_ih_l0 grad norm: 0.039192862808704376\n",
            "lstm.bias_hh_l0 grad norm: 0.039192862808704376\n",
            "fc.weight grad norm: 0.4643020033836365\n",
            "fc.bias grad norm: 0.21342091262340546\n",
            "[Batch 3000] Loss: 0.1117\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.006819541100412607\n",
            "c0 grad norm: 0.005200672894716263\n",
            "conv1.weight grad norm: 0.07564182579517365\n",
            "conv1.bias grad norm: 1.473663302498096e-09\n",
            "batch_norm.weight grad norm: 0.00976542942225933\n",
            "batch_norm.bias grad norm: 0.009061266668140888\n",
            "lstm.weight_ih_l0 grad norm: 0.04857097193598747\n",
            "lstm.weight_hh_l0 grad norm: 0.017121631652116776\n",
            "lstm.bias_ih_l0 grad norm: 0.005453006364405155\n",
            "lstm.bias_hh_l0 grad norm: 0.005453006364405155\n",
            "fc.weight grad norm: 0.19188594818115234\n",
            "fc.bias grad norm: 0.059000857174396515\n",
            "[Batch 4000] Loss: 0.0127\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.038144469261169434\n",
            "c0 grad norm: 0.02169218100607395\n",
            "conv1.weight grad norm: 0.583312451839447\n",
            "conv1.bias grad norm: 1.0785708326466192e-08\n",
            "batch_norm.weight grad norm: 0.05800478160381317\n",
            "batch_norm.bias grad norm: 0.049446433782577515\n",
            "lstm.weight_ih_l0 grad norm: 0.3090803325176239\n",
            "lstm.weight_hh_l0 grad norm: 0.09956591576337814\n",
            "lstm.bias_ih_l0 grad norm: 0.03246704861521721\n",
            "lstm.bias_hh_l0 grad norm: 0.03246704861521721\n",
            "fc.weight grad norm: 0.7019006013870239\n",
            "fc.bias grad norm: 0.25567761063575745\n",
            "[Batch 5000] Loss: 0.2529\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.025657014921307564\n",
            "c0 grad norm: 0.025482768192887306\n",
            "conv1.weight grad norm: 0.45061299204826355\n",
            "conv1.bias grad norm: 8.608864732195798e-09\n",
            "batch_norm.weight grad norm: 0.054720018059015274\n",
            "batch_norm.bias grad norm: 0.026316575706005096\n",
            "lstm.weight_ih_l0 grad norm: 0.2912389934062958\n",
            "lstm.weight_hh_l0 grad norm: 0.08133985847234726\n",
            "lstm.bias_ih_l0 grad norm: 0.016743337735533714\n",
            "lstm.bias_hh_l0 grad norm: 0.016743337735533714\n",
            "fc.weight grad norm: 0.5129824280738831\n",
            "fc.bias grad norm: 0.06944721192121506\n",
            "[Batch 6000] Loss: 0.1145\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.00683217030018568\n",
            "c0 grad norm: 0.008473571389913559\n",
            "conv1.weight grad norm: 0.13037219643592834\n",
            "conv1.bias grad norm: 2.4505102391714217e-09\n",
            "batch_norm.weight grad norm: 0.018808595836162567\n",
            "batch_norm.bias grad norm: 0.017010493203997612\n",
            "lstm.weight_ih_l0 grad norm: 0.11305753886699677\n",
            "lstm.weight_hh_l0 grad norm: 0.03514433279633522\n",
            "lstm.bias_ih_l0 grad norm: 0.01074621919542551\n",
            "lstm.bias_hh_l0 grad norm: 0.01074621919542551\n",
            "fc.weight grad norm: 0.21052186191082\n",
            "fc.bias grad norm: 0.036760490387678146\n",
            "[Batch 7000] Loss: 0.0264\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.023837272077798843\n",
            "c0 grad norm: 0.014881742186844349\n",
            "conv1.weight grad norm: 0.14728981256484985\n",
            "conv1.bias grad norm: 4.058949176766191e-09\n",
            "batch_norm.weight grad norm: 0.019473792985081673\n",
            "batch_norm.bias grad norm: 0.020124224945902824\n",
            "lstm.weight_ih_l0 grad norm: 0.1200294941663742\n",
            "lstm.weight_hh_l0 grad norm: 0.03729509189724922\n",
            "lstm.bias_ih_l0 grad norm: 0.01280384510755539\n",
            "lstm.bias_hh_l0 grad norm: 0.01280384510755539\n",
            "fc.weight grad norm: 0.26409366726875305\n",
            "fc.bias grad norm: 0.11626193672418594\n",
            "[Batch 8000] Loss: 0.0337\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.016257936134934425\n",
            "c0 grad norm: 0.014095896854996681\n",
            "conv1.weight grad norm: 0.20441634953022003\n",
            "conv1.bias grad norm: 3.1666929078966177e-09\n",
            "batch_norm.weight grad norm: 0.024669529870152473\n",
            "batch_norm.bias grad norm: 0.023138560354709625\n",
            "lstm.weight_ih_l0 grad norm: 0.1365342140197754\n",
            "lstm.weight_hh_l0 grad norm: 0.03958345577120781\n",
            "lstm.bias_ih_l0 grad norm: 0.013056126423180103\n",
            "lstm.bias_hh_l0 grad norm: 0.013056126423180103\n",
            "fc.weight grad norm: 0.33949556946754456\n",
            "fc.bias grad norm: 0.02397547848522663\n",
            "[Batch 9000] Loss: 0.0365\n",
            "Total Epoch Loss: 880.1315\n",
            "\n",
            "Epoch 93\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.021954059600830078\n",
            "c0 grad norm: 0.0189079400151968\n",
            "conv1.weight grad norm: 0.43827056884765625\n",
            "conv1.bias grad norm: 5.551120008107091e-09\n",
            "batch_norm.weight grad norm: 0.040832098573446274\n",
            "batch_norm.bias grad norm: 0.03679032251238823\n",
            "lstm.weight_ih_l0 grad norm: 0.24807605147361755\n",
            "lstm.weight_hh_l0 grad norm: 0.06257393211126328\n",
            "lstm.bias_ih_l0 grad norm: 0.023350179195404053\n",
            "lstm.bias_hh_l0 grad norm: 0.023350179195404053\n",
            "fc.weight grad norm: 0.46437960863113403\n",
            "fc.bias grad norm: 0.16202297806739807\n",
            "[Batch 0] Loss: 0.0678\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.09813117980957031\n",
            "c0 grad norm: 0.06452419608831406\n",
            "conv1.weight grad norm: 0.9894429445266724\n",
            "conv1.bias grad norm: 1.2803843318920372e-08\n",
            "batch_norm.weight grad norm: 0.11075785011053085\n",
            "batch_norm.bias grad norm: 0.111715167760849\n",
            "lstm.weight_ih_l0 grad norm: 0.59285569190979\n",
            "lstm.weight_hh_l0 grad norm: 0.238262340426445\n",
            "lstm.bias_ih_l0 grad norm: 0.06980018317699432\n",
            "lstm.bias_hh_l0 grad norm: 0.06980018317699432\n",
            "fc.weight grad norm: 0.3978349268436432\n",
            "fc.bias grad norm: 0.10222012549638748\n",
            "[Batch 1000] Loss: 0.0669\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.027147755026817322\n",
            "c0 grad norm: 0.02776237204670906\n",
            "conv1.weight grad norm: 0.48357537388801575\n",
            "conv1.bias grad norm: 7.003448487807873e-09\n",
            "batch_norm.weight grad norm: 0.051751814782619476\n",
            "batch_norm.bias grad norm: 0.045426808297634125\n",
            "lstm.weight_ih_l0 grad norm: 0.24929730594158173\n",
            "lstm.weight_hh_l0 grad norm: 0.09118956327438354\n",
            "lstm.bias_ih_l0 grad norm: 0.025347961112856865\n",
            "lstm.bias_hh_l0 grad norm: 0.025347961112856865\n",
            "fc.weight grad norm: 0.5545536875724792\n",
            "fc.bias grad norm: 0.25293591618537903\n",
            "[Batch 2000] Loss: 0.1341\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.025451013818383217\n",
            "c0 grad norm: 0.031060369685292244\n",
            "conv1.weight grad norm: 0.5062434077262878\n",
            "conv1.bias grad norm: 5.824453364056126e-09\n",
            "batch_norm.weight grad norm: 0.0433267280459404\n",
            "batch_norm.bias grad norm: 0.04867393895983696\n",
            "lstm.weight_ih_l0 grad norm: 0.24855414032936096\n",
            "lstm.weight_hh_l0 grad norm: 0.07030170410871506\n",
            "lstm.bias_ih_l0 grad norm: 0.02250780537724495\n",
            "lstm.bias_hh_l0 grad norm: 0.02250780537724495\n",
            "fc.weight grad norm: 0.6382180452346802\n",
            "fc.bias grad norm: 0.015049036592245102\n",
            "[Batch 3000] Loss: 0.1437\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.04052068293094635\n",
            "c0 grad norm: 0.036595847457647324\n",
            "conv1.weight grad norm: 0.8157323002815247\n",
            "conv1.bias grad norm: 1.1413278322436327e-08\n",
            "batch_norm.weight grad norm: 0.05391252040863037\n",
            "batch_norm.bias grad norm: 0.06212810426950455\n",
            "lstm.weight_ih_l0 grad norm: 0.3765789270401001\n",
            "lstm.weight_hh_l0 grad norm: 0.15408410131931305\n",
            "lstm.bias_ih_l0 grad norm: 0.04526810720562935\n",
            "lstm.bias_hh_l0 grad norm: 0.04526810720562935\n",
            "fc.weight grad norm: 0.8648899793624878\n",
            "fc.bias grad norm: 0.44212958216667175\n",
            "[Batch 4000] Loss: 0.4397\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.017100878059864044\n",
            "c0 grad norm: 0.00795906875282526\n",
            "conv1.weight grad norm: 0.2150438129901886\n",
            "conv1.bias grad norm: 3.9713432542498595e-09\n",
            "batch_norm.weight grad norm: 0.017827697098255157\n",
            "batch_norm.bias grad norm: 0.022192416712641716\n",
            "lstm.weight_ih_l0 grad norm: 0.09539958089590073\n",
            "lstm.weight_hh_l0 grad norm: 0.026071831583976746\n",
            "lstm.bias_ih_l0 grad norm: 0.011472445912659168\n",
            "lstm.bias_hh_l0 grad norm: 0.011472445912659168\n",
            "fc.weight grad norm: 0.19672228395938873\n",
            "fc.bias grad norm: 0.09958527237176895\n",
            "[Batch 5000] Loss: 0.0213\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.010056335479021072\n",
            "c0 grad norm: 0.004743346944451332\n",
            "conv1.weight grad norm: 0.09017651528120041\n",
            "conv1.bias grad norm: 2.0604196127038676e-09\n",
            "batch_norm.weight grad norm: 0.012875218875706196\n",
            "batch_norm.bias grad norm: 0.01114183384925127\n",
            "lstm.weight_ih_l0 grad norm: 0.06323227286338806\n",
            "lstm.weight_hh_l0 grad norm: 0.01817658729851246\n",
            "lstm.bias_ih_l0 grad norm: 0.006017367355525494\n",
            "lstm.bias_hh_l0 grad norm: 0.006017367355525494\n",
            "fc.weight grad norm: 0.25822713971138\n",
            "fc.bias grad norm: 0.10329639166593552\n",
            "[Batch 6000] Loss: 0.0350\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.01234066765755415\n",
            "c0 grad norm: 0.01585722528398037\n",
            "conv1.weight grad norm: 0.19720754027366638\n",
            "conv1.bias grad norm: 5.523703716647788e-09\n",
            "batch_norm.weight grad norm: 0.025714991614222527\n",
            "batch_norm.bias grad norm: 0.027867108583450317\n",
            "lstm.weight_ih_l0 grad norm: 0.13158661127090454\n",
            "lstm.weight_hh_l0 grad norm: 0.04861552268266678\n",
            "lstm.bias_ih_l0 grad norm: 0.014634697698056698\n",
            "lstm.bias_hh_l0 grad norm: 0.014634697698056698\n",
            "fc.weight grad norm: 0.17599612474441528\n",
            "fc.bias grad norm: 0.010010549798607826\n",
            "[Batch 7000] Loss: 0.0256\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.026762062683701515\n",
            "c0 grad norm: 0.017460966482758522\n",
            "conv1.weight grad norm: 0.4301595389842987\n",
            "conv1.bias grad norm: 8.207287294226262e-09\n",
            "batch_norm.weight grad norm: 0.03701107203960419\n",
            "batch_norm.bias grad norm: 0.03640985116362572\n",
            "lstm.weight_ih_l0 grad norm: 0.20143923163414001\n",
            "lstm.weight_hh_l0 grad norm: 0.0563967265188694\n",
            "lstm.bias_ih_l0 grad norm: 0.01996704563498497\n",
            "lstm.bias_hh_l0 grad norm: 0.01996704563498497\n",
            "fc.weight grad norm: 0.28344932198524475\n",
            "fc.bias grad norm: 0.11360672861337662\n",
            "[Batch 8000] Loss: 0.0592\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.02015804313123226\n",
            "c0 grad norm: 0.015623819082975388\n",
            "conv1.weight grad norm: 0.28211742639541626\n",
            "conv1.bias grad norm: 7.27431670455303e-09\n",
            "batch_norm.weight grad norm: 0.03273792564868927\n",
            "batch_norm.bias grad norm: 0.02174563892185688\n",
            "lstm.weight_ih_l0 grad norm: 0.16898974776268005\n",
            "lstm.weight_hh_l0 grad norm: 0.06172141432762146\n",
            "lstm.bias_ih_l0 grad norm: 0.017197396606206894\n",
            "lstm.bias_hh_l0 grad norm: 0.017197396606206894\n",
            "fc.weight grad norm: 0.4059605896472931\n",
            "fc.bias grad norm: 0.20597800612449646\n",
            "[Batch 9000] Loss: 0.0926\n",
            "Total Epoch Loss: 880.6074\n",
            "\n",
            "Epoch 94\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.00781677383929491\n",
            "c0 grad norm: 0.008938736282289028\n",
            "conv1.weight grad norm: 0.18894414603710175\n",
            "conv1.bias grad norm: 2.414952460227937e-09\n",
            "batch_norm.weight grad norm: 0.018698792904615402\n",
            "batch_norm.bias grad norm: 0.021068666130304337\n",
            "lstm.weight_ih_l0 grad norm: 0.10270398110151291\n",
            "lstm.weight_hh_l0 grad norm: 0.04377381131052971\n",
            "lstm.bias_ih_l0 grad norm: 0.011255460791289806\n",
            "lstm.bias_hh_l0 grad norm: 0.011255460791289806\n",
            "fc.weight grad norm: 0.09420842677354813\n",
            "fc.bias grad norm: 0.0024241404607892036\n",
            "[Batch 0] Loss: 0.0213\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.018967656418681145\n",
            "c0 grad norm: 0.016471166163682938\n",
            "conv1.weight grad norm: 0.3164944350719452\n",
            "conv1.bias grad norm: 4.1513574799978414e-09\n",
            "batch_norm.weight grad norm: 0.03234786540269852\n",
            "batch_norm.bias grad norm: 0.02435653656721115\n",
            "lstm.weight_ih_l0 grad norm: 0.18818031251430511\n",
            "lstm.weight_hh_l0 grad norm: 0.056544166058301926\n",
            "lstm.bias_ih_l0 grad norm: 0.015079943463206291\n",
            "lstm.bias_hh_l0 grad norm: 0.015079943463206291\n",
            "fc.weight grad norm: 0.4705387353897095\n",
            "fc.bias grad norm: 0.1314845085144043\n",
            "[Batch 1000] Loss: 0.0807\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.05625535920262337\n",
            "c0 grad norm: 0.03560050204396248\n",
            "conv1.weight grad norm: 0.5941437482833862\n",
            "conv1.bias grad norm: 1.3358531170126753e-08\n",
            "batch_norm.weight grad norm: 0.06254928559064865\n",
            "batch_norm.bias grad norm: 0.054793260991573334\n",
            "lstm.weight_ih_l0 grad norm: 0.34398308396339417\n",
            "lstm.weight_hh_l0 grad norm: 0.10939770936965942\n",
            "lstm.bias_ih_l0 grad norm: 0.029730645939707756\n",
            "lstm.bias_hh_l0 grad norm: 0.029730645939707756\n",
            "fc.weight grad norm: 0.9786540269851685\n",
            "fc.bias grad norm: 0.23192402720451355\n",
            "[Batch 2000] Loss: 0.4384\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.07333057373762131\n",
            "c0 grad norm: 0.04944019392132759\n",
            "conv1.weight grad norm: 0.8077272176742554\n",
            "conv1.bias grad norm: 1.0423153895544601e-08\n",
            "batch_norm.weight grad norm: 0.07551006227731705\n",
            "batch_norm.bias grad norm: 0.08017414063215256\n",
            "lstm.weight_ih_l0 grad norm: 0.44914814829826355\n",
            "lstm.weight_hh_l0 grad norm: 0.13741916418075562\n",
            "lstm.bias_ih_l0 grad norm: 0.045982714742422104\n",
            "lstm.bias_hh_l0 grad norm: 0.045982714742422104\n",
            "fc.weight grad norm: 0.3610917925834656\n",
            "fc.bias grad norm: 0.12601661682128906\n",
            "[Batch 3000] Loss: 0.0805\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.02889520861208439\n",
            "c0 grad norm: 0.01891709864139557\n",
            "conv1.weight grad norm: 0.4079936444759369\n",
            "conv1.bias grad norm: 5.796507274169471e-09\n",
            "batch_norm.weight grad norm: 0.03644857555627823\n",
            "batch_norm.bias grad norm: 0.035890817642211914\n",
            "lstm.weight_ih_l0 grad norm: 0.19673658907413483\n",
            "lstm.weight_hh_l0 grad norm: 0.05233826860785484\n",
            "lstm.bias_ih_l0 grad norm: 0.019938303157687187\n",
            "lstm.bias_hh_l0 grad norm: 0.019938303157687187\n",
            "fc.weight grad norm: 0.27741408348083496\n",
            "fc.bias grad norm: 0.15037865936756134\n",
            "[Batch 4000] Loss: 0.0397\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.13482606410980225\n",
            "c0 grad norm: 0.0775054544210434\n",
            "conv1.weight grad norm: 1.4475547075271606\n",
            "conv1.bias grad norm: 3.1987788418064156e-08\n",
            "batch_norm.weight grad norm: 0.12496954202651978\n",
            "batch_norm.bias grad norm: 0.16230839490890503\n",
            "lstm.weight_ih_l0 grad norm: 0.6529015302658081\n",
            "lstm.weight_hh_l0 grad norm: 0.22504830360412598\n",
            "lstm.bias_ih_l0 grad norm: 0.08077597618103027\n",
            "lstm.bias_hh_l0 grad norm: 0.08077597618103027\n",
            "fc.weight grad norm: 0.6524941325187683\n",
            "fc.bias grad norm: 0.16084760427474976\n",
            "[Batch 5000] Loss: 0.4432\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.019245905801653862\n",
            "c0 grad norm: 0.014742456376552582\n",
            "conv1.weight grad norm: 0.15876439213752747\n",
            "conv1.bias grad norm: 3.513753288331145e-09\n",
            "batch_norm.weight grad norm: 0.021670527756214142\n",
            "batch_norm.bias grad norm: 0.02378666028380394\n",
            "lstm.weight_ih_l0 grad norm: 0.11943067610263824\n",
            "lstm.weight_hh_l0 grad norm: 0.04437856748700142\n",
            "lstm.bias_ih_l0 grad norm: 0.01553572528064251\n",
            "lstm.bias_hh_l0 grad norm: 0.01553572528064251\n",
            "fc.weight grad norm: 0.38405555486679077\n",
            "fc.bias grad norm: 0.13258622586727142\n",
            "[Batch 6000] Loss: 0.0383\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.08370348811149597\n",
            "c0 grad norm: 0.057873886078596115\n",
            "conv1.weight grad norm: 1.32166588306427\n",
            "conv1.bias grad norm: 2.497386830668802e-08\n",
            "batch_norm.weight grad norm: 0.10929080843925476\n",
            "batch_norm.bias grad norm: 0.1207302063703537\n",
            "lstm.weight_ih_l0 grad norm: 0.5072140097618103\n",
            "lstm.weight_hh_l0 grad norm: 0.14760828018188477\n",
            "lstm.bias_ih_l0 grad norm: 0.06087400019168854\n",
            "lstm.bias_hh_l0 grad norm: 0.06087400019168854\n",
            "fc.weight grad norm: 0.45883476734161377\n",
            "fc.bias grad norm: 0.06290923058986664\n",
            "[Batch 7000] Loss: 0.1119\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.02894081175327301\n",
            "c0 grad norm: 0.01723376475274563\n",
            "conv1.weight grad norm: 0.2836942970752716\n",
            "conv1.bias grad norm: 4.85488493850994e-09\n",
            "batch_norm.weight grad norm: 0.02307787351310253\n",
            "batch_norm.bias grad norm: 0.03543047234416008\n",
            "lstm.weight_ih_l0 grad norm: 0.13728328049182892\n",
            "lstm.weight_hh_l0 grad norm: 0.04371247440576553\n",
            "lstm.bias_ih_l0 grad norm: 0.02194809913635254\n",
            "lstm.bias_hh_l0 grad norm: 0.02194809913635254\n",
            "fc.weight grad norm: 0.352524071931839\n",
            "fc.bias grad norm: 0.10707753151655197\n",
            "[Batch 8000] Loss: 0.0402\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.039146315306425095\n",
            "c0 grad norm: 0.024319453164935112\n",
            "conv1.weight grad norm: 0.4106716513633728\n",
            "conv1.bias grad norm: 1.0201164357681591e-08\n",
            "batch_norm.weight grad norm: 0.038589958101511\n",
            "batch_norm.bias grad norm: 0.0689113438129425\n",
            "lstm.weight_ih_l0 grad norm: 0.235415518283844\n",
            "lstm.weight_hh_l0 grad norm: 0.09401537477970123\n",
            "lstm.bias_ih_l0 grad norm: 0.02736753597855568\n",
            "lstm.bias_hh_l0 grad norm: 0.02736753597855568\n",
            "fc.weight grad norm: 0.5864696502685547\n",
            "fc.bias grad norm: 0.24112999439239502\n",
            "[Batch 9000] Loss: 0.1399\n",
            "Total Epoch Loss: 878.1994\n",
            "\n",
            "Epoch 95\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.09840793162584305\n",
            "c0 grad norm: 0.03868997097015381\n",
            "conv1.weight grad norm: 0.8965933322906494\n",
            "conv1.bias grad norm: 1.8004371327151603e-08\n",
            "batch_norm.weight grad norm: 0.08545155823230743\n",
            "batch_norm.bias grad norm: 0.11836590617895126\n",
            "lstm.weight_ih_l0 grad norm: 0.42525121569633484\n",
            "lstm.weight_hh_l0 grad norm: 0.0913943499326706\n",
            "lstm.bias_ih_l0 grad norm: 0.05430446192622185\n",
            "lstm.bias_hh_l0 grad norm: 0.05430446192622185\n",
            "fc.weight grad norm: 0.35579633712768555\n",
            "fc.bias grad norm: 0.10866441577672958\n",
            "[Batch 0] Loss: 0.0779\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.02075130306184292\n",
            "c0 grad norm: 0.0129614332690835\n",
            "conv1.weight grad norm: 0.30250057578086853\n",
            "conv1.bias grad norm: 4.611650172847703e-09\n",
            "batch_norm.weight grad norm: 0.03297499194741249\n",
            "batch_norm.bias grad norm: 0.03963092714548111\n",
            "lstm.weight_ih_l0 grad norm: 0.16566196084022522\n",
            "lstm.weight_hh_l0 grad norm: 0.06399048119783401\n",
            "lstm.bias_ih_l0 grad norm: 0.01617990806698799\n",
            "lstm.bias_hh_l0 grad norm: 0.01617990806698799\n",
            "fc.weight grad norm: 0.2974054217338562\n",
            "fc.bias grad norm: 0.056735552847385406\n",
            "[Batch 1000] Loss: 0.0522\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.10868193209171295\n",
            "c0 grad norm: 0.03390992432832718\n",
            "conv1.weight grad norm: 0.8944622874259949\n",
            "conv1.bias grad norm: 1.672377258898905e-08\n",
            "batch_norm.weight grad norm: 0.10827454179525375\n",
            "batch_norm.bias grad norm: 0.11320462822914124\n",
            "lstm.weight_ih_l0 grad norm: 0.4795669913291931\n",
            "lstm.weight_hh_l0 grad norm: 0.08328289538621902\n",
            "lstm.bias_ih_l0 grad norm: 0.047782450914382935\n",
            "lstm.bias_hh_l0 grad norm: 0.047782450914382935\n",
            "fc.weight grad norm: 0.3861715495586395\n",
            "fc.bias grad norm: 0.07367825508117676\n",
            "[Batch 2000] Loss: 0.0834\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.0543050691485405\n",
            "c0 grad norm: 0.04030797258019447\n",
            "conv1.weight grad norm: 0.5052667260169983\n",
            "conv1.bias grad norm: 1.780360037173523e-08\n",
            "batch_norm.weight grad norm: 0.061304301023483276\n",
            "batch_norm.bias grad norm: 0.055989909917116165\n",
            "lstm.weight_ih_l0 grad norm: 0.36621513962745667\n",
            "lstm.weight_hh_l0 grad norm: 0.1272077113389969\n",
            "lstm.bias_ih_l0 grad norm: 0.04137241840362549\n",
            "lstm.bias_hh_l0 grad norm: 0.04137241840362549\n",
            "fc.weight grad norm: 0.4511409401893616\n",
            "fc.bias grad norm: 0.05607890337705612\n",
            "[Batch 3000] Loss: 0.1206\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.03482615947723389\n",
            "c0 grad norm: 0.03221851587295532\n",
            "conv1.weight grad norm: 0.4044764041900635\n",
            "conv1.bias grad norm: 7.3003256773063185e-09\n",
            "batch_norm.weight grad norm: 0.056184764951467514\n",
            "batch_norm.bias grad norm: 0.050240226089954376\n",
            "lstm.weight_ih_l0 grad norm: 0.2908394932746887\n",
            "lstm.weight_hh_l0 grad norm: 0.07771793007850647\n",
            "lstm.bias_ih_l0 grad norm: 0.027896007522940636\n",
            "lstm.bias_hh_l0 grad norm: 0.027896007522940636\n",
            "fc.weight grad norm: 0.28645840287208557\n",
            "fc.bias grad norm: 0.08489474654197693\n",
            "[Batch 4000] Loss: 0.0319\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.019923178479075432\n",
            "c0 grad norm: 0.02615930512547493\n",
            "conv1.weight grad norm: 0.38991791009902954\n",
            "conv1.bias grad norm: 7.187418660237199e-09\n",
            "batch_norm.weight grad norm: 0.04481905698776245\n",
            "batch_norm.bias grad norm: 0.04141774773597717\n",
            "lstm.weight_ih_l0 grad norm: 0.2625604569911957\n",
            "lstm.weight_hh_l0 grad norm: 0.09041298180818558\n",
            "lstm.bias_ih_l0 grad norm: 0.02455984242260456\n",
            "lstm.bias_hh_l0 grad norm: 0.02455984242260456\n",
            "fc.weight grad norm: 0.6737792491912842\n",
            "fc.bias grad norm: 0.2259424775838852\n",
            "[Batch 5000] Loss: 0.1157\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.0096901785582304\n",
            "c0 grad norm: 0.012146067805588245\n",
            "conv1.weight grad norm: 0.30020326375961304\n",
            "conv1.bias grad norm: 8.165082832078951e-09\n",
            "batch_norm.weight grad norm: 0.03987114876508713\n",
            "batch_norm.bias grad norm: 0.028865884989500046\n",
            "lstm.weight_ih_l0 grad norm: 0.24521376192569733\n",
            "lstm.weight_hh_l0 grad norm: 0.061041589826345444\n",
            "lstm.bias_ih_l0 grad norm: 0.01938793621957302\n",
            "lstm.bias_hh_l0 grad norm: 0.01938793621957302\n",
            "fc.weight grad norm: 0.3820112347602844\n",
            "fc.bias grad norm: 0.1263793408870697\n",
            "[Batch 6000] Loss: 0.0500\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.02202894538640976\n",
            "c0 grad norm: 0.019339151680469513\n",
            "conv1.weight grad norm: 0.2650480270385742\n",
            "conv1.bias grad norm: 6.663047003030442e-09\n",
            "batch_norm.weight grad norm: 0.032293274998664856\n",
            "batch_norm.bias grad norm: 0.03045291267335415\n",
            "lstm.weight_ih_l0 grad norm: 0.15980704128742218\n",
            "lstm.weight_hh_l0 grad norm: 0.057658035308122635\n",
            "lstm.bias_ih_l0 grad norm: 0.018024053424596786\n",
            "lstm.bias_hh_l0 grad norm: 0.018024053424596786\n",
            "fc.weight grad norm: 0.2687748670578003\n",
            "fc.bias grad norm: 0.12341853976249695\n",
            "[Batch 7000] Loss: 0.0400\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.00507690291851759\n",
            "c0 grad norm: 0.004491809289902449\n",
            "conv1.weight grad norm: 0.13469088077545166\n",
            "conv1.bias grad norm: 1.58367652325353e-09\n",
            "batch_norm.weight grad norm: 0.008969801478087902\n",
            "batch_norm.bias grad norm: 0.010412294417619705\n",
            "lstm.weight_ih_l0 grad norm: 0.05289079248905182\n",
            "lstm.weight_hh_l0 grad norm: 0.017617471516132355\n",
            "lstm.bias_ih_l0 grad norm: 0.00521779153496027\n",
            "lstm.bias_hh_l0 grad norm: 0.00521779153496027\n",
            "fc.weight grad norm: 0.1398775726556778\n",
            "fc.bias grad norm: 0.057395052164793015\n",
            "[Batch 8000] Loss: 0.0081\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.02348550222814083\n",
            "c0 grad norm: 0.011185306124389172\n",
            "conv1.weight grad norm: 0.32834455370903015\n",
            "conv1.bias grad norm: 4.130200625951375e-09\n",
            "batch_norm.weight grad norm: 0.025903480127453804\n",
            "batch_norm.bias grad norm: 0.035162560641765594\n",
            "lstm.weight_ih_l0 grad norm: 0.1647191047668457\n",
            "lstm.weight_hh_l0 grad norm: 0.056148089468479156\n",
            "lstm.bias_ih_l0 grad norm: 0.017627455294132233\n",
            "lstm.bias_hh_l0 grad norm: 0.017627455294132233\n",
            "fc.weight grad norm: 0.2742398977279663\n",
            "fc.bias grad norm: 0.10560861974954605\n",
            "[Batch 9000] Loss: 0.0551\n",
            "Total Epoch Loss: 876.2071\n",
            "\n",
            "Epoch 96\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03590206801891327\n",
            "c0 grad norm: 0.03841105476021767\n",
            "conv1.weight grad norm: 0.6853932738304138\n",
            "conv1.bias grad norm: 9.832726632907907e-09\n",
            "batch_norm.weight grad norm: 0.0647403746843338\n",
            "batch_norm.bias grad norm: 0.04420702904462814\n",
            "lstm.weight_ih_l0 grad norm: 0.38452035188674927\n",
            "lstm.weight_hh_l0 grad norm: 0.11077944934368134\n",
            "lstm.bias_ih_l0 grad norm: 0.04019685089588165\n",
            "lstm.bias_hh_l0 grad norm: 0.04019685089588165\n",
            "fc.weight grad norm: 0.46724796295166016\n",
            "fc.bias grad norm: 0.23339848220348358\n",
            "[Batch 0] Loss: 0.0996\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.008955509401857853\n",
            "c0 grad norm: 0.005955681204795837\n",
            "conv1.weight grad norm: 0.06896786391735077\n",
            "conv1.bias grad norm: 1.590364062664662e-09\n",
            "batch_norm.weight grad norm: 0.009334241040050983\n",
            "batch_norm.bias grad norm: 0.011838201433420181\n",
            "lstm.weight_ih_l0 grad norm: 0.05413033813238144\n",
            "lstm.weight_hh_l0 grad norm: 0.01733015477657318\n",
            "lstm.bias_ih_l0 grad norm: 0.006396746728569269\n",
            "lstm.bias_hh_l0 grad norm: 0.006396746728569269\n",
            "fc.weight grad norm: 0.2062336653470993\n",
            "fc.bias grad norm: 0.04987229034304619\n",
            "[Batch 1000] Loss: 0.0156\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.02247232384979725\n",
            "c0 grad norm: 0.019194435328245163\n",
            "conv1.weight grad norm: 0.2704295814037323\n",
            "conv1.bias grad norm: 7.361566467523062e-09\n",
            "batch_norm.weight grad norm: 0.028814440593123436\n",
            "batch_norm.bias grad norm: 0.03028995543718338\n",
            "lstm.weight_ih_l0 grad norm: 0.2011519968509674\n",
            "lstm.weight_hh_l0 grad norm: 0.06183994933962822\n",
            "lstm.bias_ih_l0 grad norm: 0.019125595688819885\n",
            "lstm.bias_hh_l0 grad norm: 0.019125595688819885\n",
            "fc.weight grad norm: 0.21751759946346283\n",
            "fc.bias grad norm: 0.05946726351976395\n",
            "[Batch 2000] Loss: 0.0577\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.007271842565387487\n",
            "c0 grad norm: 0.00817216094583273\n",
            "conv1.weight grad norm: 0.1612834632396698\n",
            "conv1.bias grad norm: 2.040690061377859e-09\n",
            "batch_norm.weight grad norm: 0.01653817482292652\n",
            "batch_norm.bias grad norm: 0.017158441245555878\n",
            "lstm.weight_ih_l0 grad norm: 0.1104116216301918\n",
            "lstm.weight_hh_l0 grad norm: 0.03888614475727081\n",
            "lstm.bias_ih_l0 grad norm: 0.011232185177505016\n",
            "lstm.bias_hh_l0 grad norm: 0.011232185177505016\n",
            "fc.weight grad norm: 0.21869690716266632\n",
            "fc.bias grad norm: 0.11921316385269165\n",
            "[Batch 3000] Loss: 0.0248\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.009156984277069569\n",
            "c0 grad norm: 0.00570231769233942\n",
            "conv1.weight grad norm: 0.13501310348510742\n",
            "conv1.bias grad norm: 2.4755677507926066e-09\n",
            "batch_norm.weight grad norm: 0.013932138681411743\n",
            "batch_norm.bias grad norm: 0.010489990003407001\n",
            "lstm.weight_ih_l0 grad norm: 0.060534700751304626\n",
            "lstm.weight_hh_l0 grad norm: 0.01639472134411335\n",
            "lstm.bias_ih_l0 grad norm: 0.006175343878567219\n",
            "lstm.bias_hh_l0 grad norm: 0.006175343878567219\n",
            "fc.weight grad norm: 0.21015408635139465\n",
            "fc.bias grad norm: 0.05747843161225319\n",
            "[Batch 4000] Loss: 0.0214\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.038516148924827576\n",
            "c0 grad norm: 0.02491212822496891\n",
            "conv1.weight grad norm: 0.35550758242607117\n",
            "conv1.bias grad norm: 7.247306754720739e-09\n",
            "batch_norm.weight grad norm: 0.0485699363052845\n",
            "batch_norm.bias grad norm: 0.042993221431970596\n",
            "lstm.weight_ih_l0 grad norm: 0.24983781576156616\n",
            "lstm.weight_hh_l0 grad norm: 0.06871306896209717\n",
            "lstm.bias_ih_l0 grad norm: 0.0251411572098732\n",
            "lstm.bias_hh_l0 grad norm: 0.0251411572098732\n",
            "fc.weight grad norm: 0.5121768116950989\n",
            "fc.bias grad norm: 0.13519196212291718\n",
            "[Batch 5000] Loss: 0.0857\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.021994953975081444\n",
            "c0 grad norm: 0.020753776654601097\n",
            "conv1.weight grad norm: 0.37576597929000854\n",
            "conv1.bias grad norm: 3.805515458310538e-09\n",
            "batch_norm.weight grad norm: 0.03505667671561241\n",
            "batch_norm.bias grad norm: 0.03455762937664986\n",
            "lstm.weight_ih_l0 grad norm: 0.21034863591194153\n",
            "lstm.weight_hh_l0 grad norm: 0.07465073466300964\n",
            "lstm.bias_ih_l0 grad norm: 0.02061612531542778\n",
            "lstm.bias_hh_l0 grad norm: 0.02061612531542778\n",
            "fc.weight grad norm: 0.4281150996685028\n",
            "fc.bias grad norm: 0.110746830701828\n",
            "[Batch 6000] Loss: 0.0842\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.06607247143983841\n",
            "c0 grad norm: 0.02662680298089981\n",
            "conv1.weight grad norm: 0.6894609332084656\n",
            "conv1.bias grad norm: 1.2718027520008945e-08\n",
            "batch_norm.weight grad norm: 0.1209852546453476\n",
            "batch_norm.bias grad norm: 0.062490079551935196\n",
            "lstm.weight_ih_l0 grad norm: 0.5724931359291077\n",
            "lstm.weight_hh_l0 grad norm: 0.1001802608370781\n",
            "lstm.bias_ih_l0 grad norm: 0.04916880279779434\n",
            "lstm.bias_hh_l0 grad norm: 0.04916880279779434\n",
            "fc.weight grad norm: 0.5493358969688416\n",
            "fc.bias grad norm: 0.14751090109348297\n",
            "[Batch 7000] Loss: 0.1283\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.09444400668144226\n",
            "c0 grad norm: 0.039312005043029785\n",
            "conv1.weight grad norm: 1.0473828315734863\n",
            "conv1.bias grad norm: 3.054346109365724e-08\n",
            "batch_norm.weight grad norm: 0.10488580167293549\n",
            "batch_norm.bias grad norm: 0.10422450304031372\n",
            "lstm.weight_ih_l0 grad norm: 0.3741723597049713\n",
            "lstm.weight_hh_l0 grad norm: 0.09568478167057037\n",
            "lstm.bias_ih_l0 grad norm: 0.05406511202454567\n",
            "lstm.bias_hh_l0 grad norm: 0.05406511202454567\n",
            "fc.weight grad norm: 0.48478877544403076\n",
            "fc.bias grad norm: 0.09974080324172974\n",
            "[Batch 8000] Loss: 0.0767\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.04450441151857376\n",
            "c0 grad norm: 0.019003864377737045\n",
            "conv1.weight grad norm: 0.4870903789997101\n",
            "conv1.bias grad norm: 1.2047755681976469e-08\n",
            "batch_norm.weight grad norm: 0.057677898555994034\n",
            "batch_norm.bias grad norm: 0.04182562232017517\n",
            "lstm.weight_ih_l0 grad norm: 0.2632717788219452\n",
            "lstm.weight_hh_l0 grad norm: 0.07015921175479889\n",
            "lstm.bias_ih_l0 grad norm: 0.029634738340973854\n",
            "lstm.bias_hh_l0 grad norm: 0.029634738340973854\n",
            "fc.weight grad norm: 0.4730244576931\n",
            "fc.bias grad norm: 0.1357094943523407\n",
            "[Batch 9000] Loss: 0.0932\n",
            "Total Epoch Loss: 882.0634\n",
            "\n",
            "Epoch 97\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.1000155434012413\n",
            "c0 grad norm: 0.06708018481731415\n",
            "conv1.weight grad norm: 1.017175555229187\n",
            "conv1.bias grad norm: 3.5594734271171546e-08\n",
            "batch_norm.weight grad norm: 0.15108488500118256\n",
            "batch_norm.bias grad norm: 0.10035677254199982\n",
            "lstm.weight_ih_l0 grad norm: 0.8305661678314209\n",
            "lstm.weight_hh_l0 grad norm: 0.26224690675735474\n",
            "lstm.bias_ih_l0 grad norm: 0.08608462661504745\n",
            "lstm.bias_hh_l0 grad norm: 0.08608462661504745\n",
            "fc.weight grad norm: 0.9751655459403992\n",
            "fc.bias grad norm: 0.2531329393386841\n",
            "[Batch 0] Loss: 0.4221\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.014718281105160713\n",
            "c0 grad norm: 0.012810937128961086\n",
            "conv1.weight grad norm: 0.254043847322464\n",
            "conv1.bias grad norm: 5.610001796441111e-09\n",
            "batch_norm.weight grad norm: 0.03356747329235077\n",
            "batch_norm.bias grad norm: 0.021544262766838074\n",
            "lstm.weight_ih_l0 grad norm: 0.13792505860328674\n",
            "lstm.weight_hh_l0 grad norm: 0.04238384589552879\n",
            "lstm.bias_ih_l0 grad norm: 0.01401086337864399\n",
            "lstm.bias_hh_l0 grad norm: 0.01401086337864399\n",
            "fc.weight grad norm: 0.29036396741867065\n",
            "fc.bias grad norm: 0.12797120213508606\n",
            "[Batch 1000] Loss: 0.0377\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.023438794538378716\n",
            "c0 grad norm: 0.01891488954424858\n",
            "conv1.weight grad norm: 0.3054763078689575\n",
            "conv1.bias grad norm: 5.704290817476476e-09\n",
            "batch_norm.weight grad norm: 0.033639419823884964\n",
            "batch_norm.bias grad norm: 0.024852601811289787\n",
            "lstm.weight_ih_l0 grad norm: 0.16811567544937134\n",
            "lstm.weight_hh_l0 grad norm: 0.0532892607152462\n",
            "lstm.bias_ih_l0 grad norm: 0.01608848012983799\n",
            "lstm.bias_hh_l0 grad norm: 0.01608848012983799\n",
            "fc.weight grad norm: 0.43613380193710327\n",
            "fc.bias grad norm: 0.1412605196237564\n",
            "[Batch 2000] Loss: 0.0746\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.05015425384044647\n",
            "c0 grad norm: 0.0172813069075346\n",
            "conv1.weight grad norm: 0.7573758363723755\n",
            "conv1.bias grad norm: 1.1761782658936681e-08\n",
            "batch_norm.weight grad norm: 0.049294959753751755\n",
            "batch_norm.bias grad norm: 0.04081480950117111\n",
            "lstm.weight_ih_l0 grad norm: 0.3070385754108429\n",
            "lstm.weight_hh_l0 grad norm: 0.08534538000822067\n",
            "lstm.bias_ih_l0 grad norm: 0.03200528770685196\n",
            "lstm.bias_hh_l0 grad norm: 0.03200528770685196\n",
            "fc.weight grad norm: 0.49194833636283875\n",
            "fc.bias grad norm: 0.18052995204925537\n",
            "[Batch 3000] Loss: 0.0796\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.026071015745401382\n",
            "c0 grad norm: 0.012584040872752666\n",
            "conv1.weight grad norm: 0.24188151955604553\n",
            "conv1.bias grad norm: 4.797985564408691e-09\n",
            "batch_norm.weight grad norm: 0.025822680443525314\n",
            "batch_norm.bias grad norm: 0.033384792506694794\n",
            "lstm.weight_ih_l0 grad norm: 0.15951859951019287\n",
            "lstm.weight_hh_l0 grad norm: 0.05092107877135277\n",
            "lstm.bias_ih_l0 grad norm: 0.01938299648463726\n",
            "lstm.bias_hh_l0 grad norm: 0.01938299648463726\n",
            "fc.weight grad norm: 0.3905966281890869\n",
            "fc.bias grad norm: 0.14571337401866913\n",
            "[Batch 4000] Loss: 0.0689\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.008305134251713753\n",
            "c0 grad norm: 0.007237695623189211\n",
            "conv1.weight grad norm: 0.16141259670257568\n",
            "conv1.bias grad norm: 3.994827135755941e-09\n",
            "batch_norm.weight grad norm: 0.021900415420532227\n",
            "batch_norm.bias grad norm: 0.019216317683458328\n",
            "lstm.weight_ih_l0 grad norm: 0.10868554562330246\n",
            "lstm.weight_hh_l0 grad norm: 0.03441210091114044\n",
            "lstm.bias_ih_l0 grad norm: 0.00988275557756424\n",
            "lstm.bias_hh_l0 grad norm: 0.00988275557756424\n",
            "fc.weight grad norm: 0.17886774241924286\n",
            "fc.bias grad norm: 0.040752965956926346\n",
            "[Batch 5000] Loss: 0.0424\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.014501194469630718\n",
            "c0 grad norm: 0.011878406628966331\n",
            "conv1.weight grad norm: 0.26036638021469116\n",
            "conv1.bias grad norm: 4.7327231023075456e-09\n",
            "batch_norm.weight grad norm: 0.02473348006606102\n",
            "batch_norm.bias grad norm: 0.02465362846851349\n",
            "lstm.weight_ih_l0 grad norm: 0.13203029334545135\n",
            "lstm.weight_hh_l0 grad norm: 0.04611070826649666\n",
            "lstm.bias_ih_l0 grad norm: 0.01348581351339817\n",
            "lstm.bias_hh_l0 grad norm: 0.01348581351339817\n",
            "fc.weight grad norm: 0.30916455388069153\n",
            "fc.bias grad norm: 0.04483324661850929\n",
            "[Batch 6000] Loss: 0.0473\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.04774085804820061\n",
            "c0 grad norm: 0.026341265067458153\n",
            "conv1.weight grad norm: 0.488068550825119\n",
            "conv1.bias grad norm: 9.227294484048798e-09\n",
            "batch_norm.weight grad norm: 0.05224641412496567\n",
            "batch_norm.bias grad norm: 0.05737471207976341\n",
            "lstm.weight_ih_l0 grad norm: 0.23774048686027527\n",
            "lstm.weight_hh_l0 grad norm: 0.07110025733709335\n",
            "lstm.bias_ih_l0 grad norm: 0.023176584392786026\n",
            "lstm.bias_hh_l0 grad norm: 0.023176584392786026\n",
            "fc.weight grad norm: 0.4366404712200165\n",
            "fc.bias grad norm: 0.22754596173763275\n",
            "[Batch 7000] Loss: 0.0889\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.01816098764538765\n",
            "c0 grad norm: 0.018288206309080124\n",
            "conv1.weight grad norm: 0.2504301965236664\n",
            "conv1.bias grad norm: 5.253447898923014e-09\n",
            "batch_norm.weight grad norm: 0.02701018936932087\n",
            "batch_norm.bias grad norm: 0.026195017620921135\n",
            "lstm.weight_ih_l0 grad norm: 0.14255863428115845\n",
            "lstm.weight_hh_l0 grad norm: 0.050690460950136185\n",
            "lstm.bias_ih_l0 grad norm: 0.016001664102077484\n",
            "lstm.bias_hh_l0 grad norm: 0.016001664102077484\n",
            "fc.weight grad norm: 0.2789158821105957\n",
            "fc.bias grad norm: 0.1353636085987091\n",
            "[Batch 8000] Loss: 0.0474\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.12012829631567001\n",
            "c0 grad norm: 0.06829037517309189\n",
            "conv1.weight grad norm: 1.3003445863723755\n",
            "conv1.bias grad norm: 3.2795753668324323e-08\n",
            "batch_norm.weight grad norm: 0.13424354791641235\n",
            "batch_norm.bias grad norm: 0.12856590747833252\n",
            "lstm.weight_ih_l0 grad norm: 0.629242479801178\n",
            "lstm.weight_hh_l0 grad norm: 0.1872829794883728\n",
            "lstm.bias_ih_l0 grad norm: 0.0730753242969513\n",
            "lstm.bias_hh_l0 grad norm: 0.0730753242969513\n",
            "fc.weight grad norm: 0.8044232130050659\n",
            "fc.bias grad norm: 0.26525983214378357\n",
            "[Batch 9000] Loss: 0.2946\n",
            "Total Epoch Loss: 879.8440\n",
            "\n",
            "Epoch 98\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.018136974424123764\n",
            "c0 grad norm: 0.011764484457671642\n",
            "conv1.weight grad norm: 0.15211805701255798\n",
            "conv1.bias grad norm: 3.008953752825505e-09\n",
            "batch_norm.weight grad norm: 0.01984631083905697\n",
            "batch_norm.bias grad norm: 0.017205920070409775\n",
            "lstm.weight_ih_l0 grad norm: 0.11524194478988647\n",
            "lstm.weight_hh_l0 grad norm: 0.030442025512456894\n",
            "lstm.bias_ih_l0 grad norm: 0.012226847000420094\n",
            "lstm.bias_hh_l0 grad norm: 0.012226847000420094\n",
            "fc.weight grad norm: 0.21661768853664398\n",
            "fc.bias grad norm: 0.0830974206328392\n",
            "[Batch 0] Loss: 0.0158\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.03585648536682129\n",
            "c0 grad norm: 0.021836983039975166\n",
            "conv1.weight grad norm: 0.3708678185939789\n",
            "conv1.bias grad norm: 8.392517791833143e-09\n",
            "batch_norm.weight grad norm: 0.03737953305244446\n",
            "batch_norm.bias grad norm: 0.03374246880412102\n",
            "lstm.weight_ih_l0 grad norm: 0.19276508688926697\n",
            "lstm.weight_hh_l0 grad norm: 0.049670685082674026\n",
            "lstm.bias_ih_l0 grad norm: 0.019397415220737457\n",
            "lstm.bias_hh_l0 grad norm: 0.019397415220737457\n",
            "fc.weight grad norm: 0.22809265553951263\n",
            "fc.bias grad norm: 0.05974813923239708\n",
            "[Batch 1000] Loss: 0.0428\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.015145612880587578\n",
            "c0 grad norm: 0.02064421772956848\n",
            "conv1.weight grad norm: 0.29568013548851013\n",
            "conv1.bias grad norm: 6.387731232848637e-09\n",
            "batch_norm.weight grad norm: 0.031097348779439926\n",
            "batch_norm.bias grad norm: 0.029819341376423836\n",
            "lstm.weight_ih_l0 grad norm: 0.17006856203079224\n",
            "lstm.weight_hh_l0 grad norm: 0.05056655779480934\n",
            "lstm.bias_ih_l0 grad norm: 0.015845688059926033\n",
            "lstm.bias_hh_l0 grad norm: 0.015845688059926033\n",
            "fc.weight grad norm: 0.2676643431186676\n",
            "fc.bias grad norm: 0.085023894906044\n",
            "[Batch 2000] Loss: 0.0553\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.04579528048634529\n",
            "c0 grad norm: 0.026916449889540672\n",
            "conv1.weight grad norm: 0.6880252361297607\n",
            "conv1.bias grad norm: 1.2502257007440676e-08\n",
            "batch_norm.weight grad norm: 0.05617106705904007\n",
            "batch_norm.bias grad norm: 0.04538220539689064\n",
            "lstm.weight_ih_l0 grad norm: 0.3473389744758606\n",
            "lstm.weight_hh_l0 grad norm: 0.08137209713459015\n",
            "lstm.bias_ih_l0 grad norm: 0.031304582953453064\n",
            "lstm.bias_hh_l0 grad norm: 0.031304582953453064\n",
            "fc.weight grad norm: 0.4238666296005249\n",
            "fc.bias grad norm: 0.14677007496356964\n",
            "[Batch 3000] Loss: 0.1482\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.018080327659845352\n",
            "c0 grad norm: 0.014344756491482258\n",
            "conv1.weight grad norm: 0.3995586037635803\n",
            "conv1.bias grad norm: 7.0985421984914865e-09\n",
            "batch_norm.weight grad norm: 0.03497639670968056\n",
            "batch_norm.bias grad norm: 0.039670899510383606\n",
            "lstm.weight_ih_l0 grad norm: 0.19986040890216827\n",
            "lstm.weight_hh_l0 grad norm: 0.0762808695435524\n",
            "lstm.bias_ih_l0 grad norm: 0.021782753989100456\n",
            "lstm.bias_hh_l0 grad norm: 0.021782753989100456\n",
            "fc.weight grad norm: 0.2609543204307556\n",
            "fc.bias grad norm: 0.040243927389383316\n",
            "[Batch 4000] Loss: 0.0717\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.046109531074762344\n",
            "c0 grad norm: 0.015614929609000683\n",
            "conv1.weight grad norm: 0.22785863280296326\n",
            "conv1.bias grad norm: 4.363303496290882e-09\n",
            "batch_norm.weight grad norm: 0.024788077920675278\n",
            "batch_norm.bias grad norm: 0.03347640484571457\n",
            "lstm.weight_ih_l0 grad norm: 0.12126756459474564\n",
            "lstm.weight_hh_l0 grad norm: 0.04441410303115845\n",
            "lstm.bias_ih_l0 grad norm: 0.01807033084332943\n",
            "lstm.bias_hh_l0 grad norm: 0.01807033084332943\n",
            "fc.weight grad norm: 0.25724631547927856\n",
            "fc.bias grad norm: 0.07192449271678925\n",
            "[Batch 5000] Loss: 0.0324\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.06449142843484879\n",
            "c0 grad norm: 0.03759755566716194\n",
            "conv1.weight grad norm: 0.6423958539962769\n",
            "conv1.bias grad norm: 1.3257329456450861e-08\n",
            "batch_norm.weight grad norm: 0.08874551951885223\n",
            "batch_norm.bias grad norm: 0.07099451869726181\n",
            "lstm.weight_ih_l0 grad norm: 0.4154151976108551\n",
            "lstm.weight_hh_l0 grad norm: 0.11972873657941818\n",
            "lstm.bias_ih_l0 grad norm: 0.03423090651631355\n",
            "lstm.bias_hh_l0 grad norm: 0.03423090651631355\n",
            "fc.weight grad norm: 0.46522510051727295\n",
            "fc.bias grad norm: 0.12668146193027496\n",
            "[Batch 6000] Loss: 0.1946\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.026820244267582893\n",
            "c0 grad norm: 0.019090045243501663\n",
            "conv1.weight grad norm: 0.3089546859264374\n",
            "conv1.bias grad norm: 5.421208371103603e-09\n",
            "batch_norm.weight grad norm: 0.032479748129844666\n",
            "batch_norm.bias grad norm: 0.0313694104552269\n",
            "lstm.weight_ih_l0 grad norm: 0.20847994089126587\n",
            "lstm.weight_hh_l0 grad norm: 0.0649857223033905\n",
            "lstm.bias_ih_l0 grad norm: 0.02317613922059536\n",
            "lstm.bias_hh_l0 grad norm: 0.02317613922059536\n",
            "fc.weight grad norm: 0.4304005205631256\n",
            "fc.bias grad norm: 0.10389228910207748\n",
            "[Batch 7000] Loss: 0.1023\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.05050737038254738\n",
            "c0 grad norm: 0.03331439197063446\n",
            "conv1.weight grad norm: 0.5384299159049988\n",
            "conv1.bias grad norm: 1.263603266465907e-08\n",
            "batch_norm.weight grad norm: 0.05444345250725746\n",
            "batch_norm.bias grad norm: 0.0791623666882515\n",
            "lstm.weight_ih_l0 grad norm: 0.2986183762550354\n",
            "lstm.weight_hh_l0 grad norm: 0.10386115312576294\n",
            "lstm.bias_ih_l0 grad norm: 0.03481339290738106\n",
            "lstm.bias_hh_l0 grad norm: 0.03481339290738106\n",
            "fc.weight grad norm: 0.2979680895805359\n",
            "fc.bias grad norm: 0.0945027694106102\n",
            "[Batch 8000] Loss: 0.0887\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.02378835715353489\n",
            "c0 grad norm: 0.0342656746506691\n",
            "conv1.weight grad norm: 0.2110408991575241\n",
            "conv1.bias grad norm: 5.502670319401659e-09\n",
            "batch_norm.weight grad norm: 0.031759776175022125\n",
            "batch_norm.bias grad norm: 0.037594158202409744\n",
            "lstm.weight_ih_l0 grad norm: 0.19795505702495575\n",
            "lstm.weight_hh_l0 grad norm: 0.06927008181810379\n",
            "lstm.bias_ih_l0 grad norm: 0.022492218762636185\n",
            "lstm.bias_hh_l0 grad norm: 0.022492218762636185\n",
            "fc.weight grad norm: 0.6698652505874634\n",
            "fc.bias grad norm: 0.24883590638637543\n",
            "[Batch 9000] Loss: 0.1208\n",
            "Total Epoch Loss: 881.7057\n",
            "\n",
            "Epoch 99\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.012370940297842026\n",
            "c0 grad norm: 0.013433098793029785\n",
            "conv1.weight grad norm: 0.30674219131469727\n",
            "conv1.bias grad norm: 3.5305895984549807e-09\n",
            "batch_norm.weight grad norm: 0.024406444281339645\n",
            "batch_norm.bias grad norm: 0.01616966538131237\n",
            "lstm.weight_ih_l0 grad norm: 0.1034429743885994\n",
            "lstm.weight_hh_l0 grad norm: 0.02898285537958145\n",
            "lstm.bias_ih_l0 grad norm: 0.009530600160360336\n",
            "lstm.bias_hh_l0 grad norm: 0.009530600160360336\n",
            "fc.weight grad norm: 0.265213280916214\n",
            "fc.bias grad norm: 0.09546305239200592\n",
            "[Batch 0] Loss: 0.0220\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.045665644109249115\n",
            "c0 grad norm: 0.04249334707856178\n",
            "conv1.weight grad norm: 0.4946228563785553\n",
            "conv1.bias grad norm: 8.946797969144882e-09\n",
            "batch_norm.weight grad norm: 0.07092679291963577\n",
            "batch_norm.bias grad norm: 0.053913768380880356\n",
            "lstm.weight_ih_l0 grad norm: 0.4124376177787781\n",
            "lstm.weight_hh_l0 grad norm: 0.11067868024110794\n",
            "lstm.bias_ih_l0 grad norm: 0.036690786480903625\n",
            "lstm.bias_hh_l0 grad norm: 0.036690786480903625\n",
            "fc.weight grad norm: 0.7179240584373474\n",
            "fc.bias grad norm: 0.2210959643125534\n",
            "[Batch 1000] Loss: 0.1992\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.04325033351778984\n",
            "c0 grad norm: 0.02786526083946228\n",
            "conv1.weight grad norm: 0.406064510345459\n",
            "conv1.bias grad norm: 1.084121947769745e-08\n",
            "batch_norm.weight grad norm: 0.04740627110004425\n",
            "batch_norm.bias grad norm: 0.05217248946428299\n",
            "lstm.weight_ih_l0 grad norm: 0.2556743621826172\n",
            "lstm.weight_hh_l0 grad norm: 0.09702648967504501\n",
            "lstm.bias_ih_l0 grad norm: 0.030237505212426186\n",
            "lstm.bias_hh_l0 grad norm: 0.030237505212426186\n",
            "fc.weight grad norm: 0.4604014456272125\n",
            "fc.bias grad norm: 0.11634575575590134\n",
            "[Batch 2000] Loss: 0.0775\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.031031936407089233\n",
            "c0 grad norm: 0.023225747048854828\n",
            "conv1.weight grad norm: 0.308340847492218\n",
            "conv1.bias grad norm: 5.348620657485981e-09\n",
            "batch_norm.weight grad norm: 0.03530046343803406\n",
            "batch_norm.bias grad norm: 0.04282723739743233\n",
            "lstm.weight_ih_l0 grad norm: 0.17746928334236145\n",
            "lstm.weight_hh_l0 grad norm: 0.06967997550964355\n",
            "lstm.bias_ih_l0 grad norm: 0.021903295069932938\n",
            "lstm.bias_hh_l0 grad norm: 0.021903295069932938\n",
            "fc.weight grad norm: 0.34967637062072754\n",
            "fc.bias grad norm: 0.19207647442817688\n",
            "[Batch 3000] Loss: 0.0609\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.044608306139707565\n",
            "c0 grad norm: 0.040588900446891785\n",
            "conv1.weight grad norm: 0.5707058310508728\n",
            "conv1.bias grad norm: 1.0959321450343396e-08\n",
            "batch_norm.weight grad norm: 0.06384499371051788\n",
            "batch_norm.bias grad norm: 0.05690706893801689\n",
            "lstm.weight_ih_l0 grad norm: 0.3354782164096832\n",
            "lstm.weight_hh_l0 grad norm: 0.13749772310256958\n",
            "lstm.bias_ih_l0 grad norm: 0.03868649899959564\n",
            "lstm.bias_hh_l0 grad norm: 0.03868649899959564\n",
            "fc.weight grad norm: 0.4981897473335266\n",
            "fc.bias grad norm: 0.18005500733852386\n",
            "[Batch 4000] Loss: 0.1872\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.046483833342790604\n",
            "c0 grad norm: 0.05223194137215614\n",
            "conv1.weight grad norm: 0.4491327404975891\n",
            "conv1.bias grad norm: 9.413064994134857e-09\n",
            "batch_norm.weight grad norm: 0.057629335671663284\n",
            "batch_norm.bias grad norm: 0.07435110956430435\n",
            "lstm.weight_ih_l0 grad norm: 0.3623228967189789\n",
            "lstm.weight_hh_l0 grad norm: 0.11848719418048859\n",
            "lstm.bias_ih_l0 grad norm: 0.04393197223544121\n",
            "lstm.bias_hh_l0 grad norm: 0.04393197223544121\n",
            "fc.weight grad norm: 0.9386767148971558\n",
            "fc.bias grad norm: 0.23830533027648926\n",
            "[Batch 5000] Loss: 0.4397\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.023038946092128754\n",
            "c0 grad norm: 0.0278177447617054\n",
            "conv1.weight grad norm: 0.27983465790748596\n",
            "conv1.bias grad norm: 5.484654508336462e-09\n",
            "batch_norm.weight grad norm: 0.03457155451178551\n",
            "batch_norm.bias grad norm: 0.029991379007697105\n",
            "lstm.weight_ih_l0 grad norm: 0.19746361672878265\n",
            "lstm.weight_hh_l0 grad norm: 0.066969133913517\n",
            "lstm.bias_ih_l0 grad norm: 0.022495565935969353\n",
            "lstm.bias_hh_l0 grad norm: 0.022495565935969353\n",
            "fc.weight grad norm: 0.26712432503700256\n",
            "fc.bias grad norm: 0.07723810523748398\n",
            "[Batch 6000] Loss: 0.0374\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.028990935534238815\n",
            "c0 grad norm: 0.023110488429665565\n",
            "conv1.weight grad norm: 0.30110010504722595\n",
            "conv1.bias grad norm: 5.620069742917622e-09\n",
            "batch_norm.weight grad norm: 0.047398604452610016\n",
            "batch_norm.bias grad norm: 0.03740193322300911\n",
            "lstm.weight_ih_l0 grad norm: 0.20639510452747345\n",
            "lstm.weight_hh_l0 grad norm: 0.06089090555906296\n",
            "lstm.bias_ih_l0 grad norm: 0.023520344868302345\n",
            "lstm.bias_hh_l0 grad norm: 0.023520344868302345\n",
            "fc.weight grad norm: 0.2579626739025116\n",
            "fc.bias grad norm: 0.10389509797096252\n",
            "[Batch 7000] Loss: 0.0472\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.03040385991334915\n",
            "c0 grad norm: 0.03700908273458481\n",
            "conv1.weight grad norm: 0.32204893231391907\n",
            "conv1.bias grad norm: 6.605937574732934e-09\n",
            "batch_norm.weight grad norm: 0.026755576953291893\n",
            "batch_norm.bias grad norm: 0.03906450793147087\n",
            "lstm.weight_ih_l0 grad norm: 0.18925172090530396\n",
            "lstm.weight_hh_l0 grad norm: 0.07456369698047638\n",
            "lstm.bias_ih_l0 grad norm: 0.02464907430112362\n",
            "lstm.bias_hh_l0 grad norm: 0.02464907430112362\n",
            "fc.weight grad norm: 0.41872638463974\n",
            "fc.bias grad norm: 0.16903094947338104\n",
            "[Batch 8000] Loss: 0.0741\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.012239915318787098\n",
            "c0 grad norm: 0.010834217071533203\n",
            "conv1.weight grad norm: 0.1948038786649704\n",
            "conv1.bias grad norm: 2.6707671629822016e-09\n",
            "batch_norm.weight grad norm: 0.02303154580295086\n",
            "batch_norm.bias grad norm: 0.022036192938685417\n",
            "lstm.weight_ih_l0 grad norm: 0.1113402470946312\n",
            "lstm.weight_hh_l0 grad norm: 0.04017580300569534\n",
            "lstm.bias_ih_l0 grad norm: 0.012004872784018517\n",
            "lstm.bias_hh_l0 grad norm: 0.012004872784018517\n",
            "fc.weight grad norm: 0.33434486389160156\n",
            "fc.bias grad norm: 0.16900824010372162\n",
            "[Batch 9000] Loss: 0.0447\n",
            "Total Epoch Loss: 879.7580\n",
            "\n",
            "Epoch 100\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.022703390568494797\n",
            "c0 grad norm: 0.008879820816218853\n",
            "conv1.weight grad norm: 0.2078261524438858\n",
            "conv1.bias grad norm: 3.190659514373806e-09\n",
            "batch_norm.weight grad norm: 0.021224016323685646\n",
            "batch_norm.bias grad norm: 0.02288709580898285\n",
            "lstm.weight_ih_l0 grad norm: 0.10780928283929825\n",
            "lstm.weight_hh_l0 grad norm: 0.02886325865983963\n",
            "lstm.bias_ih_l0 grad norm: 0.012340117245912552\n",
            "lstm.bias_hh_l0 grad norm: 0.012340117245912552\n",
            "fc.weight grad norm: 0.2078012377023697\n",
            "fc.bias grad norm: 0.08842042833566666\n",
            "[Batch 0] Loss: 0.0109\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.02917642891407013\n",
            "c0 grad norm: 0.017774533480405807\n",
            "conv1.weight grad norm: 0.19242243468761444\n",
            "conv1.bias grad norm: 5.288068649633715e-09\n",
            "batch_norm.weight grad norm: 0.027159271761775017\n",
            "batch_norm.bias grad norm: 0.026305144652724266\n",
            "lstm.weight_ih_l0 grad norm: 0.1746104508638382\n",
            "lstm.weight_hh_l0 grad norm: 0.048178695142269135\n",
            "lstm.bias_ih_l0 grad norm: 0.016095394268631935\n",
            "lstm.bias_hh_l0 grad norm: 0.016095394268631935\n",
            "fc.weight grad norm: 0.25910624861717224\n",
            "fc.bias grad norm: 0.0994725301861763\n",
            "[Batch 1000] Loss: 0.0494\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.030878785997629166\n",
            "c0 grad norm: 0.014000996015965939\n",
            "conv1.weight grad norm: 0.20777784287929535\n",
            "conv1.bias grad norm: 4.92351226455412e-09\n",
            "batch_norm.weight grad norm: 0.02292502485215664\n",
            "batch_norm.bias grad norm: 0.025801178067922592\n",
            "lstm.weight_ih_l0 grad norm: 0.14891411364078522\n",
            "lstm.weight_hh_l0 grad norm: 0.05173177272081375\n",
            "lstm.bias_ih_l0 grad norm: 0.016499480232596397\n",
            "lstm.bias_hh_l0 grad norm: 0.016499480232596397\n",
            "fc.weight grad norm: 0.25042012333869934\n",
            "fc.bias grad norm: 0.11204138398170471\n",
            "[Batch 2000] Loss: 0.0280\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.04430241510272026\n",
            "c0 grad norm: 0.019079308956861496\n",
            "conv1.weight grad norm: 0.2595788836479187\n",
            "conv1.bias grad norm: 5.871333641493948e-09\n",
            "batch_norm.weight grad norm: 0.040380991995334625\n",
            "batch_norm.bias grad norm: 0.034958790987730026\n",
            "lstm.weight_ih_l0 grad norm: 0.2698967754840851\n",
            "lstm.weight_hh_l0 grad norm: 0.07628542929887772\n",
            "lstm.bias_ih_l0 grad norm: 0.033815205097198486\n",
            "lstm.bias_hh_l0 grad norm: 0.033815205097198486\n",
            "fc.weight grad norm: 0.5625935792922974\n",
            "fc.bias grad norm: 0.16367705166339874\n",
            "[Batch 3000] Loss: 0.1044\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.022808751091361046\n",
            "c0 grad norm: 0.025726668536663055\n",
            "conv1.weight grad norm: 0.4336939752101898\n",
            "conv1.bias grad norm: 7.749260788614265e-09\n",
            "batch_norm.weight grad norm: 0.03900440037250519\n",
            "batch_norm.bias grad norm: 0.046434056013822556\n",
            "lstm.weight_ih_l0 grad norm: 0.23542113602161407\n",
            "lstm.weight_hh_l0 grad norm: 0.07379080355167389\n",
            "lstm.bias_ih_l0 grad norm: 0.02424711175262928\n",
            "lstm.bias_hh_l0 grad norm: 0.02424711175262928\n",
            "fc.weight grad norm: 0.5979500412940979\n",
            "fc.bias grad norm: 0.26881805062294006\n",
            "[Batch 4000] Loss: 0.1205\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.007035966031253338\n",
            "c0 grad norm: 0.0070363664999604225\n",
            "conv1.weight grad norm: 0.16732653975486755\n",
            "conv1.bias grad norm: 3.798112491182337e-09\n",
            "batch_norm.weight grad norm: 0.021823067218065262\n",
            "batch_norm.bias grad norm: 0.019977087154984474\n",
            "lstm.weight_ih_l0 grad norm: 0.10848252475261688\n",
            "lstm.weight_hh_l0 grad norm: 0.02897404506802559\n",
            "lstm.bias_ih_l0 grad norm: 0.008393187075853348\n",
            "lstm.bias_hh_l0 grad norm: 0.008393187075853348\n",
            "fc.weight grad norm: 0.09419827908277512\n",
            "fc.bias grad norm: 0.02850804105401039\n",
            "[Batch 5000] Loss: 0.0130\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.038329750299453735\n",
            "c0 grad norm: 0.01988978125154972\n",
            "conv1.weight grad norm: 0.2865847647190094\n",
            "conv1.bias grad norm: 1.0092494839852861e-08\n",
            "batch_norm.weight grad norm: 0.028866592794656754\n",
            "batch_norm.bias grad norm: 0.03689919039607048\n",
            "lstm.weight_ih_l0 grad norm: 0.18428495526313782\n",
            "lstm.weight_hh_l0 grad norm: 0.044255487620830536\n",
            "lstm.bias_ih_l0 grad norm: 0.020337605848908424\n",
            "lstm.bias_hh_l0 grad norm: 0.020337605848908424\n",
            "fc.weight grad norm: 0.288023978471756\n",
            "fc.bias grad norm: 0.08997909724712372\n",
            "[Batch 6000] Loss: 0.0234\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.014526666142046452\n",
            "c0 grad norm: 0.014598513953387737\n",
            "conv1.weight grad norm: 0.3103850185871124\n",
            "conv1.bias grad norm: 5.684135828687431e-09\n",
            "batch_norm.weight grad norm: 0.030413081869482994\n",
            "batch_norm.bias grad norm: 0.026893388479948044\n",
            "lstm.weight_ih_l0 grad norm: 0.18584753572940826\n",
            "lstm.weight_hh_l0 grad norm: 0.052280765026807785\n",
            "lstm.bias_ih_l0 grad norm: 0.01682092621922493\n",
            "lstm.bias_hh_l0 grad norm: 0.01682092621922493\n",
            "fc.weight grad norm: 0.3794406056404114\n",
            "fc.bias grad norm: 0.13754595816135406\n",
            "[Batch 7000] Loss: 0.0598\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.0482492670416832\n",
            "c0 grad norm: 0.02868334949016571\n",
            "conv1.weight grad norm: 0.669479250907898\n",
            "conv1.bias grad norm: 8.844200927171642e-09\n",
            "batch_norm.weight grad norm: 0.06716365367174149\n",
            "batch_norm.bias grad norm: 0.0751136988401413\n",
            "lstm.weight_ih_l0 grad norm: 0.26669666171073914\n",
            "lstm.weight_hh_l0 grad norm: 0.07265567034482956\n",
            "lstm.bias_ih_l0 grad norm: 0.02801588736474514\n",
            "lstm.bias_hh_l0 grad norm: 0.02801588736474514\n",
            "fc.weight grad norm: 0.40726080536842346\n",
            "fc.bias grad norm: 0.10679155588150024\n",
            "[Batch 8000] Loss: 0.0864\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.014182919636368752\n",
            "c0 grad norm: 0.013754433020949364\n",
            "conv1.weight grad norm: 0.20851002633571625\n",
            "conv1.bias grad norm: 3.1102915798442154e-09\n",
            "batch_norm.weight grad norm: 0.02429170161485672\n",
            "batch_norm.bias grad norm: 0.018907776102423668\n",
            "lstm.weight_ih_l0 grad norm: 0.1277981698513031\n",
            "lstm.weight_hh_l0 grad norm: 0.04527435824275017\n",
            "lstm.bias_ih_l0 grad norm: 0.012202107347548008\n",
            "lstm.bias_hh_l0 grad norm: 0.012202107347548008\n",
            "fc.weight grad norm: 0.20104748010635376\n",
            "fc.bias grad norm: 0.028691431507468224\n",
            "[Batch 9000] Loss: 0.0254\n",
            "Total Epoch Loss: 878.8804\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation Loop**"
      ],
      "metadata": {
        "id": "KQZnmQwHKgtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "NHST5l_jKncM"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        preds = model(batch_X)\n",
        "        all_preds.append(preds)\n",
        "        all_targets.append(batch_y)"
      ],
      "metadata": {
        "id": "SMC0mH6mKlXy"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
        "all_targets = torch.cat(all_targets, dim=0).cpu().numpy()"
      ],
      "metadata": {
        "id": "LPJ5NNNkKsJj"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(all_targets, all_preds)\n",
        "mae = mean_absolute_error(all_targets, all_preds)\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev97wbzbKxAE",
        "outputId": "514906d6-e481-4753-afdc-51e594906827"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.2657\n",
            "MAE: 0.3696\n",
            "R² Score: 0.8110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Model**"
      ],
      "metadata": {
        "id": "eV3uq9_aMCjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'convlstm.pth')"
      ],
      "metadata": {
        "id": "M_b_jbDWKy73"
      },
      "execution_count": 107,
      "outputs": []
    }
  ]
}
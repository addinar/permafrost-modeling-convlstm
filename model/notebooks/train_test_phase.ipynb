{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7zfSKw1D/r4+brwHxPxfq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/addinar/permafrost-modeling-convlstm/blob/main/model/notebooks/train_test_phase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Packages and Libraries**"
      ],
      "metadata": {
        "id": "tdu99N1VNCDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAzksnsrM8JA",
        "outputId": "a1fb21cc-6567-4837-91d5-36d1300cc17d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Have `convlstm.py` and `dataset.py` loaded in colab files."
      ],
      "metadata": {
        "id": "lonxy5nINGIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content')"
      ],
      "metadata": {
        "id": "pHGh3yUFOMz7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "UtP6Ul_3PYDZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "rqec8Mn2pElS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3b18WvROqi9",
        "outputId": "14b542ca-7a4d-40f0-c37a-f8456dfe574a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Inspect DF**"
      ],
      "metadata": {
        "id": "3DbnOUThPsdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = userdata.get('preprocessed_path')\n",
        "df = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "oULEuyk_PKTd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN9oLVcEPouY",
        "outputId": "7fb3e8ae-2e17-4026-e5aa-95544b7f3d6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'date', 'snow_albedo', 'snow_cover', 'snow_density',\n",
              "       'snow_depth', 'snowfall_sum', 'surface_latent_heat_flux_sum',\n",
              "       'surface_sensible_heat_flux_sum',\n",
              "       'surface_solar_radiation_downwards_sum',\n",
              "       'surface_thermal_radiation_downwards_sum', 'skin_temperature',\n",
              "       'total_precipitation_sum', 'avg_volumetric_water_content',\n",
              "       'TBFI_skin_temperature', 'TBFI_soil_temperature_level_1',\n",
              "       'TBFI_soil_temperature_level_2', 'TBFI_soil_temperature_level_3',\n",
              "       'band_1', 'band_2', 'band_3', 'band_4', 'band_5', 'band_6'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "DcS4anDpPzDO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRycbptvP7DO",
        "outputId": "3e635a89-b034-421e-9d1a-c0df9fc5eb4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50262, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "1W2Nz0I5P8-p",
        "outputId": "86a98ab4-88da-43aa-cf40-ee86997fe250"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date                                       0\n",
              "snow_albedo                                0\n",
              "snow_cover                                 0\n",
              "snow_density                               0\n",
              "snow_depth                                 0\n",
              "snowfall_sum                               0\n",
              "surface_latent_heat_flux_sum               0\n",
              "surface_sensible_heat_flux_sum             0\n",
              "surface_solar_radiation_downwards_sum      0\n",
              "surface_thermal_radiation_downwards_sum    0\n",
              "skin_temperature                           0\n",
              "total_precipitation_sum                    0\n",
              "avg_volumetric_water_content               0\n",
              "TBFI_skin_temperature                      0\n",
              "TBFI_soil_temperature_level_1              0\n",
              "TBFI_soil_temperature_level_2              0\n",
              "TBFI_soil_temperature_level_3              0\n",
              "band_1                                     0\n",
              "band_2                                     0\n",
              "band_3                                     0\n",
              "band_4                                     0\n",
              "band_5                                     0\n",
              "band_6                                     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_albedo</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_cover</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_density</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snow_depth</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>snowfall_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_latent_heat_flux_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_sensible_heat_flux_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_solar_radiation_downwards_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_thermal_radiation_downwards_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skin_temperature</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total_precipitation_sum</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avg_volumetric_water_content</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_skin_temperature</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_soil_temperature_level_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_soil_temperature_level_2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TBFI_soil_temperature_level_3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>band_6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add an additional col 'Year'."
      ],
      "metadata": {
        "id": "C-QCc2kfQTpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['Year'] = df['date'].dt.year"
      ],
      "metadata": {
        "id": "nAmaoRn_QXIu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Year'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_1N8tdDQrIM",
        "outputId": "1542940b-b944-4168-f8fb-27a1891a47d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
              "       2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022,\n",
              "       2023], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the size of each year\n",
        "band_1 = df[df['band_1'] == 1]\n",
        "band_2 = df[df['band_2'] == 1]\n",
        "band_3 = df[df['band_3'] == 1]\n",
        "band_4 = df[df['band_4'] == 1]\n",
        "band_5 = df[df['band_5'] == 1]\n",
        "band_6 = df[df['band_6'] == 1]"
      ],
      "metadata": {
        "id": "ieIMu-ldw0RM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bands = [band_1, band_2, band_3, band_4, band_5, band_6]\n",
        "days_in_year = set()\n",
        "for band in bands:\n",
        "  for year in band['Year'].unique():\n",
        "    size = band[band['Year'] == year].shape[0]\n",
        "    days_in_year.add(size)\n",
        "\n",
        "print(days_in_year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmTokVnxxClv",
        "outputId": "5ae836c1-c837-4613-e152-84a68a063e29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{364, 365}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we know that there is not much variation for number of days in a year. This will make sequencing more straightforward."
      ],
      "metadata": {
        "id": "-FjTs3ZIyIW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train-Test Split**"
      ],
      "metadata": {
        "id": "8L3dDralQApK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to be band-conscious during the split so band data does not leak.\n",
        "\n",
        "Create sliding-window sequences."
      ],
      "metadata": {
        "id": "7J6bJAi6QEq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "band_cols = ['band_1', 'band_2', 'band_3', 'band_4', 'band_5', 'band_6']"
      ],
      "metadata": {
        "id": "98cl5jLEQL0v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_list, y_train_list = [], []\n",
        "X_test_list, y_test_list = [], []"
      ],
      "metadata": {
        "id": "KJIXFjCiRAaz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_features = ['date', 'Year', 'TBFI_skin_temperature',\n",
        "                'TBFI_soil_temperature_level_1', 'TBFI_soil_temperature_level_2',\n",
        "                'TBFI_soil_temperature_level_3']\n",
        "\n",
        "targets = ['TBFI_skin_temperature', 'TBFI_soil_temperature_level_1',\n",
        "           'TBFI_soil_temperature_level_2', 'TBFI_soil_temperature_level_3']\n",
        "\n",
        "features = df.drop(columns=not_features, axis=1).columns"
      ],
      "metadata": {
        "id": "oP9p3se1Ruyf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kla6pUmVGU0S",
        "outputId": "046f1dc2-d488-4e7e-f5df-de0c129e3c75"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['snow_albedo', 'snow_cover', 'snow_density', 'snow_depth',\n",
              "       'snowfall_sum', 'surface_latent_heat_flux_sum',\n",
              "       'surface_sensible_heat_flux_sum',\n",
              "       'surface_solar_radiation_downwards_sum',\n",
              "       'surface_thermal_radiation_downwards_sum', 'skin_temperature',\n",
              "       'total_precipitation_sum', 'avg_volumetric_water_content', 'band_1',\n",
              "       'band_2', 'band_3', 'band_4', 'band_5', 'band_6'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fazibBnQGYFX",
        "outputId": "3e97fd36-1b96-4a91-c75a-f4d133c31ada"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2"
      ],
      "metadata": {
        "id": "gKgEt_2G72h3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in band_cols:\n",
        "  data = df[df[col] == 1].reset_index(drop=True) # filter by band\n",
        "  X_windows = np.lib.stride_tricks.sliding_window_view(data[features], (window_size, len(features)))\n",
        "  X_windows = X_windows.reshape(-1, window_size, len(features))\n",
        "\n",
        "  y_windows = data[targets].values[window_size:]\n",
        "\n",
        "  split_index = np.where(data['Year'].values[window_size:] <= 2018)[0]\n",
        "  test_index = np.where(data['Year'].values[window_size:] > 2018)[0]\n",
        "\n",
        "  X_train_list.append(X_windows[split_index])\n",
        "  y_train_list.append(y_windows[split_index])\n",
        "\n",
        "  X_test_list.append(X_windows[test_index])\n",
        "  y_test_list.append(y_windows[test_index])"
      ],
      "metadata": {
        "id": "LxorpZDORDQC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(np.concatenate(X_train_list), dtype=torch.float32)\n",
        "y_train = torch.tensor(np.concatenate(y_train_list), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate(X_test_list), dtype=torch.float32)\n",
        "y_test = torch.tensor(np.concatenate(y_test_list), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "uqrN1-d_SZLy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data Shapes:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing Data Shapes:\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngdKHahT8tmB",
        "outputId": "81c009c7-a0cb-4a14-d1f3-4e2c30c85bde"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Shapes: torch.Size([39324, 2, 18]) torch.Size([39324, 4])\n",
            "Testing Data Shapes: torch.Size([10926, 2, 18]) torch.Size([10926, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize Dataset and Dataloaders**"
      ],
      "metadata": {
        "id": "48N08OiTSliF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import PermafrostDataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "X6RX-Xo7SnNx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PermafrostDataset(X_train, y_train)\n",
        "test_dataset = PermafrostDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "SLVxLWBB9K-F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "ERTK-wrl9Z4y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader) # num batches per epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0sblxNYZVJy",
        "outputId": "313d95df-8d03-40c3-832f-fbf26bd2eaf4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9831"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop**"
      ],
      "metadata": {
        "id": "Wfr3WHR1-KQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from convlstm import ConvLSTM\n",
        "from torch.nn import MSELoss\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "bzO492A6-Ued"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvLSTM(input_features=len(features))\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# old optimizer =  Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "loss_fn = MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)"
      ],
      "metadata": {
        "id": "XOvKyWrdZ_JG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients(model, max_norm=1.0):\n",
        "    for param in model.parameters():\n",
        "        torch.nn.utils.clip_grad_norm_(param, max_norm)"
      ],
      "metadata": {
        "id": "01ARlw3BaQUP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SioeDpSDe3Lh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_losses = []\n",
        "\n",
        "def train_model(model, train_loader, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}\")\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = loss_fn(preds, y_batch)\n",
        "            loss.backward()\n",
        "\n",
        "            if batch_idx % 1000 == 0:\n",
        "              print(f\"\\n[Batch {batch_idx}] Gradient Norms:\")\n",
        "              for name, param in model.named_parameters():\n",
        "                  if param.grad is not None:\n",
        "                      print(f\"{name} grad norm: {param.grad.norm().item()}\")\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 1000 == 0:\n",
        "                print(f\"[Batch {batch_idx}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Total Epoch Loss: {epoch_loss:.4f}\")\n",
        "        epoch_losses.append(epoch_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(epoch_losses, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kDD76jRkdGeK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D9fwM_xYaUGQ",
        "outputId": "8aa7c200-585a-4446-c01f-0b3f394700e9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "h0 grad norm: 0.07077840715646744\n",
            "c0 grad norm: 0.06839170306921005\n",
            "conv1.weight grad norm: 0.7118015289306641\n",
            "conv1.bias grad norm: 0.1518615335226059\n",
            "norm.weight grad norm: 0.10799846798181534\n",
            "norm.bias grad norm: 0.13794247806072235\n",
            "lstm.weight_ih_l0 grad norm: 0.4597948491573334\n",
            "lstm.weight_hh_l0 grad norm: 0.1400284618139267\n",
            "lstm.bias_ih_l0 grad norm: 0.04487304762005806\n",
            "lstm.bias_hh_l0 grad norm: 0.04487304762005806\n",
            "fc.weight grad norm: 0.3923908472061157\n",
            "fc.bias grad norm: 0.1342066079378128\n",
            "[Batch 3000] Loss: 0.0804\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.04278506711125374\n",
            "c0 grad norm: 0.09192518889904022\n",
            "conv1.weight grad norm: 0.723369300365448\n",
            "conv1.bias grad norm: 0.16631171107292175\n",
            "norm.weight grad norm: 0.08582712709903717\n",
            "norm.bias grad norm: 0.1150272786617279\n",
            "lstm.weight_ih_l0 grad norm: 0.5717445015907288\n",
            "lstm.weight_hh_l0 grad norm: 0.18561717867851257\n",
            "lstm.bias_ih_l0 grad norm: 0.05623798444867134\n",
            "lstm.bias_hh_l0 grad norm: 0.05623798444867134\n",
            "fc.weight grad norm: 1.336511492729187\n",
            "fc.bias grad norm: 0.44329068064689636\n",
            "[Batch 4000] Loss: 0.4766\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.023689711466431618\n",
            "c0 grad norm: 0.04460390657186508\n",
            "conv1.weight grad norm: 0.34728530049324036\n",
            "conv1.bias grad norm: 0.05869186669588089\n",
            "norm.weight grad norm: 0.04230644553899765\n",
            "norm.bias grad norm: 0.048545315861701965\n",
            "lstm.weight_ih_l0 grad norm: 0.2071477472782135\n",
            "lstm.weight_hh_l0 grad norm: 0.05557096749544144\n",
            "lstm.bias_ih_l0 grad norm: 0.018886849284172058\n",
            "lstm.bias_hh_l0 grad norm: 0.018886849284172058\n",
            "fc.weight grad norm: 0.29922157526016235\n",
            "fc.bias grad norm: 0.09621741622686386\n",
            "[Batch 5000] Loss: 0.0240\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.043450791388750076\n",
            "c0 grad norm: 0.051395952701568604\n",
            "conv1.weight grad norm: 0.3944430947303772\n",
            "conv1.bias grad norm: 0.13557159900665283\n",
            "norm.weight grad norm: 0.057094961404800415\n",
            "norm.bias grad norm: 0.07143726199865341\n",
            "lstm.weight_ih_l0 grad norm: 0.2752494812011719\n",
            "lstm.weight_hh_l0 grad norm: 0.053947560489177704\n",
            "lstm.bias_ih_l0 grad norm: 0.026975663378834724\n",
            "lstm.bias_hh_l0 grad norm: 0.026975663378834724\n",
            "fc.weight grad norm: 0.17887119948863983\n",
            "fc.bias grad norm: 0.038790520280599594\n",
            "[Batch 6000] Loss: 0.0591\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.009554574266076088\n",
            "c0 grad norm: 0.016589222475886345\n",
            "conv1.weight grad norm: 0.16087646782398224\n",
            "conv1.bias grad norm: 0.024620098993182182\n",
            "norm.weight grad norm: 0.02225811965763569\n",
            "norm.bias grad norm: 0.023463495075702667\n",
            "lstm.weight_ih_l0 grad norm: 0.1391691267490387\n",
            "lstm.weight_hh_l0 grad norm: 0.04534932225942612\n",
            "lstm.bias_ih_l0 grad norm: 0.013525870628654957\n",
            "lstm.bias_hh_l0 grad norm: 0.013525870628654957\n",
            "fc.weight grad norm: 0.2798594832420349\n",
            "fc.bias grad norm: 0.0911884605884552\n",
            "[Batch 7000] Loss: 0.0237\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.015731416642665863\n",
            "c0 grad norm: 0.05577574670314789\n",
            "conv1.weight grad norm: 0.1677345335483551\n",
            "conv1.bias grad norm: 0.041064485907554626\n",
            "norm.weight grad norm: 0.03050660900771618\n",
            "norm.bias grad norm: 0.030722659081220627\n",
            "lstm.weight_ih_l0 grad norm: 0.1540004462003708\n",
            "lstm.weight_hh_l0 grad norm: 0.040688976645469666\n",
            "lstm.bias_ih_l0 grad norm: 0.015066265128552914\n",
            "lstm.bias_hh_l0 grad norm: 0.015066265128552914\n",
            "fc.weight grad norm: 0.3414648473262787\n",
            "fc.bias grad norm: 0.10268740355968475\n",
            "[Batch 8000] Loss: 0.0388\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.0636703297495842\n",
            "c0 grad norm: 0.09627589583396912\n",
            "conv1.weight grad norm: 0.676654040813446\n",
            "conv1.bias grad norm: 0.10165892541408539\n",
            "norm.weight grad norm: 0.08847784250974655\n",
            "norm.bias grad norm: 0.11072855442762375\n",
            "lstm.weight_ih_l0 grad norm: 0.49222949147224426\n",
            "lstm.weight_hh_l0 grad norm: 0.15724261105060577\n",
            "lstm.bias_ih_l0 grad norm: 0.04831048846244812\n",
            "lstm.bias_hh_l0 grad norm: 0.04831048846244812\n",
            "fc.weight grad norm: 0.6600729823112488\n",
            "fc.bias grad norm: 0.1775742620229721\n",
            "[Batch 9000] Loss: 0.0933\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 69\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.024052875116467476\n",
            "c0 grad norm: 0.02362418733537197\n",
            "conv1.weight grad norm: 0.30409055948257446\n",
            "conv1.bias grad norm: 0.060193561017513275\n",
            "norm.weight grad norm: 0.03316018357872963\n",
            "norm.bias grad norm: 0.04485362023115158\n",
            "lstm.weight_ih_l0 grad norm: 0.2009536176919937\n",
            "lstm.weight_hh_l0 grad norm: 0.0537598542869091\n",
            "lstm.bias_ih_l0 grad norm: 0.018930776044726372\n",
            "lstm.bias_hh_l0 grad norm: 0.018930776044726372\n",
            "fc.weight grad norm: 0.4417744278907776\n",
            "fc.bias grad norm: 0.10162906348705292\n",
            "[Batch 0] Loss: 0.0857\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.013561678119003773\n",
            "c0 grad norm: 0.02865780144929886\n",
            "conv1.weight grad norm: 0.2133750319480896\n",
            "conv1.bias grad norm: 0.04752490296959877\n",
            "norm.weight grad norm: 0.024732841178774834\n",
            "norm.bias grad norm: 0.03320278972387314\n",
            "lstm.weight_ih_l0 grad norm: 0.1252477765083313\n",
            "lstm.weight_hh_l0 grad norm: 0.033648163080215454\n",
            "lstm.bias_ih_l0 grad norm: 0.011721475049853325\n",
            "lstm.bias_hh_l0 grad norm: 0.011721475049853325\n",
            "fc.weight grad norm: 0.18651720881462097\n",
            "fc.bias grad norm: 0.037844955921173096\n",
            "[Batch 1000] Loss: 0.0071\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.015437230467796326\n",
            "c0 grad norm: 0.024050582200288773\n",
            "conv1.weight grad norm: 0.09712616354227066\n",
            "conv1.bias grad norm: 0.01640719175338745\n",
            "norm.weight grad norm: 0.01873919740319252\n",
            "norm.bias grad norm: 0.01934242434799671\n",
            "lstm.weight_ih_l0 grad norm: 0.11612002551555634\n",
            "lstm.weight_hh_l0 grad norm: 0.02405981346964836\n",
            "lstm.bias_ih_l0 grad norm: 0.011214524507522583\n",
            "lstm.bias_hh_l0 grad norm: 0.011214524507522583\n",
            "fc.weight grad norm: 0.16333459317684174\n",
            "fc.bias grad norm: 0.07040581107139587\n",
            "[Batch 2000] Loss: 0.0106\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.0938250869512558\n",
            "c0 grad norm: 0.13502563536167145\n",
            "conv1.weight grad norm: 1.1732550859451294\n",
            "conv1.bias grad norm: 0.18851745128631592\n",
            "norm.weight grad norm: 0.1537671536207199\n",
            "norm.bias grad norm: 0.2137775868177414\n",
            "lstm.weight_ih_l0 grad norm: 0.7275256514549255\n",
            "lstm.weight_hh_l0 grad norm: 0.2134074717760086\n",
            "lstm.bias_ih_l0 grad norm: 0.07107946276664734\n",
            "lstm.bias_hh_l0 grad norm: 0.07107946276664734\n",
            "fc.weight grad norm: 0.8296144008636475\n",
            "fc.bias grad norm: 0.287638783454895\n",
            "[Batch 3000] Loss: 0.2638\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.008991306647658348\n",
            "c0 grad norm: 0.024879127740859985\n",
            "conv1.weight grad norm: 0.10054419189691544\n",
            "conv1.bias grad norm: 0.02548372931778431\n",
            "norm.weight grad norm: 0.013886930420994759\n",
            "norm.bias grad norm: 0.01652105152606964\n",
            "lstm.weight_ih_l0 grad norm: 0.07773531228303909\n",
            "lstm.weight_hh_l0 grad norm: 0.022322753444314003\n",
            "lstm.bias_ih_l0 grad norm: 0.0074987332336604595\n",
            "lstm.bias_hh_l0 grad norm: 0.0074987332336604595\n",
            "fc.weight grad norm: 0.20636168122291565\n",
            "fc.bias grad norm: 0.05352482572197914\n",
            "[Batch 4000] Loss: 0.0143\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.017190061509609222\n",
            "c0 grad norm: 0.059285614639520645\n",
            "conv1.weight grad norm: 0.23062659800052643\n",
            "conv1.bias grad norm: 0.04598367586731911\n",
            "norm.weight grad norm: 0.03789035975933075\n",
            "norm.bias grad norm: 0.031882770359516144\n",
            "lstm.weight_ih_l0 grad norm: 0.2361040860414505\n",
            "lstm.weight_hh_l0 grad norm: 0.06940948218107224\n",
            "lstm.bias_ih_l0 grad norm: 0.02168840728700161\n",
            "lstm.bias_hh_l0 grad norm: 0.02168840728700161\n",
            "fc.weight grad norm: 0.4722978174686432\n",
            "fc.bias grad norm: 0.09724348783493042\n",
            "[Batch 5000] Loss: 0.0437\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.016972623765468597\n",
            "c0 grad norm: 0.05360502377152443\n",
            "conv1.weight grad norm: 0.34904688596725464\n",
            "conv1.bias grad norm: 0.06070321425795555\n",
            "norm.weight grad norm: 0.057759106159210205\n",
            "norm.bias grad norm: 0.06235270947217941\n",
            "lstm.weight_ih_l0 grad norm: 0.2357374131679535\n",
            "lstm.weight_hh_l0 grad norm: 0.07119952142238617\n",
            "lstm.bias_ih_l0 grad norm: 0.02195775881409645\n",
            "lstm.bias_hh_l0 grad norm: 0.02195775881409645\n",
            "fc.weight grad norm: 0.24576786160469055\n",
            "fc.bias grad norm: 0.07815315574407578\n",
            "[Batch 6000] Loss: 0.0284\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.041857633739709854\n",
            "c0 grad norm: 0.07631563395261765\n",
            "conv1.weight grad norm: 0.6298179626464844\n",
            "conv1.bias grad norm: 0.09370110929012299\n",
            "norm.weight grad norm: 0.08108314871788025\n",
            "norm.bias grad norm: 0.09855739027261734\n",
            "lstm.weight_ih_l0 grad norm: 0.6703586578369141\n",
            "lstm.weight_hh_l0 grad norm: 0.20699357986450195\n",
            "lstm.bias_ih_l0 grad norm: 0.06328031420707703\n",
            "lstm.bias_hh_l0 grad norm: 0.06328031420707703\n",
            "fc.weight grad norm: 0.730514645576477\n",
            "fc.bias grad norm: 0.29215967655181885\n",
            "[Batch 7000] Loss: 0.3749\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.04713186249136925\n",
            "c0 grad norm: 0.055793602019548416\n",
            "conv1.weight grad norm: 0.45753586292266846\n",
            "conv1.bias grad norm: 0.11352241784334183\n",
            "norm.weight grad norm: 0.06132325157523155\n",
            "norm.bias grad norm: 0.083963543176651\n",
            "lstm.weight_ih_l0 grad norm: 0.3238358497619629\n",
            "lstm.weight_hh_l0 grad norm: 0.09506237506866455\n",
            "lstm.bias_ih_l0 grad norm: 0.03460792452096939\n",
            "lstm.bias_hh_l0 grad norm: 0.03460792452096939\n",
            "fc.weight grad norm: 0.40237507224082947\n",
            "fc.bias grad norm: 0.19588424265384674\n",
            "[Batch 8000] Loss: 0.0526\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.016436858102679253\n",
            "c0 grad norm: 0.025414995849132538\n",
            "conv1.weight grad norm: 0.278305321931839\n",
            "conv1.bias grad norm: 0.06211109086871147\n",
            "norm.weight grad norm: 0.04326019436120987\n",
            "norm.bias grad norm: 0.05030953139066696\n",
            "lstm.weight_ih_l0 grad norm: 0.2855610251426697\n",
            "lstm.weight_hh_l0 grad norm: 0.11860453337430954\n",
            "lstm.bias_ih_l0 grad norm: 0.028947781771421432\n",
            "lstm.bias_hh_l0 grad norm: 0.028947781771421432\n",
            "fc.weight grad norm: 0.27981823682785034\n",
            "fc.bias grad norm: 0.09095783531665802\n",
            "[Batch 9000] Loss: 0.0275\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 70\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.05320076271891594\n",
            "c0 grad norm: 0.053599391132593155\n",
            "conv1.weight grad norm: 0.7622565031051636\n",
            "conv1.bias grad norm: 0.13671745359897614\n",
            "norm.weight grad norm: 0.08575623482465744\n",
            "norm.bias grad norm: 0.11022971570491791\n",
            "lstm.weight_ih_l0 grad norm: 0.4087093770503998\n",
            "lstm.weight_hh_l0 grad norm: 0.10135134309530258\n",
            "lstm.bias_ih_l0 grad norm: 0.040872205048799515\n",
            "lstm.bias_hh_l0 grad norm: 0.040872205048799515\n",
            "fc.weight grad norm: 0.6737514734268188\n",
            "fc.bias grad norm: 0.2827853858470917\n",
            "[Batch 0] Loss: 0.2133\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.031982552260160446\n",
            "c0 grad norm: 0.037848275154829025\n",
            "conv1.weight grad norm: 0.31005215644836426\n",
            "conv1.bias grad norm: 0.04977051168680191\n",
            "norm.weight grad norm: 0.041113682091236115\n",
            "norm.bias grad norm: 0.03352571278810501\n",
            "lstm.weight_ih_l0 grad norm: 0.22080263495445251\n",
            "lstm.weight_hh_l0 grad norm: 0.06734465807676315\n",
            "lstm.bias_ih_l0 grad norm: 0.021404633298516273\n",
            "lstm.bias_hh_l0 grad norm: 0.021404633298516273\n",
            "fc.weight grad norm: 0.27965879440307617\n",
            "fc.bias grad norm: 0.12038972973823547\n",
            "[Batch 1000] Loss: 0.0313\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.012332729995250702\n",
            "c0 grad norm: 0.02978453040122986\n",
            "conv1.weight grad norm: 0.13694564998149872\n",
            "conv1.bias grad norm: 0.03035147115588188\n",
            "norm.weight grad norm: 0.021527182310819626\n",
            "norm.bias grad norm: 0.022604385390877724\n",
            "lstm.weight_ih_l0 grad norm: 0.0968814268708229\n",
            "lstm.weight_hh_l0 grad norm: 0.025536686182022095\n",
            "lstm.bias_ih_l0 grad norm: 0.009514260105788708\n",
            "lstm.bias_hh_l0 grad norm: 0.009514260105788708\n",
            "fc.weight grad norm: 0.22553004324436188\n",
            "fc.bias grad norm: 0.051575835794210434\n",
            "[Batch 2000] Loss: 0.0241\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.014027291908860207\n",
            "c0 grad norm: 0.030008841305971146\n",
            "conv1.weight grad norm: 0.30368977785110474\n",
            "conv1.bias grad norm: 0.030712727457284927\n",
            "norm.weight grad norm: 0.02357577532529831\n",
            "norm.bias grad norm: 0.02419471926987171\n",
            "lstm.weight_ih_l0 grad norm: 0.16224516928195953\n",
            "lstm.weight_hh_l0 grad norm: 0.053236283361911774\n",
            "lstm.bias_ih_l0 grad norm: 0.013792716898024082\n",
            "lstm.bias_hh_l0 grad norm: 0.013792716898024082\n",
            "fc.weight grad norm: 0.36104604601860046\n",
            "fc.bias grad norm: 0.09506552666425705\n",
            "[Batch 3000] Loss: 0.0340\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.048388514667749405\n",
            "c0 grad norm: 0.0429718941450119\n",
            "conv1.weight grad norm: 0.41112181544303894\n",
            "conv1.bias grad norm: 0.1011379063129425\n",
            "norm.weight grad norm: 0.05420832335948944\n",
            "norm.bias grad norm: 0.06826266646385193\n",
            "lstm.weight_ih_l0 grad norm: 0.3088829219341278\n",
            "lstm.weight_hh_l0 grad norm: 0.07823923230171204\n",
            "lstm.bias_ih_l0 grad norm: 0.030362950637936592\n",
            "lstm.bias_hh_l0 grad norm: 0.030362950637936592\n",
            "fc.weight grad norm: 0.40946120023727417\n",
            "fc.bias grad norm: 0.1531694382429123\n",
            "[Batch 4000] Loss: 0.0601\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.08104734122753143\n",
            "c0 grad norm: 0.07473808526992798\n",
            "conv1.weight grad norm: 1.4203957319259644\n",
            "conv1.bias grad norm: 0.20980557799339294\n",
            "norm.weight grad norm: 0.17264312505722046\n",
            "norm.bias grad norm: 0.211174875497818\n",
            "lstm.weight_ih_l0 grad norm: 0.6764311790466309\n",
            "lstm.weight_hh_l0 grad norm: 0.13759294152259827\n",
            "lstm.bias_ih_l0 grad norm: 0.0655631348490715\n",
            "lstm.bias_hh_l0 grad norm: 0.0655631348490715\n",
            "fc.weight grad norm: 0.43983137607574463\n",
            "fc.bias grad norm: 0.20603717863559723\n",
            "[Batch 5000] Loss: 0.1337\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.045552030205726624\n",
            "c0 grad norm: 0.04517704248428345\n",
            "conv1.weight grad norm: 0.5178292393684387\n",
            "conv1.bias grad norm: 0.09773676097393036\n",
            "norm.weight grad norm: 0.07146232575178146\n",
            "norm.bias grad norm: 0.06993291527032852\n",
            "lstm.weight_ih_l0 grad norm: 0.3673883378505707\n",
            "lstm.weight_hh_l0 grad norm: 0.11106200516223907\n",
            "lstm.bias_ih_l0 grad norm: 0.03307873383164406\n",
            "lstm.bias_hh_l0 grad norm: 0.03307873383164406\n",
            "fc.weight grad norm: 0.33212515711784363\n",
            "fc.bias grad norm: 0.07823439687490463\n",
            "[Batch 6000] Loss: 0.0368\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.06947788596153259\n",
            "c0 grad norm: 0.0518619604408741\n",
            "conv1.weight grad norm: 0.641635537147522\n",
            "conv1.bias grad norm: 0.16539014875888824\n",
            "norm.weight grad norm: 0.09096692502498627\n",
            "norm.bias grad norm: 0.12485721707344055\n",
            "lstm.weight_ih_l0 grad norm: 0.3918503224849701\n",
            "lstm.weight_hh_l0 grad norm: 0.08663349598646164\n",
            "lstm.bias_ih_l0 grad norm: 0.03898569196462631\n",
            "lstm.bias_hh_l0 grad norm: 0.03898569196462631\n",
            "fc.weight grad norm: 0.37900710105895996\n",
            "fc.bias grad norm: 0.1669258177280426\n",
            "[Batch 7000] Loss: 0.0453\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.012743989937007427\n",
            "c0 grad norm: 0.01907944679260254\n",
            "conv1.weight grad norm: 0.1476028859615326\n",
            "conv1.bias grad norm: 0.019466033205389977\n",
            "norm.weight grad norm: 0.02279386855661869\n",
            "norm.bias grad norm: 0.01830696500837803\n",
            "lstm.weight_ih_l0 grad norm: 0.11266440898180008\n",
            "lstm.weight_hh_l0 grad norm: 0.030680092051625252\n",
            "lstm.bias_ih_l0 grad norm: 0.01074888277798891\n",
            "lstm.bias_hh_l0 grad norm: 0.01074888277798891\n",
            "fc.weight grad norm: 0.1668393909931183\n",
            "fc.bias grad norm: 0.05443497374653816\n",
            "[Batch 8000] Loss: 0.0233\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.053007468581199646\n",
            "c0 grad norm: 0.13794606924057007\n",
            "conv1.weight grad norm: 0.45948368310928345\n",
            "conv1.bias grad norm: 0.10604008287191391\n",
            "norm.weight grad norm: 0.06752575933933258\n",
            "norm.bias grad norm: 0.0842144638299942\n",
            "lstm.weight_ih_l0 grad norm: 0.44246435165405273\n",
            "lstm.weight_hh_l0 grad norm: 0.13341738283634186\n",
            "lstm.bias_ih_l0 grad norm: 0.04480926692485809\n",
            "lstm.bias_hh_l0 grad norm: 0.04480926692485809\n",
            "fc.weight grad norm: 0.5380234718322754\n",
            "fc.bias grad norm: 0.2517777979373932\n",
            "[Batch 9000] Loss: 0.1090\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 71\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03002016991376877\n",
            "c0 grad norm: 0.033548541367053986\n",
            "conv1.weight grad norm: 0.4388089179992676\n",
            "conv1.bias grad norm: 0.1088922768831253\n",
            "norm.weight grad norm: 0.052142463624477386\n",
            "norm.bias grad norm: 0.07486569136381149\n",
            "lstm.weight_ih_l0 grad norm: 0.309286892414093\n",
            "lstm.weight_hh_l0 grad norm: 0.0755215585231781\n",
            "lstm.bias_ih_l0 grad norm: 0.03147575259208679\n",
            "lstm.bias_hh_l0 grad norm: 0.03147575259208679\n",
            "fc.weight grad norm: 0.2662929594516754\n",
            "fc.bias grad norm: 0.10297650843858719\n",
            "[Batch 0] Loss: 0.0745\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.052677687257528305\n",
            "c0 grad norm: 0.09664737433195114\n",
            "conv1.weight grad norm: 0.7491897940635681\n",
            "conv1.bias grad norm: 0.16597706079483032\n",
            "norm.weight grad norm: 0.08775319159030914\n",
            "norm.bias grad norm: 0.14095669984817505\n",
            "lstm.weight_ih_l0 grad norm: 0.3821406066417694\n",
            "lstm.weight_hh_l0 grad norm: 0.08330001682043076\n",
            "lstm.bias_ih_l0 grad norm: 0.03668325021862984\n",
            "lstm.bias_hh_l0 grad norm: 0.03668325021862984\n",
            "fc.weight grad norm: 0.45859721302986145\n",
            "fc.bias grad norm: 0.23936396837234497\n",
            "[Batch 1000] Loss: 0.1063\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.06906700134277344\n",
            "c0 grad norm: 0.03395066782832146\n",
            "conv1.weight grad norm: 0.9535164833068848\n",
            "conv1.bias grad norm: 0.1958613395690918\n",
            "norm.weight grad norm: 0.06933573633432388\n",
            "norm.bias grad norm: 0.11577080190181732\n",
            "lstm.weight_ih_l0 grad norm: 0.4757164418697357\n",
            "lstm.weight_hh_l0 grad norm: 0.08769232034683228\n",
            "lstm.bias_ih_l0 grad norm: 0.04852462559938431\n",
            "lstm.bias_hh_l0 grad norm: 0.04852462559938431\n",
            "fc.weight grad norm: 0.2537362277507782\n",
            "fc.bias grad norm: 0.07910299301147461\n",
            "[Batch 2000] Loss: 0.0303\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.01314200833439827\n",
            "c0 grad norm: 0.0329417921602726\n",
            "conv1.weight grad norm: 0.15511704981327057\n",
            "conv1.bias grad norm: 0.03444428741931915\n",
            "norm.weight grad norm: 0.030085543170571327\n",
            "norm.bias grad norm: 0.03159241005778313\n",
            "lstm.weight_ih_l0 grad norm: 0.1912476271390915\n",
            "lstm.weight_hh_l0 grad norm: 0.04939570650458336\n",
            "lstm.bias_ih_l0 grad norm: 0.01826801523566246\n",
            "lstm.bias_hh_l0 grad norm: 0.01826801523566246\n",
            "fc.weight grad norm: 0.24403169751167297\n",
            "fc.bias grad norm: 0.0742497593164444\n",
            "[Batch 3000] Loss: 0.0366\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.06596793234348297\n",
            "c0 grad norm: 0.06460070610046387\n",
            "conv1.weight grad norm: 0.7476632595062256\n",
            "conv1.bias grad norm: 0.1852898895740509\n",
            "norm.weight grad norm: 0.10009928792715073\n",
            "norm.bias grad norm: 0.12659305334091187\n",
            "lstm.weight_ih_l0 grad norm: 0.49133145809173584\n",
            "lstm.weight_hh_l0 grad norm: 0.11438364535570145\n",
            "lstm.bias_ih_l0 grad norm: 0.045980118215084076\n",
            "lstm.bias_hh_l0 grad norm: 0.045980118215084076\n",
            "fc.weight grad norm: 0.4715060889720917\n",
            "fc.bias grad norm: 0.16795051097869873\n",
            "[Batch 4000] Loss: 0.0909\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.048259928822517395\n",
            "c0 grad norm: 0.0934411957859993\n",
            "conv1.weight grad norm: 0.5741596221923828\n",
            "conv1.bias grad norm: 0.11634533852338791\n",
            "norm.weight grad norm: 0.06841518729925156\n",
            "norm.bias grad norm: 0.06525518000125885\n",
            "lstm.weight_ih_l0 grad norm: 0.41499608755111694\n",
            "lstm.weight_hh_l0 grad norm: 0.10161543637514114\n",
            "lstm.bias_ih_l0 grad norm: 0.039996515959501266\n",
            "lstm.bias_hh_l0 grad norm: 0.039996515959501266\n",
            "fc.weight grad norm: 0.4059165120124817\n",
            "fc.bias grad norm: 0.05650276318192482\n",
            "[Batch 5000] Loss: 0.1161\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.0078031462617218494\n",
            "c0 grad norm: 0.023196103051304817\n",
            "conv1.weight grad norm: 0.1532248556613922\n",
            "conv1.bias grad norm: 0.04040050506591797\n",
            "norm.weight grad norm: 0.02513423003256321\n",
            "norm.bias grad norm: 0.028964275494217873\n",
            "lstm.weight_ih_l0 grad norm: 0.1543208807706833\n",
            "lstm.weight_hh_l0 grad norm: 0.03914032131433487\n",
            "lstm.bias_ih_l0 grad norm: 0.01480478048324585\n",
            "lstm.bias_hh_l0 grad norm: 0.01480478048324585\n",
            "fc.weight grad norm: 0.3000342547893524\n",
            "fc.bias grad norm: 0.11892902851104736\n",
            "[Batch 6000] Loss: 0.0383\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.012774293310940266\n",
            "c0 grad norm: 0.015543289482593536\n",
            "conv1.weight grad norm: 0.11721829324960709\n",
            "conv1.bias grad norm: 0.03196496143937111\n",
            "norm.weight grad norm: 0.0159537922590971\n",
            "norm.bias grad norm: 0.02078932337462902\n",
            "lstm.weight_ih_l0 grad norm: 0.11042434722185135\n",
            "lstm.weight_hh_l0 grad norm: 0.032634224742650986\n",
            "lstm.bias_ih_l0 grad norm: 0.01142586674541235\n",
            "lstm.bias_hh_l0 grad norm: 0.01142586674541235\n",
            "fc.weight grad norm: 0.1669997125864029\n",
            "fc.bias grad norm: 0.06485820561647415\n",
            "[Batch 7000] Loss: 0.0115\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.057283174246549606\n",
            "c0 grad norm: 0.06852170825004578\n",
            "conv1.weight grad norm: 0.6590682864189148\n",
            "conv1.bias grad norm: 0.12974445521831512\n",
            "norm.weight grad norm: 0.07194896787405014\n",
            "norm.bias grad norm: 0.09016668051481247\n",
            "lstm.weight_ih_l0 grad norm: 0.3813893496990204\n",
            "lstm.weight_hh_l0 grad norm: 0.09242376685142517\n",
            "lstm.bias_ih_l0 grad norm: 0.035105712711811066\n",
            "lstm.bias_hh_l0 grad norm: 0.035105712711811066\n",
            "fc.weight grad norm: 0.4077681601047516\n",
            "fc.bias grad norm: 0.1859654039144516\n",
            "[Batch 8000] Loss: 0.1152\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.024047289043664932\n",
            "c0 grad norm: 0.08841554075479507\n",
            "conv1.weight grad norm: 0.28234758973121643\n",
            "conv1.bias grad norm: 0.058943819254636765\n",
            "norm.weight grad norm: 0.05052265524864197\n",
            "norm.bias grad norm: 0.051711276173591614\n",
            "lstm.weight_ih_l0 grad norm: 0.2564719021320343\n",
            "lstm.weight_hh_l0 grad norm: 0.06747570633888245\n",
            "lstm.bias_ih_l0 grad norm: 0.023803556337952614\n",
            "lstm.bias_hh_l0 grad norm: 0.023803556337952614\n",
            "fc.weight grad norm: 0.49553409218788147\n",
            "fc.bias grad norm: 0.07641243189573288\n",
            "[Batch 9000] Loss: 0.0972\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 72\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.013392985798418522\n",
            "c0 grad norm: 0.023867731913924217\n",
            "conv1.weight grad norm: 0.22884345054626465\n",
            "conv1.bias grad norm: 0.02372078038752079\n",
            "norm.weight grad norm: 0.02416943572461605\n",
            "norm.bias grad norm: 0.025311091914772987\n",
            "lstm.weight_ih_l0 grad norm: 0.14851950109004974\n",
            "lstm.weight_hh_l0 grad norm: 0.04563676938414574\n",
            "lstm.bias_ih_l0 grad norm: 0.014769138768315315\n",
            "lstm.bias_hh_l0 grad norm: 0.014769138768315315\n",
            "fc.weight grad norm: 0.17140787839889526\n",
            "fc.bias grad norm: 0.06074092537164688\n",
            "[Batch 0] Loss: 0.0125\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.022938214242458344\n",
            "c0 grad norm: 0.012719778344035149\n",
            "conv1.weight grad norm: 0.2420947402715683\n",
            "conv1.bias grad norm: 0.06982865929603577\n",
            "norm.weight grad norm: 0.019826985895633698\n",
            "norm.bias grad norm: 0.02612273208796978\n",
            "lstm.weight_ih_l0 grad norm: 0.13214148581027985\n",
            "lstm.weight_hh_l0 grad norm: 0.027069197967648506\n",
            "lstm.bias_ih_l0 grad norm: 0.012437350116670132\n",
            "lstm.bias_hh_l0 grad norm: 0.012437350116670132\n",
            "fc.weight grad norm: 0.12478195875883102\n",
            "fc.bias grad norm: 0.029201483353972435\n",
            "[Batch 1000] Loss: 0.0089\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.031003335490822792\n",
            "c0 grad norm: 0.06877288222312927\n",
            "conv1.weight grad norm: 0.4366184175014496\n",
            "conv1.bias grad norm: 0.07199489325284958\n",
            "norm.weight grad norm: 0.05033351480960846\n",
            "norm.bias grad norm: 0.0557844452559948\n",
            "lstm.weight_ih_l0 grad norm: 0.27532467246055603\n",
            "lstm.weight_hh_l0 grad norm: 0.07492595911026001\n",
            "lstm.bias_ih_l0 grad norm: 0.026539523154497147\n",
            "lstm.bias_hh_l0 grad norm: 0.026539523154497147\n",
            "fc.weight grad norm: 0.48003241419792175\n",
            "fc.bias grad norm: 0.20840249955654144\n",
            "[Batch 2000] Loss: 0.1014\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.04741186276078224\n",
            "c0 grad norm: 0.09880920499563217\n",
            "conv1.weight grad norm: 1.37327241897583\n",
            "conv1.bias grad norm: 0.2454248070716858\n",
            "norm.weight grad norm: 0.12801706790924072\n",
            "norm.bias grad norm: 0.16776840388774872\n",
            "lstm.weight_ih_l0 grad norm: 0.6568891406059265\n",
            "lstm.weight_hh_l0 grad norm: 0.19347158074378967\n",
            "lstm.bias_ih_l0 grad norm: 0.0636768788099289\n",
            "lstm.bias_hh_l0 grad norm: 0.0636768788099289\n",
            "fc.weight grad norm: 0.687127947807312\n",
            "fc.bias grad norm: 0.18903863430023193\n",
            "[Batch 3000] Loss: 0.2500\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.011479869484901428\n",
            "c0 grad norm: 0.029734967276453972\n",
            "conv1.weight grad norm: 0.18606983125209808\n",
            "conv1.bias grad norm: 0.057214416563510895\n",
            "norm.weight grad norm: 0.023873694241046906\n",
            "norm.bias grad norm: 0.03875376284122467\n",
            "lstm.weight_ih_l0 grad norm: 0.15969252586364746\n",
            "lstm.weight_hh_l0 grad norm: 0.05263184383511543\n",
            "lstm.bias_ih_l0 grad norm: 0.015396792441606522\n",
            "lstm.bias_hh_l0 grad norm: 0.015396792441606522\n",
            "fc.weight grad norm: 0.38797643780708313\n",
            "fc.bias grad norm: 0.11141848564147949\n",
            "[Batch 4000] Loss: 0.0374\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.04967793449759483\n",
            "c0 grad norm: 0.029656805098056793\n",
            "conv1.weight grad norm: 0.5117825865745544\n",
            "conv1.bias grad norm: 0.11029597371816635\n",
            "norm.weight grad norm: 0.06196165829896927\n",
            "norm.bias grad norm: 0.06915069371461868\n",
            "lstm.weight_ih_l0 grad norm: 0.3285340964794159\n",
            "lstm.weight_hh_l0 grad norm: 0.07370567321777344\n",
            "lstm.bias_ih_l0 grad norm: 0.03238293528556824\n",
            "lstm.bias_hh_l0 grad norm: 0.03238293528556824\n",
            "fc.weight grad norm: 0.3153747022151947\n",
            "fc.bias grad norm: 0.11034566909074783\n",
            "[Batch 5000] Loss: 0.0382\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.007571743335574865\n",
            "c0 grad norm: 0.010364129208028316\n",
            "conv1.weight grad norm: 0.09425976127386093\n",
            "conv1.bias grad norm: 0.015981271862983704\n",
            "norm.weight grad norm: 0.01106210332363844\n",
            "norm.bias grad norm: 0.010284718126058578\n",
            "lstm.weight_ih_l0 grad norm: 0.0676996037364006\n",
            "lstm.weight_hh_l0 grad norm: 0.017093686386942863\n",
            "lstm.bias_ih_l0 grad norm: 0.006161414086818695\n",
            "lstm.bias_hh_l0 grad norm: 0.006161414086818695\n",
            "fc.weight grad norm: 0.15054531395435333\n",
            "fc.bias grad norm: 0.050490301102399826\n",
            "[Batch 6000] Loss: 0.0080\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.028038522228598595\n",
            "c0 grad norm: 0.06210311874747276\n",
            "conv1.weight grad norm: 0.23868152499198914\n",
            "conv1.bias grad norm: 0.045407719910144806\n",
            "norm.weight grad norm: 0.032325081527233124\n",
            "norm.bias grad norm: 0.031092608347535133\n",
            "lstm.weight_ih_l0 grad norm: 0.1909596472978592\n",
            "lstm.weight_hh_l0 grad norm: 0.050956230610609055\n",
            "lstm.bias_ih_l0 grad norm: 0.01779932714998722\n",
            "lstm.bias_hh_l0 grad norm: 0.01779932714998722\n",
            "fc.weight grad norm: 0.36171066761016846\n",
            "fc.bias grad norm: 0.1536494940519333\n",
            "[Batch 7000] Loss: 0.0364\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.0873928964138031\n",
            "c0 grad norm: 0.120756134390831\n",
            "conv1.weight grad norm: 0.9297528266906738\n",
            "conv1.bias grad norm: 0.19055376946926117\n",
            "norm.weight grad norm: 0.13665629923343658\n",
            "norm.bias grad norm: 0.15146280825138092\n",
            "lstm.weight_ih_l0 grad norm: 0.48949113488197327\n",
            "lstm.weight_hh_l0 grad norm: 0.10188865661621094\n",
            "lstm.bias_ih_l0 grad norm: 0.046489134430885315\n",
            "lstm.bias_hh_l0 grad norm: 0.046489134430885315\n",
            "fc.weight grad norm: 0.533626139163971\n",
            "fc.bias grad norm: 0.1824377328157425\n",
            "[Batch 8000] Loss: 0.2113\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.03653717413544655\n",
            "c0 grad norm: 0.04150453582406044\n",
            "conv1.weight grad norm: 0.3528827428817749\n",
            "conv1.bias grad norm: 0.05836189538240433\n",
            "norm.weight grad norm: 0.03409889340400696\n",
            "norm.bias grad norm: 0.047619983553886414\n",
            "lstm.weight_ih_l0 grad norm: 0.18535469472408295\n",
            "lstm.weight_hh_l0 grad norm: 0.05939881503582001\n",
            "lstm.bias_ih_l0 grad norm: 0.018420711159706116\n",
            "lstm.bias_hh_l0 grad norm: 0.018420711159706116\n",
            "fc.weight grad norm: 0.27146267890930176\n",
            "fc.bias grad norm: 0.07557981461286545\n",
            "[Batch 9000] Loss: 0.0217\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 73\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.0060945372097194195\n",
            "c0 grad norm: 0.01656726561486721\n",
            "conv1.weight grad norm: 0.06747692078351974\n",
            "conv1.bias grad norm: 0.01735045202076435\n",
            "norm.weight grad norm: 0.00910725723952055\n",
            "norm.bias grad norm: 0.011303157545626163\n",
            "lstm.weight_ih_l0 grad norm: 0.04761797934770584\n",
            "lstm.weight_hh_l0 grad norm: 0.013915246352553368\n",
            "lstm.bias_ih_l0 grad norm: 0.004445782396942377\n",
            "lstm.bias_hh_l0 grad norm: 0.004445782396942377\n",
            "fc.weight grad norm: 0.12383562326431274\n",
            "fc.bias grad norm: 0.03729625791311264\n",
            "[Batch 0] Loss: 0.0037\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.051213789731264114\n",
            "c0 grad norm: 0.029028521850705147\n",
            "conv1.weight grad norm: 0.3077675700187683\n",
            "conv1.bias grad norm: 0.09987559169530869\n",
            "norm.weight grad norm: 0.057887397706508636\n",
            "norm.bias grad norm: 0.08970644325017929\n",
            "lstm.weight_ih_l0 grad norm: 0.28285953402519226\n",
            "lstm.weight_hh_l0 grad norm: 0.04740743339061737\n",
            "lstm.bias_ih_l0 grad norm: 0.0287461094558239\n",
            "lstm.bias_hh_l0 grad norm: 0.0287461094558239\n",
            "fc.weight grad norm: 0.15068699419498444\n",
            "fc.bias grad norm: 0.05154848471283913\n",
            "[Batch 1000] Loss: 0.0101\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.06536444276571274\n",
            "c0 grad norm: 0.09802073985338211\n",
            "conv1.weight grad norm: 0.6304010152816772\n",
            "conv1.bias grad norm: 0.17889434099197388\n",
            "norm.weight grad norm: 0.057137858122587204\n",
            "norm.bias grad norm: 0.06715543568134308\n",
            "lstm.weight_ih_l0 grad norm: 0.2673487663269043\n",
            "lstm.weight_hh_l0 grad norm: 0.05958494171500206\n",
            "lstm.bias_ih_l0 grad norm: 0.02617659792304039\n",
            "lstm.bias_hh_l0 grad norm: 0.02617659792304039\n",
            "fc.weight grad norm: 0.3301525115966797\n",
            "fc.bias grad norm: 0.1595788300037384\n",
            "[Batch 2000] Loss: 0.0641\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.013788696378469467\n",
            "c0 grad norm: 0.015289544127881527\n",
            "conv1.weight grad norm: 0.17514632642269135\n",
            "conv1.bias grad norm: 0.042412251234054565\n",
            "norm.weight grad norm: 0.01606365479528904\n",
            "norm.bias grad norm: 0.02714102528989315\n",
            "lstm.weight_ih_l0 grad norm: 0.08518710732460022\n",
            "lstm.weight_hh_l0 grad norm: 0.018844326958060265\n",
            "lstm.bias_ih_l0 grad norm: 0.007953948341310024\n",
            "lstm.bias_hh_l0 grad norm: 0.007953948341310024\n",
            "fc.weight grad norm: 0.13538794219493866\n",
            "fc.bias grad norm: 0.02601468190550804\n",
            "[Batch 3000] Loss: 0.0122\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.02763371169567108\n",
            "c0 grad norm: 0.02776697278022766\n",
            "conv1.weight grad norm: 0.42275962233543396\n",
            "conv1.bias grad norm: 0.11864592880010605\n",
            "norm.weight grad norm: 0.04669731482863426\n",
            "norm.bias grad norm: 0.04719129577279091\n",
            "lstm.weight_ih_l0 grad norm: 0.2692561745643616\n",
            "lstm.weight_hh_l0 grad norm: 0.06252886354923248\n",
            "lstm.bias_ih_l0 grad norm: 0.027051005512475967\n",
            "lstm.bias_hh_l0 grad norm: 0.027051005512475967\n",
            "fc.weight grad norm: 0.15222880244255066\n",
            "fc.bias grad norm: 0.04075111821293831\n",
            "[Batch 4000] Loss: 0.0138\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.07816727459430695\n",
            "c0 grad norm: 0.11479809880256653\n",
            "conv1.weight grad norm: 1.736029863357544\n",
            "conv1.bias grad norm: 0.2753434479236603\n",
            "norm.weight grad norm: 0.1555481106042862\n",
            "norm.bias grad norm: 0.17081551253795624\n",
            "lstm.weight_ih_l0 grad norm: 0.7342064380645752\n",
            "lstm.weight_hh_l0 grad norm: 0.22470004856586456\n",
            "lstm.bias_ih_l0 grad norm: 0.06772242486476898\n",
            "lstm.bias_hh_l0 grad norm: 0.06772242486476898\n",
            "fc.weight grad norm: 1.0045511722564697\n",
            "fc.bias grad norm: 0.24443712830543518\n",
            "[Batch 5000] Loss: 0.4154\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.026210570707917213\n",
            "c0 grad norm: 0.12115445733070374\n",
            "conv1.weight grad norm: 0.4010702073574066\n",
            "conv1.bias grad norm: 0.07219815999269485\n",
            "norm.weight grad norm: 0.06294851750135422\n",
            "norm.bias grad norm: 0.07576877623796463\n",
            "lstm.weight_ih_l0 grad norm: 0.46487975120544434\n",
            "lstm.weight_hh_l0 grad norm: 0.1429780125617981\n",
            "lstm.bias_ih_l0 grad norm: 0.043537382036447525\n",
            "lstm.bias_hh_l0 grad norm: 0.043537382036447525\n",
            "fc.weight grad norm: 0.4305362403392792\n",
            "fc.bias grad norm: 0.10467024892568588\n",
            "[Batch 6000] Loss: 0.0745\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.048228222876787186\n",
            "c0 grad norm: 0.08690420538187027\n",
            "conv1.weight grad norm: 0.6724134087562561\n",
            "conv1.bias grad norm: 0.15106481313705444\n",
            "norm.weight grad norm: 0.08311670273542404\n",
            "norm.bias grad norm: 0.11016663163900375\n",
            "lstm.weight_ih_l0 grad norm: 0.4439055025577545\n",
            "lstm.weight_hh_l0 grad norm: 0.09263318032026291\n",
            "lstm.bias_ih_l0 grad norm: 0.041520699858665466\n",
            "lstm.bias_hh_l0 grad norm: 0.041520699858665466\n",
            "fc.weight grad norm: 0.325074702501297\n",
            "fc.bias grad norm: 0.06888816505670547\n",
            "[Batch 7000] Loss: 0.0936\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.02286255918443203\n",
            "c0 grad norm: 0.04420570656657219\n",
            "conv1.weight grad norm: 0.1367255449295044\n",
            "conv1.bias grad norm: 0.02729019895195961\n",
            "norm.weight grad norm: 0.021394887939095497\n",
            "norm.bias grad norm: 0.020409898832440376\n",
            "lstm.weight_ih_l0 grad norm: 0.1557483822107315\n",
            "lstm.weight_hh_l0 grad norm: 0.04605129361152649\n",
            "lstm.bias_ih_l0 grad norm: 0.014959720894694328\n",
            "lstm.bias_hh_l0 grad norm: 0.014959720894694328\n",
            "fc.weight grad norm: 0.23630885779857635\n",
            "fc.bias grad norm: 0.10126543045043945\n",
            "[Batch 8000] Loss: 0.0245\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.03683869540691376\n",
            "c0 grad norm: 0.043309636414051056\n",
            "conv1.weight grad norm: 0.5724595785140991\n",
            "conv1.bias grad norm: 0.12883369624614716\n",
            "norm.weight grad norm: 0.046475525945425034\n",
            "norm.bias grad norm: 0.04418491572141647\n",
            "lstm.weight_ih_l0 grad norm: 0.26954689621925354\n",
            "lstm.weight_hh_l0 grad norm: 0.05045591667294502\n",
            "lstm.bias_ih_l0 grad norm: 0.026843683794140816\n",
            "lstm.bias_hh_l0 grad norm: 0.026843683794140816\n",
            "fc.weight grad norm: 0.19904965162277222\n",
            "fc.bias grad norm: 0.0317409373819828\n",
            "[Batch 9000] Loss: 0.0219\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 74\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.010984329506754875\n",
            "c0 grad norm: 0.01605488359928131\n",
            "conv1.weight grad norm: 0.13297511637210846\n",
            "conv1.bias grad norm: 0.03560161590576172\n",
            "norm.weight grad norm: 0.01776847057044506\n",
            "norm.bias grad norm: 0.028791282325983047\n",
            "lstm.weight_ih_l0 grad norm: 0.1090506762266159\n",
            "lstm.weight_hh_l0 grad norm: 0.033032167702913284\n",
            "lstm.bias_ih_l0 grad norm: 0.011695419438183308\n",
            "lstm.bias_hh_l0 grad norm: 0.011695419438183308\n",
            "fc.weight grad norm: 0.2708037495613098\n",
            "fc.bias grad norm: 0.1128527969121933\n",
            "[Batch 0] Loss: 0.0276\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.010543687269091606\n",
            "c0 grad norm: 0.015721632167696953\n",
            "conv1.weight grad norm: 0.17021186649799347\n",
            "conv1.bias grad norm: 0.030412767082452774\n",
            "norm.weight grad norm: 0.026842093095183372\n",
            "norm.bias grad norm: 0.019966930150985718\n",
            "lstm.weight_ih_l0 grad norm: 0.14701732993125916\n",
            "lstm.weight_hh_l0 grad norm: 0.04180695489048958\n",
            "lstm.bias_ih_l0 grad norm: 0.013689038343727589\n",
            "lstm.bias_hh_l0 grad norm: 0.013689038343727589\n",
            "fc.weight grad norm: 0.35427966713905334\n",
            "fc.bias grad norm: 0.12336939573287964\n",
            "[Batch 1000] Loss: 0.0591\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.02022636868059635\n",
            "c0 grad norm: 0.028474178165197372\n",
            "conv1.weight grad norm: 0.2251022458076477\n",
            "conv1.bias grad norm: 0.06610874086618423\n",
            "norm.weight grad norm: 0.02821778878569603\n",
            "norm.bias grad norm: 0.04944669455289841\n",
            "lstm.weight_ih_l0 grad norm: 0.13307535648345947\n",
            "lstm.weight_hh_l0 grad norm: 0.03194626793265343\n",
            "lstm.bias_ih_l0 grad norm: 0.013773323968052864\n",
            "lstm.bias_hh_l0 grad norm: 0.013773323968052864\n",
            "fc.weight grad norm: 0.1685580015182495\n",
            "fc.bias grad norm: 0.060062069445848465\n",
            "[Batch 2000] Loss: 0.0196\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.012739754281938076\n",
            "c0 grad norm: 0.011670597828924656\n",
            "conv1.weight grad norm: 0.11001065373420715\n",
            "conv1.bias grad norm: 0.03118259645998478\n",
            "norm.weight grad norm: 0.01652676798403263\n",
            "norm.bias grad norm: 0.02348398230969906\n",
            "lstm.weight_ih_l0 grad norm: 0.12732058763504028\n",
            "lstm.weight_hh_l0 grad norm: 0.04026054963469505\n",
            "lstm.bias_ih_l0 grad norm: 0.012571591883897781\n",
            "lstm.bias_hh_l0 grad norm: 0.012571591883897781\n",
            "fc.weight grad norm: 0.20772945880889893\n",
            "fc.bias grad norm: 0.0684916228055954\n",
            "[Batch 3000] Loss: 0.0100\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.0109260817989707\n",
            "c0 grad norm: 0.04289695620536804\n",
            "conv1.weight grad norm: 0.15930689871311188\n",
            "conv1.bias grad norm: 0.024668514728546143\n",
            "norm.weight grad norm: 0.029727304354310036\n",
            "norm.bias grad norm: 0.02800976298749447\n",
            "lstm.weight_ih_l0 grad norm: 0.14462952315807343\n",
            "lstm.weight_hh_l0 grad norm: 0.04246581718325615\n",
            "lstm.bias_ih_l0 grad norm: 0.013477017171680927\n",
            "lstm.bias_hh_l0 grad norm: 0.013477017171680927\n",
            "fc.weight grad norm: 0.25773683190345764\n",
            "fc.bias grad norm: 0.08295349776744843\n",
            "[Batch 4000] Loss: 0.0321\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.13024894893169403\n",
            "c0 grad norm: 0.16184264421463013\n",
            "conv1.weight grad norm: 1.8844047784805298\n",
            "conv1.bias grad norm: 0.2799929976463318\n",
            "norm.weight grad norm: 0.24442793428897858\n",
            "norm.bias grad norm: 0.2734391689300537\n",
            "lstm.weight_ih_l0 grad norm: 1.0662950277328491\n",
            "lstm.weight_hh_l0 grad norm: 0.198531374335289\n",
            "lstm.bias_ih_l0 grad norm: 0.10599881410598755\n",
            "lstm.bias_hh_l0 grad norm: 0.10599881410598755\n",
            "fc.weight grad norm: 0.898598313331604\n",
            "fc.bias grad norm: 0.3656305968761444\n",
            "[Batch 5000] Loss: 0.3698\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.016632510349154472\n",
            "c0 grad norm: 0.015385954640805721\n",
            "conv1.weight grad norm: 0.17384803295135498\n",
            "conv1.bias grad norm: 0.03229404240846634\n",
            "norm.weight grad norm: 0.017618324607610703\n",
            "norm.bias grad norm: 0.018978511914610863\n",
            "lstm.weight_ih_l0 grad norm: 0.12645256519317627\n",
            "lstm.weight_hh_l0 grad norm: 0.0363117940723896\n",
            "lstm.bias_ih_l0 grad norm: 0.012121143750846386\n",
            "lstm.bias_hh_l0 grad norm: 0.012121143750846386\n",
            "fc.weight grad norm: 0.12349741160869598\n",
            "fc.bias grad norm: 0.054338239133358\n",
            "[Batch 6000] Loss: 0.0118\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.030303189530968666\n",
            "c0 grad norm: 0.021306507289409637\n",
            "conv1.weight grad norm: 0.20924104750156403\n",
            "conv1.bias grad norm: 0.0654338076710701\n",
            "norm.weight grad norm: 0.03544049337506294\n",
            "norm.bias grad norm: 0.04262043163180351\n",
            "lstm.weight_ih_l0 grad norm: 0.2532876431941986\n",
            "lstm.weight_hh_l0 grad norm: 0.07440707832574844\n",
            "lstm.bias_ih_l0 grad norm: 0.024361249059438705\n",
            "lstm.bias_hh_l0 grad norm: 0.024361249059438705\n",
            "fc.weight grad norm: 0.27927860617637634\n",
            "fc.bias grad norm: 0.08522457629442215\n",
            "[Batch 7000] Loss: 0.0278\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.0256816316395998\n",
            "c0 grad norm: 0.0419267900288105\n",
            "conv1.weight grad norm: 0.3501611649990082\n",
            "conv1.bias grad norm: 0.05279575660824776\n",
            "norm.weight grad norm: 0.047932498157024384\n",
            "norm.bias grad norm: 0.04333483427762985\n",
            "lstm.weight_ih_l0 grad norm: 0.260878324508667\n",
            "lstm.weight_hh_l0 grad norm: 0.08100283145904541\n",
            "lstm.bias_ih_l0 grad norm: 0.024535637348890305\n",
            "lstm.bias_hh_l0 grad norm: 0.024535637348890305\n",
            "fc.weight grad norm: 0.2685454487800598\n",
            "fc.bias grad norm: 0.09157510846853256\n",
            "[Batch 8000] Loss: 0.0406\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.021544087678194046\n",
            "c0 grad norm: 0.034829314798116684\n",
            "conv1.weight grad norm: 0.24232478439807892\n",
            "conv1.bias grad norm: 0.08332587033510208\n",
            "norm.weight grad norm: 0.040434423834085464\n",
            "norm.bias grad norm: 0.04734755679965019\n",
            "lstm.weight_ih_l0 grad norm: 0.18905945122241974\n",
            "lstm.weight_hh_l0 grad norm: 0.065506212413311\n",
            "lstm.bias_ih_l0 grad norm: 0.01806436851620674\n",
            "lstm.bias_hh_l0 grad norm: 0.01806436851620674\n",
            "fc.weight grad norm: 0.21442463994026184\n",
            "fc.bias grad norm: 0.08271095901727676\n",
            "[Batch 9000] Loss: 0.0224\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 75\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.01222811359912157\n",
            "c0 grad norm: 0.008928028866648674\n",
            "conv1.weight grad norm: 0.14535942673683167\n",
            "conv1.bias grad norm: 0.0376436673104763\n",
            "norm.weight grad norm: 0.017599137499928474\n",
            "norm.bias grad norm: 0.021308669820427895\n",
            "lstm.weight_ih_l0 grad norm: 0.11069709807634354\n",
            "lstm.weight_hh_l0 grad norm: 0.028728516772389412\n",
            "lstm.bias_ih_l0 grad norm: 0.010956231504678726\n",
            "lstm.bias_hh_l0 grad norm: 0.010956231504678726\n",
            "fc.weight grad norm: 0.17918363213539124\n",
            "fc.bias grad norm: 0.07147003710269928\n",
            "[Batch 0] Loss: 0.0182\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.006910994183272123\n",
            "c0 grad norm: 0.015629800036549568\n",
            "conv1.weight grad norm: 0.11176129430532455\n",
            "conv1.bias grad norm: 0.020026948302984238\n",
            "norm.weight grad norm: 0.012544930912554264\n",
            "norm.bias grad norm: 0.0125352768227458\n",
            "lstm.weight_ih_l0 grad norm: 0.07996973395347595\n",
            "lstm.weight_hh_l0 grad norm: 0.023320913314819336\n",
            "lstm.bias_ih_l0 grad norm: 0.007790610194206238\n",
            "lstm.bias_hh_l0 grad norm: 0.007790610194206238\n",
            "fc.weight grad norm: 0.09001969546079636\n",
            "fc.bias grad norm: 0.021878061816096306\n",
            "[Batch 1000] Loss: 0.0061\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.009427872486412525\n",
            "c0 grad norm: 0.01892562210559845\n",
            "conv1.weight grad norm: 0.07381845265626907\n",
            "conv1.bias grad norm: 0.016684597358107567\n",
            "norm.weight grad norm: 0.00981770921498537\n",
            "norm.bias grad norm: 0.010501672513782978\n",
            "lstm.weight_ih_l0 grad norm: 0.06614944338798523\n",
            "lstm.weight_hh_l0 grad norm: 0.01835210993885994\n",
            "lstm.bias_ih_l0 grad norm: 0.006092363502830267\n",
            "lstm.bias_hh_l0 grad norm: 0.006092363502830267\n",
            "fc.weight grad norm: 0.12259156256914139\n",
            "fc.bias grad norm: 0.04300567880272865\n",
            "[Batch 2000] Loss: 0.0075\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.04777452349662781\n",
            "c0 grad norm: 0.04703275114297867\n",
            "conv1.weight grad norm: 0.39179280400276184\n",
            "conv1.bias grad norm: 0.08084626495838165\n",
            "norm.weight grad norm: 0.05633202940225601\n",
            "norm.bias grad norm: 0.06924161314964294\n",
            "lstm.weight_ih_l0 grad norm: 0.31291988492012024\n",
            "lstm.weight_hh_l0 grad norm: 0.07092724740505219\n",
            "lstm.bias_ih_l0 grad norm: 0.02962668240070343\n",
            "lstm.bias_hh_l0 grad norm: 0.02962668240070343\n",
            "fc.weight grad norm: 0.30788174271583557\n",
            "fc.bias grad norm: 0.15965388715267181\n",
            "[Batch 3000] Loss: 0.0418\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.02888571098446846\n",
            "c0 grad norm: 0.04157230257987976\n",
            "conv1.weight grad norm: 0.276775598526001\n",
            "conv1.bias grad norm: 0.05317625775933266\n",
            "norm.weight grad norm: 0.035745617002248764\n",
            "norm.bias grad norm: 0.027276860550045967\n",
            "lstm.weight_ih_l0 grad norm: 0.21579483151435852\n",
            "lstm.weight_hh_l0 grad norm: 0.05538792535662651\n",
            "lstm.bias_ih_l0 grad norm: 0.0202163215726614\n",
            "lstm.bias_hh_l0 grad norm: 0.0202163215726614\n",
            "fc.weight grad norm: 0.20306046307086945\n",
            "fc.bias grad norm: 0.044772837311029434\n",
            "[Batch 4000] Loss: 0.0249\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.020069461315870285\n",
            "c0 grad norm: 0.025857213884592056\n",
            "conv1.weight grad norm: 0.19903616607189178\n",
            "conv1.bias grad norm: 0.04137643426656723\n",
            "norm.weight grad norm: 0.028887266293168068\n",
            "norm.bias grad norm: 0.030084863305091858\n",
            "lstm.weight_ih_l0 grad norm: 0.20470893383026123\n",
            "lstm.weight_hh_l0 grad norm: 0.05489478260278702\n",
            "lstm.bias_ih_l0 grad norm: 0.015695955604314804\n",
            "lstm.bias_hh_l0 grad norm: 0.015695955604314804\n",
            "fc.weight grad norm: 0.20119279623031616\n",
            "fc.bias grad norm: 0.05816197395324707\n",
            "[Batch 5000] Loss: 0.0213\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.017960280179977417\n",
            "c0 grad norm: 0.0215785913169384\n",
            "conv1.weight grad norm: 0.3002200126647949\n",
            "conv1.bias grad norm: 0.07942570745944977\n",
            "norm.weight grad norm: 0.04148656129837036\n",
            "norm.bias grad norm: 0.04697998985648155\n",
            "lstm.weight_ih_l0 grad norm: 0.1780533790588379\n",
            "lstm.weight_hh_l0 grad norm: 0.05035051703453064\n",
            "lstm.bias_ih_l0 grad norm: 0.016565971076488495\n",
            "lstm.bias_hh_l0 grad norm: 0.016565971076488495\n",
            "fc.weight grad norm: 0.20305265486240387\n",
            "fc.bias grad norm: 0.0920659601688385\n",
            "[Batch 6000] Loss: 0.0598\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.028259005397558212\n",
            "c0 grad norm: 0.059993330389261246\n",
            "conv1.weight grad norm: 0.35957470536231995\n",
            "conv1.bias grad norm: 0.08721482753753662\n",
            "norm.weight grad norm: 0.0675891637802124\n",
            "norm.bias grad norm: 0.06655171513557434\n",
            "lstm.weight_ih_l0 grad norm: 0.3625154197216034\n",
            "lstm.weight_hh_l0 grad norm: 0.09317418932914734\n",
            "lstm.bias_ih_l0 grad norm: 0.034439489245414734\n",
            "lstm.bias_hh_l0 grad norm: 0.034439489245414734\n",
            "fc.weight grad norm: 0.4273974895477295\n",
            "fc.bias grad norm: 0.19832956790924072\n",
            "[Batch 7000] Loss: 0.0762\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.016218386590480804\n",
            "c0 grad norm: 0.0288863442838192\n",
            "conv1.weight grad norm: 0.40149593353271484\n",
            "conv1.bias grad norm: 0.08700411021709442\n",
            "norm.weight grad norm: 0.060109466314315796\n",
            "norm.bias grad norm: 0.05823222175240517\n",
            "lstm.weight_ih_l0 grad norm: 0.21897999942302704\n",
            "lstm.weight_hh_l0 grad norm: 0.06855914741754532\n",
            "lstm.bias_ih_l0 grad norm: 0.019900919869542122\n",
            "lstm.bias_hh_l0 grad norm: 0.019900919869542122\n",
            "fc.weight grad norm: 0.31798458099365234\n",
            "fc.bias grad norm: 0.09110596776008606\n",
            "[Batch 8000] Loss: 0.0620\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.05716278403997421\n",
            "c0 grad norm: 0.08615074306726456\n",
            "conv1.weight grad norm: 0.7833864092826843\n",
            "conv1.bias grad norm: 0.1388331800699234\n",
            "norm.weight grad norm: 0.0737135112285614\n",
            "norm.bias grad norm: 0.09751824289560318\n",
            "lstm.weight_ih_l0 grad norm: 0.6219404935836792\n",
            "lstm.weight_hh_l0 grad norm: 0.15354931354522705\n",
            "lstm.bias_ih_l0 grad norm: 0.06224492937326431\n",
            "lstm.bias_hh_l0 grad norm: 0.06224492937326431\n",
            "fc.weight grad norm: 0.41730791330337524\n",
            "fc.bias grad norm: 0.1318204402923584\n",
            "[Batch 9000] Loss: 0.0539\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 76\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.05158427730202675\n",
            "c0 grad norm: 0.09785324335098267\n",
            "conv1.weight grad norm: 0.5681685209274292\n",
            "conv1.bias grad norm: 0.1224825531244278\n",
            "norm.weight grad norm: 0.06947548687458038\n",
            "norm.bias grad norm: 0.09767767786979675\n",
            "lstm.weight_ih_l0 grad norm: 0.3637358844280243\n",
            "lstm.weight_hh_l0 grad norm: 0.0736168846487999\n",
            "lstm.bias_ih_l0 grad norm: 0.036933738738298416\n",
            "lstm.bias_hh_l0 grad norm: 0.036933738738298416\n",
            "fc.weight grad norm: 0.57122802734375\n",
            "fc.bias grad norm: 0.22828309237957\n",
            "[Batch 0] Loss: 0.1259\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.022011728957295418\n",
            "c0 grad norm: 0.030269907787442207\n",
            "conv1.weight grad norm: 0.1868985891342163\n",
            "conv1.bias grad norm: 0.0505620539188385\n",
            "norm.weight grad norm: 0.030775023624300957\n",
            "norm.bias grad norm: 0.03978046774864197\n",
            "lstm.weight_ih_l0 grad norm: 0.20946431159973145\n",
            "lstm.weight_hh_l0 grad norm: 0.041706912219524384\n",
            "lstm.bias_ih_l0 grad norm: 0.020794132724404335\n",
            "lstm.bias_hh_l0 grad norm: 0.020794132724404335\n",
            "fc.weight grad norm: 0.23030966520309448\n",
            "fc.bias grad norm: 0.11600056290626526\n",
            "[Batch 1000] Loss: 0.0247\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.1540571004152298\n",
            "c0 grad norm: 0.23679578304290771\n",
            "conv1.weight grad norm: 2.2885701656341553\n",
            "conv1.bias grad norm: 0.6013076901435852\n",
            "norm.weight grad norm: 0.26477885246276855\n",
            "norm.bias grad norm: 0.33263498544692993\n",
            "lstm.weight_ih_l0 grad norm: 1.4142183065414429\n",
            "lstm.weight_hh_l0 grad norm: 0.4419153332710266\n",
            "lstm.bias_ih_l0 grad norm: 0.13908588886260986\n",
            "lstm.bias_hh_l0 grad norm: 0.13908588886260986\n",
            "fc.weight grad norm: 1.539996862411499\n",
            "fc.bias grad norm: 0.5176526308059692\n",
            "[Batch 2000] Loss: 0.6103\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.07208029180765152\n",
            "c0 grad norm: 0.06293603777885437\n",
            "conv1.weight grad norm: 0.7698372602462769\n",
            "conv1.bias grad norm: 0.17267116904258728\n",
            "norm.weight grad norm: 0.04711567983031273\n",
            "norm.bias grad norm: 0.06692283600568771\n",
            "lstm.weight_ih_l0 grad norm: 0.3545132279396057\n",
            "lstm.weight_hh_l0 grad norm: 0.07348860055208206\n",
            "lstm.bias_ih_l0 grad norm: 0.03578684478998184\n",
            "lstm.bias_hh_l0 grad norm: 0.03578684478998184\n",
            "fc.weight grad norm: 0.18958640098571777\n",
            "fc.bias grad norm: 0.06282003968954086\n",
            "[Batch 3000] Loss: 0.0222\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.03456966578960419\n",
            "c0 grad norm: 0.03027164377272129\n",
            "conv1.weight grad norm: 0.24091356992721558\n",
            "conv1.bias grad norm: 0.058964889496564865\n",
            "norm.weight grad norm: 0.026199473068118095\n",
            "norm.bias grad norm: 0.03096538782119751\n",
            "lstm.weight_ih_l0 grad norm: 0.18306408822536469\n",
            "lstm.weight_hh_l0 grad norm: 0.04080985486507416\n",
            "lstm.bias_ih_l0 grad norm: 0.017968768253922462\n",
            "lstm.bias_hh_l0 grad norm: 0.017968768253922462\n",
            "fc.weight grad norm: 0.20294754207134247\n",
            "fc.bias grad norm: 0.05197576433420181\n",
            "[Batch 4000] Loss: 0.0178\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.016723262146115303\n",
            "c0 grad norm: 0.03893324360251427\n",
            "conv1.weight grad norm: 0.2921309769153595\n",
            "conv1.bias grad norm: 0.08361703902482986\n",
            "norm.weight grad norm: 0.04967430606484413\n",
            "norm.bias grad norm: 0.06158456951379776\n",
            "lstm.weight_ih_l0 grad norm: 0.27322089672088623\n",
            "lstm.weight_hh_l0 grad norm: 0.08039749413728714\n",
            "lstm.bias_ih_l0 grad norm: 0.024892544373869896\n",
            "lstm.bias_hh_l0 grad norm: 0.024892544373869896\n",
            "fc.weight grad norm: 0.3362977206707001\n",
            "fc.bias grad norm: 0.07873377203941345\n",
            "[Batch 5000] Loss: 0.0289\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.008160066790878773\n",
            "c0 grad norm: 0.029307981953024864\n",
            "conv1.weight grad norm: 0.10980945825576782\n",
            "conv1.bias grad norm: 0.020346619188785553\n",
            "norm.weight grad norm: 0.018242305144667625\n",
            "norm.bias grad norm: 0.02115330845117569\n",
            "lstm.weight_ih_l0 grad norm: 0.1121286153793335\n",
            "lstm.weight_hh_l0 grad norm: 0.033917222172021866\n",
            "lstm.bias_ih_l0 grad norm: 0.010470499284565449\n",
            "lstm.bias_hh_l0 grad norm: 0.010470499284565449\n",
            "fc.weight grad norm: 0.37389400601387024\n",
            "fc.bias grad norm: 0.1214277371764183\n",
            "[Batch 6000] Loss: 0.0399\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.017309239134192467\n",
            "c0 grad norm: 0.025252144783735275\n",
            "conv1.weight grad norm: 0.17868569493293762\n",
            "conv1.bias grad norm: 0.046766653656959534\n",
            "norm.weight grad norm: 0.025047099217772484\n",
            "norm.bias grad norm: 0.037552185356616974\n",
            "lstm.weight_ih_l0 grad norm: 0.12280971556901932\n",
            "lstm.weight_hh_l0 grad norm: 0.032213546335697174\n",
            "lstm.bias_ih_l0 grad norm: 0.013453101739287376\n",
            "lstm.bias_hh_l0 grad norm: 0.013453101739287376\n",
            "fc.weight grad norm: 0.2011650651693344\n",
            "fc.bias grad norm: 0.0891166627407074\n",
            "[Batch 7000] Loss: 0.0145\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.008159845136106014\n",
            "c0 grad norm: 0.01645836792886257\n",
            "conv1.weight grad norm: 0.13394106924533844\n",
            "conv1.bias grad norm: 0.020905964076519012\n",
            "norm.weight grad norm: 0.013787694275379181\n",
            "norm.bias grad norm: 0.01340845599770546\n",
            "lstm.weight_ih_l0 grad norm: 0.08609553426504135\n",
            "lstm.weight_hh_l0 grad norm: 0.024696117267012596\n",
            "lstm.bias_ih_l0 grad norm: 0.008085851557552814\n",
            "lstm.bias_hh_l0 grad norm: 0.008085851557552814\n",
            "fc.weight grad norm: 0.13355906307697296\n",
            "fc.bias grad norm: 0.046767327934503555\n",
            "[Batch 8000] Loss: 0.0109\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.06925185024738312\n",
            "c0 grad norm: 0.10585293918848038\n",
            "conv1.weight grad norm: 0.9842188954353333\n",
            "conv1.bias grad norm: 0.23338447511196136\n",
            "norm.weight grad norm: 0.10647068172693253\n",
            "norm.bias grad norm: 0.16397251188755035\n",
            "lstm.weight_ih_l0 grad norm: 0.6827184557914734\n",
            "lstm.weight_hh_l0 grad norm: 0.15927641093730927\n",
            "lstm.bias_ih_l0 grad norm: 0.06606239080429077\n",
            "lstm.bias_hh_l0 grad norm: 0.06606239080429077\n",
            "fc.weight grad norm: 0.5952314138412476\n",
            "fc.bias grad norm: 0.2887267470359802\n",
            "[Batch 9000] Loss: 0.2360\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 77\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.06022088602185249\n",
            "c0 grad norm: 0.017609329894185066\n",
            "conv1.weight grad norm: 0.3463839292526245\n",
            "conv1.bias grad norm: 0.07670218497514725\n",
            "norm.weight grad norm: 0.055418577045202255\n",
            "norm.bias grad norm: 0.06640743464231491\n",
            "lstm.weight_ih_l0 grad norm: 0.34164032340049744\n",
            "lstm.weight_hh_l0 grad norm: 0.05482694134116173\n",
            "lstm.bias_ih_l0 grad norm: 0.03251875936985016\n",
            "lstm.bias_hh_l0 grad norm: 0.03251875936985016\n",
            "fc.weight grad norm: 0.26119622588157654\n",
            "fc.bias grad norm: 0.08289697766304016\n",
            "[Batch 0] Loss: 0.0387\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.04571919143199921\n",
            "c0 grad norm: 0.033607639372348785\n",
            "conv1.weight grad norm: 0.3772245943546295\n",
            "conv1.bias grad norm: 0.10446161031723022\n",
            "norm.weight grad norm: 0.049262240529060364\n",
            "norm.bias grad norm: 0.06278675049543381\n",
            "lstm.weight_ih_l0 grad norm: 0.3027992248535156\n",
            "lstm.weight_hh_l0 grad norm: 0.07516010850667953\n",
            "lstm.bias_ih_l0 grad norm: 0.030077021569013596\n",
            "lstm.bias_hh_l0 grad norm: 0.030077021569013596\n",
            "fc.weight grad norm: 0.287237286567688\n",
            "fc.bias grad norm: 0.01889410801231861\n",
            "[Batch 1000] Loss: 0.0405\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.00794711522758007\n",
            "c0 grad norm: 0.018787521868944168\n",
            "conv1.weight grad norm: 0.12082675099372864\n",
            "conv1.bias grad norm: 0.024904286488890648\n",
            "norm.weight grad norm: 0.015050766989588737\n",
            "norm.bias grad norm: 0.01973201334476471\n",
            "lstm.weight_ih_l0 grad norm: 0.08992383629083633\n",
            "lstm.weight_hh_l0 grad norm: 0.028435800224542618\n",
            "lstm.bias_ih_l0 grad norm: 0.008538144640624523\n",
            "lstm.bias_hh_l0 grad norm: 0.008538144640624523\n",
            "fc.weight grad norm: 0.2118421196937561\n",
            "fc.bias grad norm: 0.06637181341648102\n",
            "[Batch 2000] Loss: 0.0136\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.023128783330321312\n",
            "c0 grad norm: 0.019773386418819427\n",
            "conv1.weight grad norm: 0.18197570741176605\n",
            "conv1.bias grad norm: 0.04375433176755905\n",
            "norm.weight grad norm: 0.021717144176363945\n",
            "norm.bias grad norm: 0.029139524325728416\n",
            "lstm.weight_ih_l0 grad norm: 0.15428964793682098\n",
            "lstm.weight_hh_l0 grad norm: 0.037661440670490265\n",
            "lstm.bias_ih_l0 grad norm: 0.014602035284042358\n",
            "lstm.bias_hh_l0 grad norm: 0.014602035284042358\n",
            "fc.weight grad norm: 0.22336606681346893\n",
            "fc.bias grad norm: 0.08922948688268661\n",
            "[Batch 3000] Loss: 0.0138\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.0210727471858263\n",
            "c0 grad norm: 0.0546865276992321\n",
            "conv1.weight grad norm: 0.3612079620361328\n",
            "conv1.bias grad norm: 0.09024720638990402\n",
            "norm.weight grad norm: 0.0643872395157814\n",
            "norm.bias grad norm: 0.06882533431053162\n",
            "lstm.weight_ih_l0 grad norm: 0.2990033030509949\n",
            "lstm.weight_hh_l0 grad norm: 0.07958494871854782\n",
            "lstm.bias_ih_l0 grad norm: 0.028524180874228477\n",
            "lstm.bias_hh_l0 grad norm: 0.028524180874228477\n",
            "fc.weight grad norm: 0.20944027602672577\n",
            "fc.bias grad norm: 0.031630851328372955\n",
            "[Batch 4000] Loss: 0.0400\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.049437277019023895\n",
            "c0 grad norm: 0.04890360310673714\n",
            "conv1.weight grad norm: 0.7013306021690369\n",
            "conv1.bias grad norm: 0.18134817481040955\n",
            "norm.weight grad norm: 0.07512415945529938\n",
            "norm.bias grad norm: 0.10072901844978333\n",
            "lstm.weight_ih_l0 grad norm: 0.5520792007446289\n",
            "lstm.weight_hh_l0 grad norm: 0.1741814762353897\n",
            "lstm.bias_ih_l0 grad norm: 0.04934525117278099\n",
            "lstm.bias_hh_l0 grad norm: 0.04934525117278099\n",
            "fc.weight grad norm: 0.3241790235042572\n",
            "fc.bias grad norm: 0.09443089365959167\n",
            "[Batch 5000] Loss: 0.0419\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.027365192770957947\n",
            "c0 grad norm: 0.029593229293823242\n",
            "conv1.weight grad norm: 0.4305885136127472\n",
            "conv1.bias grad norm: 0.07201975584030151\n",
            "norm.weight grad norm: 0.05927645415067673\n",
            "norm.bias grad norm: 0.07069844752550125\n",
            "lstm.weight_ih_l0 grad norm: 0.33679530024528503\n",
            "lstm.weight_hh_l0 grad norm: 0.08694087713956833\n",
            "lstm.bias_ih_l0 grad norm: 0.032372307032346725\n",
            "lstm.bias_hh_l0 grad norm: 0.032372307032346725\n",
            "fc.weight grad norm: 0.3191245198249817\n",
            "fc.bias grad norm: 0.08338548243045807\n",
            "[Batch 6000] Loss: 0.0587\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.044683486223220825\n",
            "c0 grad norm: 0.055773284286260605\n",
            "conv1.weight grad norm: 0.467248797416687\n",
            "conv1.bias grad norm: 0.1258549988269806\n",
            "norm.weight grad norm: 0.08009583503007889\n",
            "norm.bias grad norm: 0.07898113131523132\n",
            "lstm.weight_ih_l0 grad norm: 0.46752670407295227\n",
            "lstm.weight_hh_l0 grad norm: 0.1452195644378662\n",
            "lstm.bias_ih_l0 grad norm: 0.04351706802845001\n",
            "lstm.bias_hh_l0 grad norm: 0.04351706802845001\n",
            "fc.weight grad norm: 0.8421928286552429\n",
            "fc.bias grad norm: 0.26415199041366577\n",
            "[Batch 7000] Loss: 0.2693\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.047004539519548416\n",
            "c0 grad norm: 0.0942939892411232\n",
            "conv1.weight grad norm: 0.5849726796150208\n",
            "conv1.bias grad norm: 0.12136828154325485\n",
            "norm.weight grad norm: 0.09873224794864655\n",
            "norm.bias grad norm: 0.11703240126371384\n",
            "lstm.weight_ih_l0 grad norm: 0.5586161017417908\n",
            "lstm.weight_hh_l0 grad norm: 0.1593635529279709\n",
            "lstm.bias_ih_l0 grad norm: 0.05206242576241493\n",
            "lstm.bias_hh_l0 grad norm: 0.05206242576241493\n",
            "fc.weight grad norm: 0.5034273862838745\n",
            "fc.bias grad norm: 0.20193535089492798\n",
            "[Batch 8000] Loss: 0.0900\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.01108501199632883\n",
            "c0 grad norm: 0.020682482048869133\n",
            "conv1.weight grad norm: 0.15404467284679413\n",
            "conv1.bias grad norm: 0.027619903907179832\n",
            "norm.weight grad norm: 0.022325091063976288\n",
            "norm.bias grad norm: 0.01851375214755535\n",
            "lstm.weight_ih_l0 grad norm: 0.1321646124124527\n",
            "lstm.weight_hh_l0 grad norm: 0.03509149327874184\n",
            "lstm.bias_ih_l0 grad norm: 0.01213875412940979\n",
            "lstm.bias_hh_l0 grad norm: 0.01213875412940979\n",
            "fc.weight grad norm: 0.1312635987997055\n",
            "fc.bias grad norm: 0.04582970216870308\n",
            "[Batch 9000] Loss: 0.0056\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 78\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.021603409200906754\n",
            "c0 grad norm: 0.06767462939023972\n",
            "conv1.weight grad norm: 0.26527896523475647\n",
            "conv1.bias grad norm: 0.0817321389913559\n",
            "norm.weight grad norm: 0.032045621424913406\n",
            "norm.bias grad norm: 0.04149933159351349\n",
            "lstm.weight_ih_l0 grad norm: 0.2042814940214157\n",
            "lstm.weight_hh_l0 grad norm: 0.048095688223838806\n",
            "lstm.bias_ih_l0 grad norm: 0.01996585913002491\n",
            "lstm.bias_hh_l0 grad norm: 0.01996585913002491\n",
            "fc.weight grad norm: 0.31062695384025574\n",
            "fc.bias grad norm: 0.16471539437770844\n",
            "[Batch 0] Loss: 0.0654\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.05019776523113251\n",
            "c0 grad norm: 0.04566477984189987\n",
            "conv1.weight grad norm: 0.447518914937973\n",
            "conv1.bias grad norm: 0.09654310345649719\n",
            "norm.weight grad norm: 0.05271007865667343\n",
            "norm.bias grad norm: 0.06022573262453079\n",
            "lstm.weight_ih_l0 grad norm: 0.30271658301353455\n",
            "lstm.weight_hh_l0 grad norm: 0.07532086968421936\n",
            "lstm.bias_ih_l0 grad norm: 0.02933155745267868\n",
            "lstm.bias_hh_l0 grad norm: 0.02933155745267868\n",
            "fc.weight grad norm: 0.31707116961479187\n",
            "fc.bias grad norm: 0.11225219070911407\n",
            "[Batch 1000] Loss: 0.0502\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.014605564065277576\n",
            "c0 grad norm: 0.032168805599212646\n",
            "conv1.weight grad norm: 0.25454506278038025\n",
            "conv1.bias grad norm: 0.058190908282995224\n",
            "norm.weight grad norm: 0.029931385070085526\n",
            "norm.bias grad norm: 0.03274410218000412\n",
            "lstm.weight_ih_l0 grad norm: 0.17590679228305817\n",
            "lstm.weight_hh_l0 grad norm: 0.049204107373952866\n",
            "lstm.bias_ih_l0 grad norm: 0.01653275452554226\n",
            "lstm.bias_hh_l0 grad norm: 0.01653275452554226\n",
            "fc.weight grad norm: 0.17378494143486023\n",
            "fc.bias grad norm: 0.042724791914224625\n",
            "[Batch 2000] Loss: 0.0118\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.012628702446818352\n",
            "c0 grad norm: 0.0141455652192235\n",
            "conv1.weight grad norm: 0.20873664319515228\n",
            "conv1.bias grad norm: 0.0419461727142334\n",
            "norm.weight grad norm: 0.02019602060317993\n",
            "norm.bias grad norm: 0.024016257375478745\n",
            "lstm.weight_ih_l0 grad norm: 0.11021087318658829\n",
            "lstm.weight_hh_l0 grad norm: 0.034460991621017456\n",
            "lstm.bias_ih_l0 grad norm: 0.01094512827694416\n",
            "lstm.bias_hh_l0 grad norm: 0.01094512827694416\n",
            "fc.weight grad norm: 0.12663239240646362\n",
            "fc.bias grad norm: 0.056102950125932693\n",
            "[Batch 3000] Loss: 0.0096\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.01947086863219738\n",
            "c0 grad norm: 0.022957060486078262\n",
            "conv1.weight grad norm: 0.15290531516075134\n",
            "conv1.bias grad norm: 0.05570380762219429\n",
            "norm.weight grad norm: 0.026452386751770973\n",
            "norm.bias grad norm: 0.03505031764507294\n",
            "lstm.weight_ih_l0 grad norm: 0.18024329841136932\n",
            "lstm.weight_hh_l0 grad norm: 0.04235488176345825\n",
            "lstm.bias_ih_l0 grad norm: 0.01630828157067299\n",
            "lstm.bias_hh_l0 grad norm: 0.01630828157067299\n",
            "fc.weight grad norm: 0.19180919229984283\n",
            "fc.bias grad norm: 0.03115001507103443\n",
            "[Batch 4000] Loss: 0.0189\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.027114491909742355\n",
            "c0 grad norm: 0.08992010354995728\n",
            "conv1.weight grad norm: 0.6713992357254028\n",
            "conv1.bias grad norm: 0.10106448829174042\n",
            "norm.weight grad norm: 0.07114054262638092\n",
            "norm.bias grad norm: 0.08106298744678497\n",
            "lstm.weight_ih_l0 grad norm: 0.3903241455554962\n",
            "lstm.weight_hh_l0 grad norm: 0.10887926071882248\n",
            "lstm.bias_ih_l0 grad norm: 0.03761644288897514\n",
            "lstm.bias_hh_l0 grad norm: 0.03761644288897514\n",
            "fc.weight grad norm: 0.5445012450218201\n",
            "fc.bias grad norm: 0.1694413721561432\n",
            "[Batch 5000] Loss: 0.1189\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.03361845761537552\n",
            "c0 grad norm: 0.03752124682068825\n",
            "conv1.weight grad norm: 0.47738954424858093\n",
            "conv1.bias grad norm: 0.0868387520313263\n",
            "norm.weight grad norm: 0.07325827330350876\n",
            "norm.bias grad norm: 0.06522700190544128\n",
            "lstm.weight_ih_l0 grad norm: 0.3758445978164673\n",
            "lstm.weight_hh_l0 grad norm: 0.11641590297222137\n",
            "lstm.bias_ih_l0 grad norm: 0.03508170694112778\n",
            "lstm.bias_hh_l0 grad norm: 0.03508170694112778\n",
            "fc.weight grad norm: 0.7734866738319397\n",
            "fc.bias grad norm: 0.2227608561515808\n",
            "[Batch 6000] Loss: 0.2357\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.05809096619486809\n",
            "c0 grad norm: 0.09370464086532593\n",
            "conv1.weight grad norm: 0.9406008124351501\n",
            "conv1.bias grad norm: 0.24316957592964172\n",
            "norm.weight grad norm: 0.13628311455249786\n",
            "norm.bias grad norm: 0.14920739829540253\n",
            "lstm.weight_ih_l0 grad norm: 0.689236581325531\n",
            "lstm.weight_hh_l0 grad norm: 0.19622333347797394\n",
            "lstm.bias_ih_l0 grad norm: 0.06717201322317123\n",
            "lstm.bias_hh_l0 grad norm: 0.06717201322317123\n",
            "fc.weight grad norm: 0.9666754007339478\n",
            "fc.bias grad norm: 0.257661372423172\n",
            "[Batch 7000] Loss: 0.3899\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.046173445880413055\n",
            "c0 grad norm: 0.07962434738874435\n",
            "conv1.weight grad norm: 0.46112778782844543\n",
            "conv1.bias grad norm: 0.12121886014938354\n",
            "norm.weight grad norm: 0.07022324204444885\n",
            "norm.bias grad norm: 0.07166945189237595\n",
            "lstm.weight_ih_l0 grad norm: 0.43054860830307007\n",
            "lstm.weight_hh_l0 grad norm: 0.12426337599754333\n",
            "lstm.bias_ih_l0 grad norm: 0.042036306113004684\n",
            "lstm.bias_hh_l0 grad norm: 0.042036306113004684\n",
            "fc.weight grad norm: 0.437927782535553\n",
            "fc.bias grad norm: 0.049200501292943954\n",
            "[Batch 8000] Loss: 0.0628\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.05101165920495987\n",
            "c0 grad norm: 0.028733838349580765\n",
            "conv1.weight grad norm: 0.7509999871253967\n",
            "conv1.bias grad norm: 0.06417984515428543\n",
            "norm.weight grad norm: 0.06093444302678108\n",
            "norm.bias grad norm: 0.04680773615837097\n",
            "lstm.weight_ih_l0 grad norm: 0.2896084487438202\n",
            "lstm.weight_hh_l0 grad norm: 0.06127043440937996\n",
            "lstm.bias_ih_l0 grad norm: 0.027027461677789688\n",
            "lstm.bias_hh_l0 grad norm: 0.027027461677789688\n",
            "fc.weight grad norm: 0.2957630455493927\n",
            "fc.bias grad norm: 0.06937181949615479\n",
            "[Batch 9000] Loss: 0.0324\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 79\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.015054161660373211\n",
            "c0 grad norm: 0.01876332052052021\n",
            "conv1.weight grad norm: 0.14802327752113342\n",
            "conv1.bias grad norm: 0.028747521340847015\n",
            "norm.weight grad norm: 0.024500509724020958\n",
            "norm.bias grad norm: 0.022928116843104362\n",
            "lstm.weight_ih_l0 grad norm: 0.1300773024559021\n",
            "lstm.weight_hh_l0 grad norm: 0.03434963896870613\n",
            "lstm.bias_ih_l0 grad norm: 0.0127333365380764\n",
            "lstm.bias_hh_l0 grad norm: 0.0127333365380764\n",
            "fc.weight grad norm: 0.22543293237686157\n",
            "fc.bias grad norm: 0.09045355021953583\n",
            "[Batch 0] Loss: 0.0299\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.030594727024435997\n",
            "c0 grad norm: 0.03597652167081833\n",
            "conv1.weight grad norm: 0.22698178887367249\n",
            "conv1.bias grad norm: 0.0622883178293705\n",
            "norm.weight grad norm: 0.02551944926381111\n",
            "norm.bias grad norm: 0.02970759943127632\n",
            "lstm.weight_ih_l0 grad norm: 0.1806880235671997\n",
            "lstm.weight_hh_l0 grad norm: 0.04105236008763313\n",
            "lstm.bias_ih_l0 grad norm: 0.017148815095424652\n",
            "lstm.bias_hh_l0 grad norm: 0.017148815095424652\n",
            "fc.weight grad norm: 0.28900039196014404\n",
            "fc.bias grad norm: 0.09963721036911011\n",
            "[Batch 1000] Loss: 0.0257\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.0364965982735157\n",
            "c0 grad norm: 0.03542540967464447\n",
            "conv1.weight grad norm: 0.2877807319164276\n",
            "conv1.bias grad norm: 0.06477317214012146\n",
            "norm.weight grad norm: 0.04858614504337311\n",
            "norm.bias grad norm: 0.05288884788751602\n",
            "lstm.weight_ih_l0 grad norm: 0.249036505818367\n",
            "lstm.weight_hh_l0 grad norm: 0.05126740783452988\n",
            "lstm.bias_ih_l0 grad norm: 0.023480067029595375\n",
            "lstm.bias_hh_l0 grad norm: 0.023480067029595375\n",
            "fc.weight grad norm: 0.37401890754699707\n",
            "fc.bias grad norm: 0.15271174907684326\n",
            "[Batch 2000] Loss: 0.0737\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.05928287282586098\n",
            "c0 grad norm: 0.06996060907840729\n",
            "conv1.weight grad norm: 1.1157870292663574\n",
            "conv1.bias grad norm: 0.1868472546339035\n",
            "norm.weight grad norm: 0.10102356225252151\n",
            "norm.bias grad norm: 0.14912714064121246\n",
            "lstm.weight_ih_l0 grad norm: 0.5551288723945618\n",
            "lstm.weight_hh_l0 grad norm: 0.16446411609649658\n",
            "lstm.bias_ih_l0 grad norm: 0.052357688546180725\n",
            "lstm.bias_hh_l0 grad norm: 0.052357688546180725\n",
            "fc.weight grad norm: 0.5608566999435425\n",
            "fc.bias grad norm: 0.12129945307970047\n",
            "[Batch 3000] Loss: 0.1419\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.044856324791908264\n",
            "c0 grad norm: 0.06414654105901718\n",
            "conv1.weight grad norm: 0.5704687237739563\n",
            "conv1.bias grad norm: 0.13676044344902039\n",
            "norm.weight grad norm: 0.07265107333660126\n",
            "norm.bias grad norm: 0.08221285790205002\n",
            "lstm.weight_ih_l0 grad norm: 0.40055978298187256\n",
            "lstm.weight_hh_l0 grad norm: 0.11433529853820801\n",
            "lstm.bias_ih_l0 grad norm: 0.03881107270717621\n",
            "lstm.bias_hh_l0 grad norm: 0.03881107270717621\n",
            "fc.weight grad norm: 0.2976628541946411\n",
            "fc.bias grad norm: 0.08006215840578079\n",
            "[Batch 4000] Loss: 0.0442\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.02484177239239216\n",
            "c0 grad norm: 0.07907618582248688\n",
            "conv1.weight grad norm: 0.2856888771057129\n",
            "conv1.bias grad norm: 0.05755341425538063\n",
            "norm.weight grad norm: 0.043134357780218124\n",
            "norm.bias grad norm: 0.03801722824573517\n",
            "lstm.weight_ih_l0 grad norm: 0.31686148047447205\n",
            "lstm.weight_hh_l0 grad norm: 0.09755068272352219\n",
            "lstm.bias_ih_l0 grad norm: 0.029678961262106895\n",
            "lstm.bias_hh_l0 grad norm: 0.029678961262106895\n",
            "fc.weight grad norm: 0.2708856165409088\n",
            "fc.bias grad norm: 0.11687229573726654\n",
            "[Batch 5000] Loss: 0.0418\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.02369004487991333\n",
            "c0 grad norm: 0.04968951642513275\n",
            "conv1.weight grad norm: 0.3146427571773529\n",
            "conv1.bias grad norm: 0.03560928627848625\n",
            "norm.weight grad norm: 0.0336378812789917\n",
            "norm.bias grad norm: 0.033339790999889374\n",
            "lstm.weight_ih_l0 grad norm: 0.17859472334384918\n",
            "lstm.weight_hh_l0 grad norm: 0.052568938583135605\n",
            "lstm.bias_ih_l0 grad norm: 0.017996802926063538\n",
            "lstm.bias_hh_l0 grad norm: 0.017996802926063538\n",
            "fc.weight grad norm: 0.3036644160747528\n",
            "fc.bias grad norm: 0.08198627084493637\n",
            "[Batch 6000] Loss: 0.0289\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.017645059153437614\n",
            "c0 grad norm: 0.037409182637929916\n",
            "conv1.weight grad norm: 0.2091154307126999\n",
            "conv1.bias grad norm: 0.06897551566362381\n",
            "norm.weight grad norm: 0.026694370433688164\n",
            "norm.bias grad norm: 0.03907988592982292\n",
            "lstm.weight_ih_l0 grad norm: 0.15531429648399353\n",
            "lstm.weight_hh_l0 grad norm: 0.048494745045900345\n",
            "lstm.bias_ih_l0 grad norm: 0.015243957750499249\n",
            "lstm.bias_hh_l0 grad norm: 0.015243957750499249\n",
            "fc.weight grad norm: 0.18544210493564606\n",
            "fc.bias grad norm: 0.0484762117266655\n",
            "[Batch 7000] Loss: 0.0195\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.02973717264831066\n",
            "c0 grad norm: 0.04050619155168533\n",
            "conv1.weight grad norm: 0.298235684633255\n",
            "conv1.bias grad norm: 0.056630317121744156\n",
            "norm.weight grad norm: 0.030497346073389053\n",
            "norm.bias grad norm: 0.02567036636173725\n",
            "lstm.weight_ih_l0 grad norm: 0.1816747486591339\n",
            "lstm.weight_hh_l0 grad norm: 0.0503271259367466\n",
            "lstm.bias_ih_l0 grad norm: 0.017687901854515076\n",
            "lstm.bias_hh_l0 grad norm: 0.017687901854515076\n",
            "fc.weight grad norm: 0.19612662494182587\n",
            "fc.bias grad norm: 0.034175947308540344\n",
            "[Batch 8000] Loss: 0.0135\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.014270131476223469\n",
            "c0 grad norm: 0.031133586540818214\n",
            "conv1.weight grad norm: 0.1611635684967041\n",
            "conv1.bias grad norm: 0.04605681821703911\n",
            "norm.weight grad norm: 0.023818364366889\n",
            "norm.bias grad norm: 0.024406922981142998\n",
            "lstm.weight_ih_l0 grad norm: 0.12827228009700775\n",
            "lstm.weight_hh_l0 grad norm: 0.03182026743888855\n",
            "lstm.bias_ih_l0 grad norm: 0.01121518760919571\n",
            "lstm.bias_hh_l0 grad norm: 0.01121518760919571\n",
            "fc.weight grad norm: 0.2183942049741745\n",
            "fc.bias grad norm: 0.04576759412884712\n",
            "[Batch 9000] Loss: 0.0282\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 80\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.014630213379859924\n",
            "c0 grad norm: 0.027298543602228165\n",
            "conv1.weight grad norm: 0.16397663950920105\n",
            "conv1.bias grad norm: 0.026927664875984192\n",
            "norm.weight grad norm: 0.023819219321012497\n",
            "norm.bias grad norm: 0.025589115917682648\n",
            "lstm.weight_ih_l0 grad norm: 0.13734441995620728\n",
            "lstm.weight_hh_l0 grad norm: 0.034091878682374954\n",
            "lstm.bias_ih_l0 grad norm: 0.013550587929785252\n",
            "lstm.bias_hh_l0 grad norm: 0.013550587929785252\n",
            "fc.weight grad norm: 0.20619066059589386\n",
            "fc.bias grad norm: 0.07905534654855728\n",
            "[Batch 0] Loss: 0.0205\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.030392764136195183\n",
            "c0 grad norm: 0.03155969828367233\n",
            "conv1.weight grad norm: 0.25991877913475037\n",
            "conv1.bias grad norm: 0.0619303360581398\n",
            "norm.weight grad norm: 0.03760972619056702\n",
            "norm.bias grad norm: 0.03969299793243408\n",
            "lstm.weight_ih_l0 grad norm: 0.24906040728092194\n",
            "lstm.weight_hh_l0 grad norm: 0.058313436806201935\n",
            "lstm.bias_ih_l0 grad norm: 0.02471667341887951\n",
            "lstm.bias_hh_l0 grad norm: 0.02471667341887951\n",
            "fc.weight grad norm: 0.22794978320598602\n",
            "fc.bias grad norm: 0.06843666732311249\n",
            "[Batch 1000] Loss: 0.0362\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.03352125361561775\n",
            "c0 grad norm: 0.04154827445745468\n",
            "conv1.weight grad norm: 0.37073779106140137\n",
            "conv1.bias grad norm: 0.08299504965543747\n",
            "norm.weight grad norm: 0.05369162559509277\n",
            "norm.bias grad norm: 0.07151477783918381\n",
            "lstm.weight_ih_l0 grad norm: 0.3112002909183502\n",
            "lstm.weight_hh_l0 grad norm: 0.08180972188711166\n",
            "lstm.bias_ih_l0 grad norm: 0.030472518876194954\n",
            "lstm.bias_hh_l0 grad norm: 0.030472518876194954\n",
            "fc.weight grad norm: 0.2795112133026123\n",
            "fc.bias grad norm: 0.025887850672006607\n",
            "[Batch 2000] Loss: 0.0299\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.035648051649332047\n",
            "c0 grad norm: 0.032919496297836304\n",
            "conv1.weight grad norm: 0.4382937550544739\n",
            "conv1.bias grad norm: 0.09231463819742203\n",
            "norm.weight grad norm: 0.04340524971485138\n",
            "norm.bias grad norm: 0.041753098368644714\n",
            "lstm.weight_ih_l0 grad norm: 0.2292977124452591\n",
            "lstm.weight_hh_l0 grad norm: 0.05861232057213783\n",
            "lstm.bias_ih_l0 grad norm: 0.021291350945830345\n",
            "lstm.bias_hh_l0 grad norm: 0.021291350945830345\n",
            "fc.weight grad norm: 0.2138897031545639\n",
            "fc.bias grad norm: 0.07042450457811356\n",
            "[Batch 3000] Loss: 0.0225\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.03739398717880249\n",
            "c0 grad norm: 0.043596819043159485\n",
            "conv1.weight grad norm: 0.44027239084243774\n",
            "conv1.bias grad norm: 0.10392989218235016\n",
            "norm.weight grad norm: 0.06162180379033089\n",
            "norm.bias grad norm: 0.0671955943107605\n",
            "lstm.weight_ih_l0 grad norm: 0.3109826445579529\n",
            "lstm.weight_hh_l0 grad norm: 0.09962642937898636\n",
            "lstm.bias_ih_l0 grad norm: 0.03048967383801937\n",
            "lstm.bias_hh_l0 grad norm: 0.03048967383801937\n",
            "fc.weight grad norm: 0.5312201976776123\n",
            "fc.bias grad norm: 0.12180487811565399\n",
            "[Batch 4000] Loss: 0.0887\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.021556967869400978\n",
            "c0 grad norm: 0.02056872472167015\n",
            "conv1.weight grad norm: 0.19916030764579773\n",
            "conv1.bias grad norm: 0.08056972175836563\n",
            "norm.weight grad norm: 0.025926949456334114\n",
            "norm.bias grad norm: 0.044924747198820114\n",
            "lstm.weight_ih_l0 grad norm: 0.1546049863100052\n",
            "lstm.weight_hh_l0 grad norm: 0.03549906983971596\n",
            "lstm.bias_ih_l0 grad norm: 0.014863716438412666\n",
            "lstm.bias_hh_l0 grad norm: 0.014863716438412666\n",
            "fc.weight grad norm: 0.2814819812774658\n",
            "fc.bias grad norm: 0.08983149379491806\n",
            "[Batch 5000] Loss: 0.0191\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.015275192447006702\n",
            "c0 grad norm: 0.018547996878623962\n",
            "conv1.weight grad norm: 0.16868816316127777\n",
            "conv1.bias grad norm: 0.030828922986984253\n",
            "norm.weight grad norm: 0.024028517305850983\n",
            "norm.bias grad norm: 0.022956199944019318\n",
            "lstm.weight_ih_l0 grad norm: 0.1143348217010498\n",
            "lstm.weight_hh_l0 grad norm: 0.032913688570261\n",
            "lstm.bias_ih_l0 grad norm: 0.011717011220753193\n",
            "lstm.bias_hh_l0 grad norm: 0.011717011220753193\n",
            "fc.weight grad norm: 0.13456672430038452\n",
            "fc.bias grad norm: 0.048070553690195084\n",
            "[Batch 6000] Loss: 0.0054\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.037327468395233154\n",
            "c0 grad norm: 0.04165726155042648\n",
            "conv1.weight grad norm: 0.973824679851532\n",
            "conv1.bias grad norm: 0.1848248690366745\n",
            "norm.weight grad norm: 0.08701436221599579\n",
            "norm.bias grad norm: 0.10663426667451859\n",
            "lstm.weight_ih_l0 grad norm: 0.44224438071250916\n",
            "lstm.weight_hh_l0 grad norm: 0.13025051355361938\n",
            "lstm.bias_ih_l0 grad norm: 0.04263047128915787\n",
            "lstm.bias_hh_l0 grad norm: 0.04263047128915787\n",
            "fc.weight grad norm: 0.6865240335464478\n",
            "fc.bias grad norm: 0.19730305671691895\n",
            "[Batch 7000] Loss: 0.1754\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.04601002857089043\n",
            "c0 grad norm: 0.07172565162181854\n",
            "conv1.weight grad norm: 0.5119272470474243\n",
            "conv1.bias grad norm: 0.13589037954807281\n",
            "norm.weight grad norm: 0.0802346020936966\n",
            "norm.bias grad norm: 0.07720708101987839\n",
            "lstm.weight_ih_l0 grad norm: 0.45414838194847107\n",
            "lstm.weight_hh_l0 grad norm: 0.12348516285419464\n",
            "lstm.bias_ih_l0 grad norm: 0.0433795303106308\n",
            "lstm.bias_hh_l0 grad norm: 0.0433795303106308\n",
            "fc.weight grad norm: 0.6285153031349182\n",
            "fc.bias grad norm: 0.15740275382995605\n",
            "[Batch 8000] Loss: 0.2344\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.35230737924575806\n",
            "c0 grad norm: 0.3101261556148529\n",
            "conv1.weight grad norm: 5.16349458694458\n",
            "conv1.bias grad norm: 1.2635838985443115\n",
            "norm.weight grad norm: 0.3965222239494324\n",
            "norm.bias grad norm: 0.526950478553772\n",
            "lstm.weight_ih_l0 grad norm: 2.148374557495117\n",
            "lstm.weight_hh_l0 grad norm: 0.3595017194747925\n",
            "lstm.bias_ih_l0 grad norm: 0.21162454783916473\n",
            "lstm.bias_hh_l0 grad norm: 0.21162454783916473\n",
            "fc.weight grad norm: 0.8447858095169067\n",
            "fc.bias grad norm: 0.24937690794467926\n",
            "[Batch 9000] Loss: 0.3396\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 81\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.014605028554797173\n",
            "c0 grad norm: 0.03700968250632286\n",
            "conv1.weight grad norm: 0.18476076424121857\n",
            "conv1.bias grad norm: 0.0313766710460186\n",
            "norm.weight grad norm: 0.028122812509536743\n",
            "norm.bias grad norm: 0.027248729020357132\n",
            "lstm.weight_ih_l0 grad norm: 0.16914401948451996\n",
            "lstm.weight_hh_l0 grad norm: 0.053512658923864365\n",
            "lstm.bias_ih_l0 grad norm: 0.016256505623459816\n",
            "lstm.bias_hh_l0 grad norm: 0.016256505623459816\n",
            "fc.weight grad norm: 0.2250189632177353\n",
            "fc.bias grad norm: 0.03898460045456886\n",
            "[Batch 0] Loss: 0.0201\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.037313371896743774\n",
            "c0 grad norm: 0.030362382531166077\n",
            "conv1.weight grad norm: 0.24915708601474762\n",
            "conv1.bias grad norm: 0.042399194091558456\n",
            "norm.weight grad norm: 0.05015810579061508\n",
            "norm.bias grad norm: 0.037423912435770035\n",
            "lstm.weight_ih_l0 grad norm: 0.24588072299957275\n",
            "lstm.weight_hh_l0 grad norm: 0.05068671703338623\n",
            "lstm.bias_ih_l0 grad norm: 0.0236829686909914\n",
            "lstm.bias_hh_l0 grad norm: 0.0236829686909914\n",
            "fc.weight grad norm: 0.2978817820549011\n",
            "fc.bias grad norm: 0.04349808022379875\n",
            "[Batch 1000] Loss: 0.0327\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.020778018981218338\n",
            "c0 grad norm: 0.038006741553545\n",
            "conv1.weight grad norm: 0.18594229221343994\n",
            "conv1.bias grad norm: 0.041701871901750565\n",
            "norm.weight grad norm: 0.032142408192157745\n",
            "norm.bias grad norm: 0.049957409501075745\n",
            "lstm.weight_ih_l0 grad norm: 0.2158442884683609\n",
            "lstm.weight_hh_l0 grad norm: 0.05927915871143341\n",
            "lstm.bias_ih_l0 grad norm: 0.020458804443478584\n",
            "lstm.bias_hh_l0 grad norm: 0.020458804443478584\n",
            "fc.weight grad norm: 0.3139428496360779\n",
            "fc.bias grad norm: 0.1465890258550644\n",
            "[Batch 2000] Loss: 0.0326\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.024606777355074883\n",
            "c0 grad norm: 0.03937676548957825\n",
            "conv1.weight grad norm: 0.4690926671028137\n",
            "conv1.bias grad norm: 0.0984550192952156\n",
            "norm.weight grad norm: 0.048534780740737915\n",
            "norm.bias grad norm: 0.047551028430461884\n",
            "lstm.weight_ih_l0 grad norm: 0.2724134624004364\n",
            "lstm.weight_hh_l0 grad norm: 0.08330643177032471\n",
            "lstm.bias_ih_l0 grad norm: 0.026611438021063805\n",
            "lstm.bias_hh_l0 grad norm: 0.026611438021063805\n",
            "fc.weight grad norm: 0.33584365248680115\n",
            "fc.bias grad norm: 0.12508264183998108\n",
            "[Batch 3000] Loss: 0.0415\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.019025947898626328\n",
            "c0 grad norm: 0.01881442219018936\n",
            "conv1.weight grad norm: 0.1731162667274475\n",
            "conv1.bias grad norm: 0.03718627989292145\n",
            "norm.weight grad norm: 0.029040005058050156\n",
            "norm.bias grad norm: 0.03102719597518444\n",
            "lstm.weight_ih_l0 grad norm: 0.1302473545074463\n",
            "lstm.weight_hh_l0 grad norm: 0.031095566228032112\n",
            "lstm.bias_ih_l0 grad norm: 0.012046361342072487\n",
            "lstm.bias_hh_l0 grad norm: 0.012046361342072487\n",
            "fc.weight grad norm: 0.21983687579631805\n",
            "fc.bias grad norm: 0.0579153373837471\n",
            "[Batch 4000] Loss: 0.0128\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.020790545269846916\n",
            "c0 grad norm: 0.06352468580007553\n",
            "conv1.weight grad norm: 0.24700278043746948\n",
            "conv1.bias grad norm: 0.052493512630462646\n",
            "norm.weight grad norm: 0.03175932914018631\n",
            "norm.bias grad norm: 0.03724273294210434\n",
            "lstm.weight_ih_l0 grad norm: 0.19491492211818695\n",
            "lstm.weight_hh_l0 grad norm: 0.05332256853580475\n",
            "lstm.bias_ih_l0 grad norm: 0.018530629575252533\n",
            "lstm.bias_hh_l0 grad norm: 0.018530629575252533\n",
            "fc.weight grad norm: 0.38707292079925537\n",
            "fc.bias grad norm: 0.10021981596946716\n",
            "[Batch 5000] Loss: 0.0683\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.09506139904260635\n",
            "c0 grad norm: 0.12849873304367065\n",
            "conv1.weight grad norm: 1.1275964975357056\n",
            "conv1.bias grad norm: 0.19499683380126953\n",
            "norm.weight grad norm: 0.14586572349071503\n",
            "norm.bias grad norm: 0.1571044623851776\n",
            "lstm.weight_ih_l0 grad norm: 0.8655471205711365\n",
            "lstm.weight_hh_l0 grad norm: 0.27762871980667114\n",
            "lstm.bias_ih_l0 grad norm: 0.0850709080696106\n",
            "lstm.bias_hh_l0 grad norm: 0.0850709080696106\n",
            "fc.weight grad norm: 1.0003477334976196\n",
            "fc.bias grad norm: 0.310380756855011\n",
            "[Batch 6000] Loss: 0.3781\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.029026025906205177\n",
            "c0 grad norm: 0.07724809646606445\n",
            "conv1.weight grad norm: 0.36634647846221924\n",
            "conv1.bias grad norm: 0.09776293486356735\n",
            "norm.weight grad norm: 0.053819481283426285\n",
            "norm.bias grad norm: 0.05767908692359924\n",
            "lstm.weight_ih_l0 grad norm: 0.26016169786453247\n",
            "lstm.weight_hh_l0 grad norm: 0.06768354028463364\n",
            "lstm.bias_ih_l0 grad norm: 0.023685896769165993\n",
            "lstm.bias_hh_l0 grad norm: 0.023685896769165993\n",
            "fc.weight grad norm: 0.3795764446258545\n",
            "fc.bias grad norm: 0.08885905146598816\n",
            "[Batch 7000] Loss: 0.0822\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.024190383031964302\n",
            "c0 grad norm: 0.010895965620875359\n",
            "conv1.weight grad norm: 0.1613728106021881\n",
            "conv1.bias grad norm: 0.03738010674715042\n",
            "norm.weight grad norm: 0.026041703298687935\n",
            "norm.bias grad norm: 0.030971482396125793\n",
            "lstm.weight_ih_l0 grad norm: 0.16877520084381104\n",
            "lstm.weight_hh_l0 grad norm: 0.03732334449887276\n",
            "lstm.bias_ih_l0 grad norm: 0.015960494056344032\n",
            "lstm.bias_hh_l0 grad norm: 0.015960494056344032\n",
            "fc.weight grad norm: 0.1833551526069641\n",
            "fc.bias grad norm: 0.074845090508461\n",
            "[Batch 8000] Loss: 0.0211\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.053474895656108856\n",
            "c0 grad norm: 0.06902849674224854\n",
            "conv1.weight grad norm: 0.7649796009063721\n",
            "conv1.bias grad norm: 0.1440034955739975\n",
            "norm.weight grad norm: 0.09292440861463547\n",
            "norm.bias grad norm: 0.12031740695238113\n",
            "lstm.weight_ih_l0 grad norm: 0.3377944231033325\n",
            "lstm.weight_hh_l0 grad norm: 0.07058335095643997\n",
            "lstm.bias_ih_l0 grad norm: 0.032222773879766464\n",
            "lstm.bias_hh_l0 grad norm: 0.032222773879766464\n",
            "fc.weight grad norm: 0.374187171459198\n",
            "fc.bias grad norm: 0.1345681995153427\n",
            "[Batch 9000] Loss: 0.0763\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 82\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.04191594198346138\n",
            "c0 grad norm: 0.09219033271074295\n",
            "conv1.weight grad norm: 0.6386619806289673\n",
            "conv1.bias grad norm: 0.08754661679267883\n",
            "norm.weight grad norm: 0.09529068320989609\n",
            "norm.bias grad norm: 0.08379949629306793\n",
            "lstm.weight_ih_l0 grad norm: 0.4618504047393799\n",
            "lstm.weight_hh_l0 grad norm: 0.1345488429069519\n",
            "lstm.bias_ih_l0 grad norm: 0.04181232675909996\n",
            "lstm.bias_hh_l0 grad norm: 0.04181232675909996\n",
            "fc.weight grad norm: 0.34817859530448914\n",
            "fc.bias grad norm: 0.10548447072505951\n",
            "[Batch 0] Loss: 0.0601\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.021384991705417633\n",
            "c0 grad norm: 0.02635810524225235\n",
            "conv1.weight grad norm: 0.22358892858028412\n",
            "conv1.bias grad norm: 0.04365973547101021\n",
            "norm.weight grad norm: 0.03993649408221245\n",
            "norm.bias grad norm: 0.03164011612534523\n",
            "lstm.weight_ih_l0 grad norm: 0.15032576024532318\n",
            "lstm.weight_hh_l0 grad norm: 0.03267944976687431\n",
            "lstm.bias_ih_l0 grad norm: 0.013405981473624706\n",
            "lstm.bias_hh_l0 grad norm: 0.013405981473624706\n",
            "fc.weight grad norm: 0.23805645108222961\n",
            "fc.bias grad norm: 0.05394134670495987\n",
            "[Batch 1000] Loss: 0.0242\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.0629582479596138\n",
            "c0 grad norm: 0.06785104423761368\n",
            "conv1.weight grad norm: 1.196027159690857\n",
            "conv1.bias grad norm: 0.16194221377372742\n",
            "norm.weight grad norm: 0.15652364492416382\n",
            "norm.bias grad norm: 0.1659950464963913\n",
            "lstm.weight_ih_l0 grad norm: 0.61665940284729\n",
            "lstm.weight_hh_l0 grad norm: 0.16457846760749817\n",
            "lstm.bias_ih_l0 grad norm: 0.06061405688524246\n",
            "lstm.bias_hh_l0 grad norm: 0.06061405688524246\n",
            "fc.weight grad norm: 0.5435770153999329\n",
            "fc.bias grad norm: 0.1728266179561615\n",
            "[Batch 2000] Loss: 0.1641\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.05519300699234009\n",
            "c0 grad norm: 0.09866060316562653\n",
            "conv1.weight grad norm: 0.9781625866889954\n",
            "conv1.bias grad norm: 0.2956601679325104\n",
            "norm.weight grad norm: 0.07939637452363968\n",
            "norm.bias grad norm: 0.09458616375923157\n",
            "lstm.weight_ih_l0 grad norm: 0.37978610396385193\n",
            "lstm.weight_hh_l0 grad norm: 0.09965765476226807\n",
            "lstm.bias_ih_l0 grad norm: 0.03727786988019943\n",
            "lstm.bias_hh_l0 grad norm: 0.03727786988019943\n",
            "fc.weight grad norm: 0.4058597683906555\n",
            "fc.bias grad norm: 0.06030035763978958\n",
            "[Batch 3000] Loss: 0.0698\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.1846211850643158\n",
            "c0 grad norm: 0.19773028790950775\n",
            "conv1.weight grad norm: 2.329003095626831\n",
            "conv1.bias grad norm: 0.7294551730155945\n",
            "norm.weight grad norm: 0.3791002333164215\n",
            "norm.bias grad norm: 0.4491233825683594\n",
            "lstm.weight_ih_l0 grad norm: 2.120115280151367\n",
            "lstm.weight_hh_l0 grad norm: 0.5372154116630554\n",
            "lstm.bias_ih_l0 grad norm: 0.20907260477542877\n",
            "lstm.bias_hh_l0 grad norm: 0.20907260477542877\n",
            "fc.weight grad norm: 0.9046477675437927\n",
            "fc.bias grad norm: 0.21165134012699127\n",
            "[Batch 4000] Loss: 0.3588\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.04922299087047577\n",
            "c0 grad norm: 0.0486079677939415\n",
            "conv1.weight grad norm: 0.38746774196624756\n",
            "conv1.bias grad norm: 0.06966648995876312\n",
            "norm.weight grad norm: 0.055762629956007004\n",
            "norm.bias grad norm: 0.05726189166307449\n",
            "lstm.weight_ih_l0 grad norm: 0.3656812012195587\n",
            "lstm.weight_hh_l0 grad norm: 0.1141204684972763\n",
            "lstm.bias_ih_l0 grad norm: 0.03470275551080704\n",
            "lstm.bias_hh_l0 grad norm: 0.03470275551080704\n",
            "fc.weight grad norm: 0.5062258839607239\n",
            "fc.bias grad norm: 0.07445526123046875\n",
            "[Batch 5000] Loss: 0.1039\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.02146305702626705\n",
            "c0 grad norm: 0.02351098507642746\n",
            "conv1.weight grad norm: 0.20478911697864532\n",
            "conv1.bias grad norm: 0.05305534973740578\n",
            "norm.weight grad norm: 0.023714693263173103\n",
            "norm.bias grad norm: 0.02669254131615162\n",
            "lstm.weight_ih_l0 grad norm: 0.13898685574531555\n",
            "lstm.weight_hh_l0 grad norm: 0.025583704933524132\n",
            "lstm.bias_ih_l0 grad norm: 0.013355044648051262\n",
            "lstm.bias_hh_l0 grad norm: 0.013355044648051262\n",
            "fc.weight grad norm: 0.15956392884254456\n",
            "fc.bias grad norm: 0.05654904618859291\n",
            "[Batch 6000] Loss: 0.0080\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.020109249278903008\n",
            "c0 grad norm: 0.047173164784908295\n",
            "conv1.weight grad norm: 0.2757757902145386\n",
            "conv1.bias grad norm: 0.05399281904101372\n",
            "norm.weight grad norm: 0.030762631446123123\n",
            "norm.bias grad norm: 0.03529646247625351\n",
            "lstm.weight_ih_l0 grad norm: 0.15267841517925262\n",
            "lstm.weight_hh_l0 grad norm: 0.0450630784034729\n",
            "lstm.bias_ih_l0 grad norm: 0.014697527512907982\n",
            "lstm.bias_hh_l0 grad norm: 0.014697527512907982\n",
            "fc.weight grad norm: 0.2714279592037201\n",
            "fc.bias grad norm: 0.11138872057199478\n",
            "[Batch 7000] Loss: 0.0355\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.01590191386640072\n",
            "c0 grad norm: 0.026543347164988518\n",
            "conv1.weight grad norm: 0.1436232030391693\n",
            "conv1.bias grad norm: 0.041746895760297775\n",
            "norm.weight grad norm: 0.02407986856997013\n",
            "norm.bias grad norm: 0.02916906587779522\n",
            "lstm.weight_ih_l0 grad norm: 0.13781069219112396\n",
            "lstm.weight_hh_l0 grad norm: 0.03331035375595093\n",
            "lstm.bias_ih_l0 grad norm: 0.013375747948884964\n",
            "lstm.bias_hh_l0 grad norm: 0.013375747948884964\n",
            "fc.weight grad norm: 0.4249526262283325\n",
            "fc.bias grad norm: 0.128037691116333\n",
            "[Batch 8000] Loss: 0.0474\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.04050029441714287\n",
            "c0 grad norm: 0.07022744417190552\n",
            "conv1.weight grad norm: 0.6113057732582092\n",
            "conv1.bias grad norm: 0.16373884677886963\n",
            "norm.weight grad norm: 0.07039494067430496\n",
            "norm.bias grad norm: 0.08660035580396652\n",
            "lstm.weight_ih_l0 grad norm: 0.38319429755210876\n",
            "lstm.weight_hh_l0 grad norm: 0.10775912553071976\n",
            "lstm.bias_ih_l0 grad norm: 0.0381365530192852\n",
            "lstm.bias_hh_l0 grad norm: 0.0381365530192852\n",
            "fc.weight grad norm: 0.491600900888443\n",
            "fc.bias grad norm: 0.14543001353740692\n",
            "[Batch 9000] Loss: 0.0624\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 83\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.009271235205233097\n",
            "c0 grad norm: 0.0123208062723279\n",
            "conv1.weight grad norm: 0.08859528601169586\n",
            "conv1.bias grad norm: 0.017870670184493065\n",
            "norm.weight grad norm: 0.015707995742559433\n",
            "norm.bias grad norm: 0.013133806176483631\n",
            "lstm.weight_ih_l0 grad norm: 0.08925168216228485\n",
            "lstm.weight_hh_l0 grad norm: 0.025339724496006966\n",
            "lstm.bias_ih_l0 grad norm: 0.008226330392062664\n",
            "lstm.bias_hh_l0 grad norm: 0.008226330392062664\n",
            "fc.weight grad norm: 0.314287006855011\n",
            "fc.bias grad norm: 0.11230839788913727\n",
            "[Batch 0] Loss: 0.0306\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.04534176364541054\n",
            "c0 grad norm: 0.04269440099596977\n",
            "conv1.weight grad norm: 0.5030965209007263\n",
            "conv1.bias grad norm: 0.07078210264444351\n",
            "norm.weight grad norm: 0.08272150158882141\n",
            "norm.bias grad norm: 0.07170165330171585\n",
            "lstm.weight_ih_l0 grad norm: 0.33430659770965576\n",
            "lstm.weight_hh_l0 grad norm: 0.07224216312170029\n",
            "lstm.bias_ih_l0 grad norm: 0.031583867967128754\n",
            "lstm.bias_hh_l0 grad norm: 0.031583867967128754\n",
            "fc.weight grad norm: 0.21865420043468475\n",
            "fc.bias grad norm: 0.05219884589314461\n",
            "[Batch 1000] Loss: 0.0359\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.15418612957000732\n",
            "c0 grad norm: 0.17443211376667023\n",
            "conv1.weight grad norm: 2.7320563793182373\n",
            "conv1.bias grad norm: 0.27597078680992126\n",
            "norm.weight grad norm: 0.23120826482772827\n",
            "norm.bias grad norm: 0.21588344871997833\n",
            "lstm.weight_ih_l0 grad norm: 1.1946066617965698\n",
            "lstm.weight_hh_l0 grad norm: 0.35351404547691345\n",
            "lstm.bias_ih_l0 grad norm: 0.11703026294708252\n",
            "lstm.bias_hh_l0 grad norm: 0.11703026294708252\n",
            "fc.weight grad norm: 1.137471079826355\n",
            "fc.bias grad norm: 0.3812212347984314\n",
            "[Batch 2000] Loss: 0.4930\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.010670322924852371\n",
            "c0 grad norm: 0.01697959378361702\n",
            "conv1.weight grad norm: 0.12721428275108337\n",
            "conv1.bias grad norm: 0.02620721608400345\n",
            "norm.weight grad norm: 0.017682185396552086\n",
            "norm.bias grad norm: 0.01977636106312275\n",
            "lstm.weight_ih_l0 grad norm: 0.10465063154697418\n",
            "lstm.weight_hh_l0 grad norm: 0.021878238767385483\n",
            "lstm.bias_ih_l0 grad norm: 0.010203764773905277\n",
            "lstm.bias_hh_l0 grad norm: 0.010203764773905277\n",
            "fc.weight grad norm: 0.19993893802165985\n",
            "fc.bias grad norm: 0.08297933638095856\n",
            "[Batch 3000] Loss: 0.0136\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.07158012688159943\n",
            "c0 grad norm: 0.06665250658988953\n",
            "conv1.weight grad norm: 0.7674987316131592\n",
            "conv1.bias grad norm: 0.15674878656864166\n",
            "norm.weight grad norm: 0.10865215212106705\n",
            "norm.bias grad norm: 0.14223629236221313\n",
            "lstm.weight_ih_l0 grad norm: 0.4980587661266327\n",
            "lstm.weight_hh_l0 grad norm: 0.11673285067081451\n",
            "lstm.bias_ih_l0 grad norm: 0.04825381189584732\n",
            "lstm.bias_hh_l0 grad norm: 0.04825381189584732\n",
            "fc.weight grad norm: 0.6203826069831848\n",
            "fc.bias grad norm: 0.14527083933353424\n",
            "[Batch 4000] Loss: 0.1151\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.025761358439922333\n",
            "c0 grad norm: 0.033261626958847046\n",
            "conv1.weight grad norm: 0.3400976359844208\n",
            "conv1.bias grad norm: 0.06317245960235596\n",
            "norm.weight grad norm: 0.05787692964076996\n",
            "norm.bias grad norm: 0.06006526201963425\n",
            "lstm.weight_ih_l0 grad norm: 0.29643359780311584\n",
            "lstm.weight_hh_l0 grad norm: 0.09251994639635086\n",
            "lstm.bias_ih_l0 grad norm: 0.03073335438966751\n",
            "lstm.bias_hh_l0 grad norm: 0.03073335438966751\n",
            "fc.weight grad norm: 0.3254877030849457\n",
            "fc.bias grad norm: 0.09761157631874084\n",
            "[Batch 5000] Loss: 0.0342\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.07601267844438553\n",
            "c0 grad norm: 0.07707030326128006\n",
            "conv1.weight grad norm: 0.6735122203826904\n",
            "conv1.bias grad norm: 0.12512429058551788\n",
            "norm.weight grad norm: 0.08789058029651642\n",
            "norm.bias grad norm: 0.08106261491775513\n",
            "lstm.weight_ih_l0 grad norm: 0.5977272987365723\n",
            "lstm.weight_hh_l0 grad norm: 0.16147415339946747\n",
            "lstm.bias_ih_l0 grad norm: 0.05852839723229408\n",
            "lstm.bias_hh_l0 grad norm: 0.05852839723229408\n",
            "fc.weight grad norm: 0.31490930914878845\n",
            "fc.bias grad norm: 0.018959734588861465\n",
            "[Batch 6000] Loss: 0.1485\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.20773157477378845\n",
            "c0 grad norm: 0.21938718855381012\n",
            "conv1.weight grad norm: 2.1330323219299316\n",
            "conv1.bias grad norm: 0.49116742610931396\n",
            "norm.weight grad norm: 0.16741932928562164\n",
            "norm.bias grad norm: 0.21261025965213776\n",
            "lstm.weight_ih_l0 grad norm: 1.0032751560211182\n",
            "lstm.weight_hh_l0 grad norm: 0.22125713527202606\n",
            "lstm.bias_ih_l0 grad norm: 0.10084930062294006\n",
            "lstm.bias_hh_l0 grad norm: 0.10084930062294006\n",
            "fc.weight grad norm: 0.9446007013320923\n",
            "fc.bias grad norm: 0.4141707420349121\n",
            "[Batch 7000] Loss: 0.2945\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.11614950746297836\n",
            "c0 grad norm: 0.06282836198806763\n",
            "conv1.weight grad norm: 1.1081489324569702\n",
            "conv1.bias grad norm: 0.3025408089160919\n",
            "norm.weight grad norm: 0.14480142295360565\n",
            "norm.bias grad norm: 0.17631837725639343\n",
            "lstm.weight_ih_l0 grad norm: 0.7683541178703308\n",
            "lstm.weight_hh_l0 grad norm: 0.21554720401763916\n",
            "lstm.bias_ih_l0 grad norm: 0.07371343672275543\n",
            "lstm.bias_hh_l0 grad norm: 0.07371343672275543\n",
            "fc.weight grad norm: 0.4186415374279022\n",
            "fc.bias grad norm: 0.20452781021595\n",
            "[Batch 8000] Loss: 0.0637\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.03210296854376793\n",
            "c0 grad norm: 0.06337609142065048\n",
            "conv1.weight grad norm: 0.6375339031219482\n",
            "conv1.bias grad norm: 0.12924852967262268\n",
            "norm.weight grad norm: 0.06587636470794678\n",
            "norm.bias grad norm: 0.08240070939064026\n",
            "lstm.weight_ih_l0 grad norm: 0.3219280242919922\n",
            "lstm.weight_hh_l0 grad norm: 0.0908573716878891\n",
            "lstm.bias_ih_l0 grad norm: 0.030325232073664665\n",
            "lstm.bias_hh_l0 grad norm: 0.030325232073664665\n",
            "fc.weight grad norm: 0.4308146834373474\n",
            "fc.bias grad norm: 0.1519477665424347\n",
            "[Batch 9000] Loss: 0.1315\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 84\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.0711413100361824\n",
            "c0 grad norm: 0.11490290611982346\n",
            "conv1.weight grad norm: 1.2406947612762451\n",
            "conv1.bias grad norm: 0.19739145040512085\n",
            "norm.weight grad norm: 0.17822223901748657\n",
            "norm.bias grad norm: 0.19705848395824432\n",
            "lstm.weight_ih_l0 grad norm: 0.9863250255584717\n",
            "lstm.weight_hh_l0 grad norm: 0.3215333819389343\n",
            "lstm.bias_ih_l0 grad norm: 0.09604810923337936\n",
            "lstm.bias_hh_l0 grad norm: 0.09604810923337936\n",
            "fc.weight grad norm: 1.3859004974365234\n",
            "fc.bias grad norm: 0.329348623752594\n",
            "[Batch 0] Loss: 0.4001\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.01872938685119152\n",
            "c0 grad norm: 0.07158903777599335\n",
            "conv1.weight grad norm: 0.2471240609884262\n",
            "conv1.bias grad norm: 0.05656054988503456\n",
            "norm.weight grad norm: 0.03957837447524071\n",
            "norm.bias grad norm: 0.04200347885489464\n",
            "lstm.weight_ih_l0 grad norm: 0.2203260213136673\n",
            "lstm.weight_hh_l0 grad norm: 0.061713144183158875\n",
            "lstm.bias_ih_l0 grad norm: 0.02081213891506195\n",
            "lstm.bias_hh_l0 grad norm: 0.02081213891506195\n",
            "fc.weight grad norm: 0.47232282161712646\n",
            "fc.bias grad norm: 0.10928783565759659\n",
            "[Batch 1000] Loss: 0.0896\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.008123589679598808\n",
            "c0 grad norm: 0.019436802715063095\n",
            "conv1.weight grad norm: 0.11419261991977692\n",
            "conv1.bias grad norm: 0.02739138714969158\n",
            "norm.weight grad norm: 0.018646880984306335\n",
            "norm.bias grad norm: 0.02067418210208416\n",
            "lstm.weight_ih_l0 grad norm: 0.09001918137073517\n",
            "lstm.weight_hh_l0 grad norm: 0.029280994087457657\n",
            "lstm.bias_ih_l0 grad norm: 0.008545990101993084\n",
            "lstm.bias_hh_l0 grad norm: 0.008545990101993084\n",
            "fc.weight grad norm: 0.1266767829656601\n",
            "fc.bias grad norm: 0.024743394926190376\n",
            "[Batch 2000] Loss: 0.0075\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.03093714453279972\n",
            "c0 grad norm: 0.025813592597842216\n",
            "conv1.weight grad norm: 0.2929838299751282\n",
            "conv1.bias grad norm: 0.07241871953010559\n",
            "norm.weight grad norm: 0.032706357538700104\n",
            "norm.bias grad norm: 0.038844142109155655\n",
            "lstm.weight_ih_l0 grad norm: 0.1744166910648346\n",
            "lstm.weight_hh_l0 grad norm: 0.037143778055906296\n",
            "lstm.bias_ih_l0 grad norm: 0.016309529542922974\n",
            "lstm.bias_hh_l0 grad norm: 0.016309529542922974\n",
            "fc.weight grad norm: 0.16950957477092743\n",
            "fc.bias grad norm: 0.0401112399995327\n",
            "[Batch 3000] Loss: 0.0185\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.023144911974668503\n",
            "c0 grad norm: 0.03881610557436943\n",
            "conv1.weight grad norm: 0.1865251511335373\n",
            "conv1.bias grad norm: 0.03287120908498764\n",
            "norm.weight grad norm: 0.02567434124648571\n",
            "norm.bias grad norm: 0.022204874083399773\n",
            "lstm.weight_ih_l0 grad norm: 0.16845710575580597\n",
            "lstm.weight_hh_l0 grad norm: 0.04080105572938919\n",
            "lstm.bias_ih_l0 grad norm: 0.015713920816779137\n",
            "lstm.bias_hh_l0 grad norm: 0.015713920816779137\n",
            "fc.weight grad norm: 0.2324182391166687\n",
            "fc.bias grad norm: 0.100954070687294\n",
            "[Batch 4000] Loss: 0.0226\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.016508445143699646\n",
            "c0 grad norm: 0.028855759650468826\n",
            "conv1.weight grad norm: 0.25742486119270325\n",
            "conv1.bias grad norm: 0.04182519391179085\n",
            "norm.weight grad norm: 0.03341084346175194\n",
            "norm.bias grad norm: 0.03410327062010765\n",
            "lstm.weight_ih_l0 grad norm: 0.17900130152702332\n",
            "lstm.weight_hh_l0 grad norm: 0.046610794961452484\n",
            "lstm.bias_ih_l0 grad norm: 0.01780903898179531\n",
            "lstm.bias_hh_l0 grad norm: 0.01780903898179531\n",
            "fc.weight grad norm: 0.2224249243736267\n",
            "fc.bias grad norm: 0.04039794206619263\n",
            "[Batch 5000] Loss: 0.0394\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.045896533876657486\n",
            "c0 grad norm: 0.04342050850391388\n",
            "conv1.weight grad norm: 0.4048326909542084\n",
            "conv1.bias grad norm: 0.10567616671323776\n",
            "norm.weight grad norm: 0.055575545877218246\n",
            "norm.bias grad norm: 0.07084625214338303\n",
            "lstm.weight_ih_l0 grad norm: 0.3344067931175232\n",
            "lstm.weight_hh_l0 grad norm: 0.0809013769030571\n",
            "lstm.bias_ih_l0 grad norm: 0.03423374891281128\n",
            "lstm.bias_hh_l0 grad norm: 0.03423374891281128\n",
            "fc.weight grad norm: 0.37906184792518616\n",
            "fc.bias grad norm: 0.1284920573234558\n",
            "[Batch 6000] Loss: 0.0271\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.015656575560569763\n",
            "c0 grad norm: 0.048297982662916183\n",
            "conv1.weight grad norm: 0.2295919954776764\n",
            "conv1.bias grad norm: 0.048050325363874435\n",
            "norm.weight grad norm: 0.029499923810362816\n",
            "norm.bias grad norm: 0.032649002969264984\n",
            "lstm.weight_ih_l0 grad norm: 0.22697888314723969\n",
            "lstm.weight_hh_l0 grad norm: 0.07016339153051376\n",
            "lstm.bias_ih_l0 grad norm: 0.02176065556704998\n",
            "lstm.bias_hh_l0 grad norm: 0.02176065556704998\n",
            "fc.weight grad norm: 0.3668791651725769\n",
            "fc.bias grad norm: 0.07017510384321213\n",
            "[Batch 7000] Loss: 0.0590\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.017191853374242783\n",
            "c0 grad norm: 0.02232687920331955\n",
            "conv1.weight grad norm: 0.2681979238986969\n",
            "conv1.bias grad norm: 0.04797236993908882\n",
            "norm.weight grad norm: 0.03575710207223892\n",
            "norm.bias grad norm: 0.034362174570560455\n",
            "lstm.weight_ih_l0 grad norm: 0.15311916172504425\n",
            "lstm.weight_hh_l0 grad norm: 0.04184537008404732\n",
            "lstm.bias_ih_l0 grad norm: 0.014449803158640862\n",
            "lstm.bias_hh_l0 grad norm: 0.014449803158640862\n",
            "fc.weight grad norm: 0.27690616250038147\n",
            "fc.bias grad norm: 0.08433795720338821\n",
            "[Batch 8000] Loss: 0.0209\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.018286917358636856\n",
            "c0 grad norm: 0.08235377073287964\n",
            "conv1.weight grad norm: 0.2380460649728775\n",
            "conv1.bias grad norm: 0.0721074715256691\n",
            "norm.weight grad norm: 0.03424059599637985\n",
            "norm.bias grad norm: 0.03515041619539261\n",
            "lstm.weight_ih_l0 grad norm: 0.2340765744447708\n",
            "lstm.weight_hh_l0 grad norm: 0.06420877575874329\n",
            "lstm.bias_ih_l0 grad norm: 0.02182910032570362\n",
            "lstm.bias_hh_l0 grad norm: 0.02182910032570362\n",
            "fc.weight grad norm: 0.2729157507419586\n",
            "fc.bias grad norm: 0.09479764103889465\n",
            "[Batch 9000] Loss: 0.0538\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 85\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.21226495504379272\n",
            "c0 grad norm: 0.2175077199935913\n",
            "conv1.weight grad norm: 4.097736358642578\n",
            "conv1.bias grad norm: 1.0215414762496948\n",
            "norm.weight grad norm: 0.289742648601532\n",
            "norm.bias grad norm: 0.3549496829509735\n",
            "lstm.weight_ih_l0 grad norm: 1.3336591720581055\n",
            "lstm.weight_hh_l0 grad norm: 0.25389474630355835\n",
            "lstm.bias_ih_l0 grad norm: 0.13678185641765594\n",
            "lstm.bias_hh_l0 grad norm: 0.13678185641765594\n",
            "fc.weight grad norm: 0.7081478238105774\n",
            "fc.bias grad norm: 0.2913982570171356\n",
            "[Batch 0] Loss: 0.2278\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.03532707318663597\n",
            "c0 grad norm: 0.08443573862314224\n",
            "conv1.weight grad norm: 0.34150150418281555\n",
            "conv1.bias grad norm: 0.07340835779905319\n",
            "norm.weight grad norm: 0.05957973375916481\n",
            "norm.bias grad norm: 0.0553290918469429\n",
            "lstm.weight_ih_l0 grad norm: 0.26492270827293396\n",
            "lstm.weight_hh_l0 grad norm: 0.0643206238746643\n",
            "lstm.bias_ih_l0 grad norm: 0.023045802488923073\n",
            "lstm.bias_hh_l0 grad norm: 0.023045802488923073\n",
            "fc.weight grad norm: 0.29213911294937134\n",
            "fc.bias grad norm: 0.07151582092046738\n",
            "[Batch 1000] Loss: 0.0543\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.09404530376195908\n",
            "c0 grad norm: 0.07794792950153351\n",
            "conv1.weight grad norm: 1.1212973594665527\n",
            "conv1.bias grad norm: 0.20797865092754364\n",
            "norm.weight grad norm: 0.17203834652900696\n",
            "norm.bias grad norm: 0.17357946932315826\n",
            "lstm.weight_ih_l0 grad norm: 0.6886304020881653\n",
            "lstm.weight_hh_l0 grad norm: 0.14418938755989075\n",
            "lstm.bias_ih_l0 grad norm: 0.06408797204494476\n",
            "lstm.bias_hh_l0 grad norm: 0.06408797204494476\n",
            "fc.weight grad norm: 0.29885709285736084\n",
            "fc.bias grad norm: 0.029693972319364548\n",
            "[Batch 2000] Loss: 0.0941\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.010937445797026157\n",
            "c0 grad norm: 0.026141803711652756\n",
            "conv1.weight grad norm: 0.17101550102233887\n",
            "conv1.bias grad norm: 0.021525977179408073\n",
            "norm.weight grad norm: 0.024450678378343582\n",
            "norm.bias grad norm: 0.019259395077824593\n",
            "lstm.weight_ih_l0 grad norm: 0.11746834963560104\n",
            "lstm.weight_hh_l0 grad norm: 0.034699782729148865\n",
            "lstm.bias_ih_l0 grad norm: 0.011468668468296528\n",
            "lstm.bias_hh_l0 grad norm: 0.011468668468296528\n",
            "fc.weight grad norm: 0.19962310791015625\n",
            "fc.bias grad norm: 0.013999735936522484\n",
            "[Batch 3000] Loss: 0.0160\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.025308383628726006\n",
            "c0 grad norm: 0.04203588142991066\n",
            "conv1.weight grad norm: 0.5517858266830444\n",
            "conv1.bias grad norm: 0.13102957606315613\n",
            "norm.weight grad norm: 0.06316148489713669\n",
            "norm.bias grad norm: 0.04476858675479889\n",
            "lstm.weight_ih_l0 grad norm: 0.27784889936447144\n",
            "lstm.weight_hh_l0 grad norm: 0.09085360169410706\n",
            "lstm.bias_ih_l0 grad norm: 0.025530973449349403\n",
            "lstm.bias_hh_l0 grad norm: 0.025530973449349403\n",
            "fc.weight grad norm: 0.2503151595592499\n",
            "fc.bias grad norm: 0.06382907181978226\n",
            "[Batch 4000] Loss: 0.0262\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.04692557826638222\n",
            "c0 grad norm: 0.0399027094244957\n",
            "conv1.weight grad norm: 0.31555068492889404\n",
            "conv1.bias grad norm: 0.09402011334896088\n",
            "norm.weight grad norm: 0.03783614933490753\n",
            "norm.bias grad norm: 0.044135600328445435\n",
            "lstm.weight_ih_l0 grad norm: 0.2573179006576538\n",
            "lstm.weight_hh_l0 grad norm: 0.04629986733198166\n",
            "lstm.bias_ih_l0 grad norm: 0.025585535913705826\n",
            "lstm.bias_hh_l0 grad norm: 0.025585535913705826\n",
            "fc.weight grad norm: 0.21519321203231812\n",
            "fc.bias grad norm: 0.02372860535979271\n",
            "[Batch 5000] Loss: 0.0153\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.03546014800667763\n",
            "c0 grad norm: 0.051360879093408585\n",
            "conv1.weight grad norm: 0.44165846705436707\n",
            "conv1.bias grad norm: 0.06947213411331177\n",
            "norm.weight grad norm: 0.056525371968746185\n",
            "norm.bias grad norm: 0.06837836652994156\n",
            "lstm.weight_ih_l0 grad norm: 0.3595510423183441\n",
            "lstm.weight_hh_l0 grad norm: 0.12104500085115433\n",
            "lstm.bias_ih_l0 grad norm: 0.03631307929754257\n",
            "lstm.bias_hh_l0 grad norm: 0.03631307929754257\n",
            "fc.weight grad norm: 0.7630444765090942\n",
            "fc.bias grad norm: 0.2424350082874298\n",
            "[Batch 6000] Loss: 0.0845\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.03610604628920555\n",
            "c0 grad norm: 0.023155294358730316\n",
            "conv1.weight grad norm: 0.25928884744644165\n",
            "conv1.bias grad norm: 0.07040686160326004\n",
            "norm.weight grad norm: 0.02661243826150894\n",
            "norm.bias grad norm: 0.03178147226572037\n",
            "lstm.weight_ih_l0 grad norm: 0.179963618516922\n",
            "lstm.weight_hh_l0 grad norm: 0.035736940801143646\n",
            "lstm.bias_ih_l0 grad norm: 0.016228290274739265\n",
            "lstm.bias_hh_l0 grad norm: 0.016228290274739265\n",
            "fc.weight grad norm: 0.17158855497837067\n",
            "fc.bias grad norm: 0.04514056444168091\n",
            "[Batch 7000] Loss: 0.0228\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.017667245119810104\n",
            "c0 grad norm: 0.024323003366589546\n",
            "conv1.weight grad norm: 0.19184979796409607\n",
            "conv1.bias grad norm: 0.050550609827041626\n",
            "norm.weight grad norm: 0.02308468706905842\n",
            "norm.bias grad norm: 0.03436734154820442\n",
            "lstm.weight_ih_l0 grad norm: 0.13407845795154572\n",
            "lstm.weight_hh_l0 grad norm: 0.04056895524263382\n",
            "lstm.bias_ih_l0 grad norm: 0.012979075312614441\n",
            "lstm.bias_hh_l0 grad norm: 0.012979075312614441\n",
            "fc.weight grad norm: 0.19500920176506042\n",
            "fc.bias grad norm: 0.03691757470369339\n",
            "[Batch 8000] Loss: 0.0108\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.004846804775297642\n",
            "c0 grad norm: 0.01736278086900711\n",
            "conv1.weight grad norm: 0.07547225803136826\n",
            "conv1.bias grad norm: 0.016970954835414886\n",
            "norm.weight grad norm: 0.013003755360841751\n",
            "norm.bias grad norm: 0.016055943444371223\n",
            "lstm.weight_ih_l0 grad norm: 0.07446080446243286\n",
            "lstm.weight_hh_l0 grad norm: 0.01943838596343994\n",
            "lstm.bias_ih_l0 grad norm: 0.007127208635210991\n",
            "lstm.bias_hh_l0 grad norm: 0.007127208635210991\n",
            "fc.weight grad norm: 0.16583360731601715\n",
            "fc.bias grad norm: 0.06216371804475784\n",
            "[Batch 9000] Loss: 0.0121\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 86\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03339860215783119\n",
            "c0 grad norm: 0.04432811215519905\n",
            "conv1.weight grad norm: 0.211561381816864\n",
            "conv1.bias grad norm: 0.0649702250957489\n",
            "norm.weight grad norm: 0.03328000381588936\n",
            "norm.bias grad norm: 0.04827618598937988\n",
            "lstm.weight_ih_l0 grad norm: 0.21879500150680542\n",
            "lstm.weight_hh_l0 grad norm: 0.053719084709882736\n",
            "lstm.bias_ih_l0 grad norm: 0.021763412281870842\n",
            "lstm.bias_hh_l0 grad norm: 0.021763412281870842\n",
            "fc.weight grad norm: 0.3046916127204895\n",
            "fc.bias grad norm: 0.11374034732580185\n",
            "[Batch 0] Loss: 0.0470\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.014542370103299618\n",
            "c0 grad norm: 0.012197496369481087\n",
            "conv1.weight grad norm: 0.12689952552318573\n",
            "conv1.bias grad norm: 0.03348454087972641\n",
            "norm.weight grad norm: 0.01749631203711033\n",
            "norm.bias grad norm: 0.02523079887032509\n",
            "lstm.weight_ih_l0 grad norm: 0.09453960508108139\n",
            "lstm.weight_hh_l0 grad norm: 0.024676620960235596\n",
            "lstm.bias_ih_l0 grad norm: 0.008909264579415321\n",
            "lstm.bias_hh_l0 grad norm: 0.008909264579415321\n",
            "fc.weight grad norm: 0.14650395512580872\n",
            "fc.bias grad norm: 0.049830347299575806\n",
            "[Batch 1000] Loss: 0.0114\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.011658133938908577\n",
            "c0 grad norm: 0.013547377660870552\n",
            "conv1.weight grad norm: 0.10151117295026779\n",
            "conv1.bias grad norm: 0.022406654432415962\n",
            "norm.weight grad norm: 0.013931768015027046\n",
            "norm.bias grad norm: 0.01433437131345272\n",
            "lstm.weight_ih_l0 grad norm: 0.09895008057355881\n",
            "lstm.weight_hh_l0 grad norm: 0.030763177201151848\n",
            "lstm.bias_ih_l0 grad norm: 0.009733904153108597\n",
            "lstm.bias_hh_l0 grad norm: 0.009733904153108597\n",
            "fc.weight grad norm: 0.16954609751701355\n",
            "fc.bias grad norm: 0.04389386996626854\n",
            "[Batch 2000] Loss: 0.0105\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.01475592516362667\n",
            "c0 grad norm: 0.03211124613881111\n",
            "conv1.weight grad norm: 0.14590540528297424\n",
            "conv1.bias grad norm: 0.03496962785720825\n",
            "norm.weight grad norm: 0.018516626209020615\n",
            "norm.bias grad norm: 0.026539264246821404\n",
            "lstm.weight_ih_l0 grad norm: 0.14382648468017578\n",
            "lstm.weight_hh_l0 grad norm: 0.03821515664458275\n",
            "lstm.bias_ih_l0 grad norm: 0.014248882420361042\n",
            "lstm.bias_hh_l0 grad norm: 0.014248882420361042\n",
            "fc.weight grad norm: 0.23085585236549377\n",
            "fc.bias grad norm: 0.07372380048036575\n",
            "[Batch 3000] Loss: 0.0208\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.018025770783424377\n",
            "c0 grad norm: 0.029274502769112587\n",
            "conv1.weight grad norm: 0.24691879749298096\n",
            "conv1.bias grad norm: 0.05965553596615791\n",
            "norm.weight grad norm: 0.02616499550640583\n",
            "norm.bias grad norm: 0.032701652497053146\n",
            "lstm.weight_ih_l0 grad norm: 0.19042041897773743\n",
            "lstm.weight_hh_l0 grad norm: 0.052953463047742844\n",
            "lstm.bias_ih_l0 grad norm: 0.01747971586883068\n",
            "lstm.bias_hh_l0 grad norm: 0.01747971586883068\n",
            "fc.weight grad norm: 0.3042815327644348\n",
            "fc.bias grad norm: 0.17718446254730225\n",
            "[Batch 4000] Loss: 0.0551\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.0061080255545675755\n",
            "c0 grad norm: 0.007324126083403826\n",
            "conv1.weight grad norm: 0.06592395156621933\n",
            "conv1.bias grad norm: 0.016144825145602226\n",
            "norm.weight grad norm: 0.009844905696809292\n",
            "norm.bias grad norm: 0.010467678308486938\n",
            "lstm.weight_ih_l0 grad norm: 0.05206742137670517\n",
            "lstm.weight_hh_l0 grad norm: 0.011651486158370972\n",
            "lstm.bias_ih_l0 grad norm: 0.004795292858034372\n",
            "lstm.bias_hh_l0 grad norm: 0.004795292858034372\n",
            "fc.weight grad norm: 0.11896053701639175\n",
            "fc.bias grad norm: 0.04917006939649582\n",
            "[Batch 5000] Loss: 0.0119\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.015122521668672562\n",
            "c0 grad norm: 0.022487759590148926\n",
            "conv1.weight grad norm: 0.16719584167003632\n",
            "conv1.bias grad norm: 0.025519391521811485\n",
            "norm.weight grad norm: 0.020520159974694252\n",
            "norm.bias grad norm: 0.020036686211824417\n",
            "lstm.weight_ih_l0 grad norm: 0.1475970447063446\n",
            "lstm.weight_hh_l0 grad norm: 0.04767564311623573\n",
            "lstm.bias_ih_l0 grad norm: 0.013696628622710705\n",
            "lstm.bias_hh_l0 grad norm: 0.013696628622710705\n",
            "fc.weight grad norm: 0.16589152812957764\n",
            "fc.bias grad norm: 0.024098694324493408\n",
            "[Batch 6000] Loss: 0.0099\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.02104710228741169\n",
            "c0 grad norm: 0.025871677324175835\n",
            "conv1.weight grad norm: 0.23398210108280182\n",
            "conv1.bias grad norm: 0.047515638172626495\n",
            "norm.weight grad norm: 0.026617340743541718\n",
            "norm.bias grad norm: 0.030414488166570663\n",
            "lstm.weight_ih_l0 grad norm: 0.20514002442359924\n",
            "lstm.weight_hh_l0 grad norm: 0.061441946774721146\n",
            "lstm.bias_ih_l0 grad norm: 0.02044004015624523\n",
            "lstm.bias_hh_l0 grad norm: 0.02044004015624523\n",
            "fc.weight grad norm: 0.18761467933654785\n",
            "fc.bias grad norm: 0.058633171021938324\n",
            "[Batch 7000] Loss: 0.0104\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.00850693229585886\n",
            "c0 grad norm: 0.01634823903441429\n",
            "conv1.weight grad norm: 0.08213624358177185\n",
            "conv1.bias grad norm: 0.026762528344988823\n",
            "norm.weight grad norm: 0.011793483980000019\n",
            "norm.bias grad norm: 0.020104844123125076\n",
            "lstm.weight_ih_l0 grad norm: 0.07201600819826126\n",
            "lstm.weight_hh_l0 grad norm: 0.020640088245272636\n",
            "lstm.bias_ih_l0 grad norm: 0.007463677320629358\n",
            "lstm.bias_hh_l0 grad norm: 0.007463677320629358\n",
            "fc.weight grad norm: 0.17854155600070953\n",
            "fc.bias grad norm: 0.060000963509082794\n",
            "[Batch 8000] Loss: 0.0142\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.05467892438173294\n",
            "c0 grad norm: 0.049531929194927216\n",
            "conv1.weight grad norm: 0.568065345287323\n",
            "conv1.bias grad norm: 0.14266043901443481\n",
            "norm.weight grad norm: 0.08389700204133987\n",
            "norm.bias grad norm: 0.09712047874927521\n",
            "lstm.weight_ih_l0 grad norm: 0.541802704334259\n",
            "lstm.weight_hh_l0 grad norm: 0.14133113622665405\n",
            "lstm.bias_ih_l0 grad norm: 0.05070564150810242\n",
            "lstm.bias_hh_l0 grad norm: 0.05070564150810242\n",
            "fc.weight grad norm: 0.8095317482948303\n",
            "fc.bias grad norm: 0.09349841624498367\n",
            "[Batch 9000] Loss: 0.5832\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 87\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.023199835792183876\n",
            "c0 grad norm: 0.03057466819882393\n",
            "conv1.weight grad norm: 0.3719092011451721\n",
            "conv1.bias grad norm: 0.05728146433830261\n",
            "norm.weight grad norm: 0.05063563212752342\n",
            "norm.bias grad norm: 0.050948526710271835\n",
            "lstm.weight_ih_l0 grad norm: 0.28093114495277405\n",
            "lstm.weight_hh_l0 grad norm: 0.09167920798063278\n",
            "lstm.bias_ih_l0 grad norm: 0.0276047233492136\n",
            "lstm.bias_hh_l0 grad norm: 0.0276047233492136\n",
            "fc.weight grad norm: 0.3175714612007141\n",
            "fc.bias grad norm: 0.07268957048654556\n",
            "[Batch 0] Loss: 0.0394\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.014739190228283405\n",
            "c0 grad norm: 0.04404555633664131\n",
            "conv1.weight grad norm: 0.16021890938282013\n",
            "conv1.bias grad norm: 0.03633932024240494\n",
            "norm.weight grad norm: 0.02423509769141674\n",
            "norm.bias grad norm: 0.02907516621053219\n",
            "lstm.weight_ih_l0 grad norm: 0.1576737016439438\n",
            "lstm.weight_hh_l0 grad norm: 0.04365776106715202\n",
            "lstm.bias_ih_l0 grad norm: 0.014904634095728397\n",
            "lstm.bias_hh_l0 grad norm: 0.014904634095728397\n",
            "fc.weight grad norm: 0.1487473100423813\n",
            "fc.bias grad norm: 0.03244656324386597\n",
            "[Batch 1000] Loss: 0.0188\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.050916124135255814\n",
            "c0 grad norm: 0.0507766455411911\n",
            "conv1.weight grad norm: 0.6961879134178162\n",
            "conv1.bias grad norm: 0.20417317748069763\n",
            "norm.weight grad norm: 0.10654210299253464\n",
            "norm.bias grad norm: 0.13945980370044708\n",
            "lstm.weight_ih_l0 grad norm: 0.5782777070999146\n",
            "lstm.weight_hh_l0 grad norm: 0.08958766609430313\n",
            "lstm.bias_ih_l0 grad norm: 0.06030238792300224\n",
            "lstm.bias_hh_l0 grad norm: 0.06030238792300224\n",
            "fc.weight grad norm: 0.24055375158786774\n",
            "fc.bias grad norm: 0.1035095825791359\n",
            "[Batch 2000] Loss: 0.0781\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.013961891643702984\n",
            "c0 grad norm: 0.011633099056780338\n",
            "conv1.weight grad norm: 0.13864363729953766\n",
            "conv1.bias grad norm: 0.03981809318065643\n",
            "norm.weight grad norm: 0.02317916788160801\n",
            "norm.bias grad norm: 0.029726093634963036\n",
            "lstm.weight_ih_l0 grad norm: 0.13285212218761444\n",
            "lstm.weight_hh_l0 grad norm: 0.03452560305595398\n",
            "lstm.bias_ih_l0 grad norm: 0.013269581831991673\n",
            "lstm.bias_hh_l0 grad norm: 0.013269581831991673\n",
            "fc.weight grad norm: 0.2951967120170593\n",
            "fc.bias grad norm: 0.12218134105205536\n",
            "[Batch 3000] Loss: 0.0381\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.0439063236117363\n",
            "c0 grad norm: 0.05427369475364685\n",
            "conv1.weight grad norm: 0.5668965578079224\n",
            "conv1.bias grad norm: 0.10295410454273224\n",
            "norm.weight grad norm: 0.06696704775094986\n",
            "norm.bias grad norm: 0.08884914219379425\n",
            "lstm.weight_ih_l0 grad norm: 0.2956302762031555\n",
            "lstm.weight_hh_l0 grad norm: 0.05875563621520996\n",
            "lstm.bias_ih_l0 grad norm: 0.029003629460930824\n",
            "lstm.bias_hh_l0 grad norm: 0.029003629460930824\n",
            "fc.weight grad norm: 0.3188038766384125\n",
            "fc.bias grad norm: 0.1804984211921692\n",
            "[Batch 4000] Loss: 0.0649\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.05828012153506279\n",
            "c0 grad norm: 0.07252566516399384\n",
            "conv1.weight grad norm: 0.49193620681762695\n",
            "conv1.bias grad norm: 0.08676029741764069\n",
            "norm.weight grad norm: 0.06789575517177582\n",
            "norm.bias grad norm: 0.06525470316410065\n",
            "lstm.weight_ih_l0 grad norm: 0.4259723722934723\n",
            "lstm.weight_hh_l0 grad norm: 0.10702391713857651\n",
            "lstm.bias_ih_l0 grad norm: 0.038748521357774734\n",
            "lstm.bias_hh_l0 grad norm: 0.038748521357774734\n",
            "fc.weight grad norm: 0.4021490514278412\n",
            "fc.bias grad norm: 0.09099496155977249\n",
            "[Batch 5000] Loss: 0.1218\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.05850657820701599\n",
            "c0 grad norm: 0.0811636671423912\n",
            "conv1.weight grad norm: 1.2925035953521729\n",
            "conv1.bias grad norm: 0.214634507894516\n",
            "norm.weight grad norm: 0.18046680092811584\n",
            "norm.bias grad norm: 0.18370485305786133\n",
            "lstm.weight_ih_l0 grad norm: 0.7836499214172363\n",
            "lstm.weight_hh_l0 grad norm: 0.24471411108970642\n",
            "lstm.bias_ih_l0 grad norm: 0.06865456700325012\n",
            "lstm.bias_hh_l0 grad norm: 0.06865456700325012\n",
            "fc.weight grad norm: 0.5399817824363708\n",
            "fc.bias grad norm: 0.12690772116184235\n",
            "[Batch 6000] Loss: 0.4543\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.028673242777585983\n",
            "c0 grad norm: 0.03880361467599869\n",
            "conv1.weight grad norm: 0.22046442329883575\n",
            "conv1.bias grad norm: 0.045717671513557434\n",
            "norm.weight grad norm: 0.03531568869948387\n",
            "norm.bias grad norm: 0.027335552498698235\n",
            "lstm.weight_ih_l0 grad norm: 0.19650974869728088\n",
            "lstm.weight_hh_l0 grad norm: 0.055805645883083344\n",
            "lstm.bias_ih_l0 grad norm: 0.019154611974954605\n",
            "lstm.bias_hh_l0 grad norm: 0.019154611974954605\n",
            "fc.weight grad norm: 0.24457785487174988\n",
            "fc.bias grad norm: 0.06257284432649612\n",
            "[Batch 7000] Loss: 0.0238\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.024323616176843643\n",
            "c0 grad norm: 0.019197920337319374\n",
            "conv1.weight grad norm: 0.27284735441207886\n",
            "conv1.bias grad norm: 0.06645363569259644\n",
            "norm.weight grad norm: 0.029166974127292633\n",
            "norm.bias grad norm: 0.04936843365430832\n",
            "lstm.weight_ih_l0 grad norm: 0.20478741824626923\n",
            "lstm.weight_hh_l0 grad norm: 0.05440818518400192\n",
            "lstm.bias_ih_l0 grad norm: 0.019024411216378212\n",
            "lstm.bias_hh_l0 grad norm: 0.019024411216378212\n",
            "fc.weight grad norm: 0.13872787356376648\n",
            "fc.bias grad norm: 0.05164458230137825\n",
            "[Batch 8000] Loss: 0.0119\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.03956513851881027\n",
            "c0 grad norm: 0.05252969637513161\n",
            "conv1.weight grad norm: 0.685967206954956\n",
            "conv1.bias grad norm: 0.10100898146629333\n",
            "norm.weight grad norm: 0.08750410377979279\n",
            "norm.bias grad norm: 0.088788241147995\n",
            "lstm.weight_ih_l0 grad norm: 0.5211442112922668\n",
            "lstm.weight_hh_l0 grad norm: 0.16107797622680664\n",
            "lstm.bias_ih_l0 grad norm: 0.04938628897070885\n",
            "lstm.bias_hh_l0 grad norm: 0.04938628897070885\n",
            "fc.weight grad norm: 0.3223063349723816\n",
            "fc.bias grad norm: 0.08771834522485733\n",
            "[Batch 9000] Loss: 0.0736\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 88\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.025039562955498695\n",
            "c0 grad norm: 0.07003848254680634\n",
            "conv1.weight grad norm: 0.285849004983902\n",
            "conv1.bias grad norm: 0.0760183036327362\n",
            "norm.weight grad norm: 0.04448884725570679\n",
            "norm.bias grad norm: 0.05384751036763191\n",
            "lstm.weight_ih_l0 grad norm: 0.2566505968570709\n",
            "lstm.weight_hh_l0 grad norm: 0.07374664396047592\n",
            "lstm.bias_ih_l0 grad norm: 0.02337084710597992\n",
            "lstm.bias_hh_l0 grad norm: 0.02337084710597992\n",
            "fc.weight grad norm: 0.29434388875961304\n",
            "fc.bias grad norm: 0.15789951384067535\n",
            "[Batch 0] Loss: 0.0477\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.05364818125963211\n",
            "c0 grad norm: 0.043060269206762314\n",
            "conv1.weight grad norm: 0.3618483245372772\n",
            "conv1.bias grad norm: 0.07586359977722168\n",
            "norm.weight grad norm: 0.05338440462946892\n",
            "norm.bias grad norm: 0.06162840873003006\n",
            "lstm.weight_ih_l0 grad norm: 0.37782225012779236\n",
            "lstm.weight_hh_l0 grad norm: 0.10828521847724915\n",
            "lstm.bias_ih_l0 grad norm: 0.037914250046014786\n",
            "lstm.bias_hh_l0 grad norm: 0.037914250046014786\n",
            "fc.weight grad norm: 0.2940331995487213\n",
            "fc.bias grad norm: 0.12359807640314102\n",
            "[Batch 1000] Loss: 0.0301\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.02473067305982113\n",
            "c0 grad norm: 0.031260885298252106\n",
            "conv1.weight grad norm: 0.2556179165840149\n",
            "conv1.bias grad norm: 0.06359757483005524\n",
            "norm.weight grad norm: 0.031520865857601166\n",
            "norm.bias grad norm: 0.0315813310444355\n",
            "lstm.weight_ih_l0 grad norm: 0.19425347447395325\n",
            "lstm.weight_hh_l0 grad norm: 0.05311422422528267\n",
            "lstm.bias_ih_l0 grad norm: 0.01887880451977253\n",
            "lstm.bias_hh_l0 grad norm: 0.01887880451977253\n",
            "fc.weight grad norm: 0.17644093930721283\n",
            "fc.bias grad norm: 0.10639644414186478\n",
            "[Batch 2000] Loss: 0.0150\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.03593429923057556\n",
            "c0 grad norm: 0.024945233017206192\n",
            "conv1.weight grad norm: 0.5941335558891296\n",
            "conv1.bias grad norm: 0.18157926201820374\n",
            "norm.weight grad norm: 0.09033064544200897\n",
            "norm.bias grad norm: 0.10474397242069244\n",
            "lstm.weight_ih_l0 grad norm: 0.3158298432826996\n",
            "lstm.weight_hh_l0 grad norm: 0.08186499029397964\n",
            "lstm.bias_ih_l0 grad norm: 0.028510697185993195\n",
            "lstm.bias_hh_l0 grad norm: 0.028510697185993195\n",
            "fc.weight grad norm: 0.22961163520812988\n",
            "fc.bias grad norm: 0.057841040194034576\n",
            "[Batch 3000] Loss: 0.0383\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.05208686739206314\n",
            "c0 grad norm: 0.075399249792099\n",
            "conv1.weight grad norm: 0.5605342984199524\n",
            "conv1.bias grad norm: 0.17759576439857483\n",
            "norm.weight grad norm: 0.07392680644989014\n",
            "norm.bias grad norm: 0.09474419802427292\n",
            "lstm.weight_ih_l0 grad norm: 0.44914954900741577\n",
            "lstm.weight_hh_l0 grad norm: 0.13228534162044525\n",
            "lstm.bias_ih_l0 grad norm: 0.04267213121056557\n",
            "lstm.bias_hh_l0 grad norm: 0.04267213121056557\n",
            "fc.weight grad norm: 0.6766306161880493\n",
            "fc.bias grad norm: 0.20806576311588287\n",
            "[Batch 4000] Loss: 0.0981\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.0496981143951416\n",
            "c0 grad norm: 0.08813849091529846\n",
            "conv1.weight grad norm: 0.4961300790309906\n",
            "conv1.bias grad norm: 0.12070902436971664\n",
            "norm.weight grad norm: 0.04944772645831108\n",
            "norm.bias grad norm: 0.05363303795456886\n",
            "lstm.weight_ih_l0 grad norm: 0.28816062211990356\n",
            "lstm.weight_hh_l0 grad norm: 0.06631844490766525\n",
            "lstm.bias_ih_l0 grad norm: 0.026188664138317108\n",
            "lstm.bias_hh_l0 grad norm: 0.026188664138317108\n",
            "fc.weight grad norm: 0.4019090533256531\n",
            "fc.bias grad norm: 0.17361707985401154\n",
            "[Batch 5000] Loss: 0.0655\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.013634726405143738\n",
            "c0 grad norm: 0.02724139764904976\n",
            "conv1.weight grad norm: 0.22214749455451965\n",
            "conv1.bias grad norm: 0.029309196397662163\n",
            "norm.weight grad norm: 0.022998420521616936\n",
            "norm.bias grad norm: 0.02908860333263874\n",
            "lstm.weight_ih_l0 grad norm: 0.11467543244361877\n",
            "lstm.weight_hh_l0 grad norm: 0.03631119802594185\n",
            "lstm.bias_ih_l0 grad norm: 0.011512571014463902\n",
            "lstm.bias_hh_l0 grad norm: 0.011512571014463902\n",
            "fc.weight grad norm: 0.2259960025548935\n",
            "fc.bias grad norm: 0.05088189244270325\n",
            "[Batch 6000] Loss: 0.0177\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.05388617888092995\n",
            "c0 grad norm: 0.04204043373465538\n",
            "conv1.weight grad norm: 0.33309754729270935\n",
            "conv1.bias grad norm: 0.11517777293920517\n",
            "norm.weight grad norm: 0.050824277102947235\n",
            "norm.bias grad norm: 0.08286095410585403\n",
            "lstm.weight_ih_l0 grad norm: 0.3045424520969391\n",
            "lstm.weight_hh_l0 grad norm: 0.06338168680667877\n",
            "lstm.bias_ih_l0 grad norm: 0.02924177050590515\n",
            "lstm.bias_hh_l0 grad norm: 0.02924177050590515\n",
            "fc.weight grad norm: 0.18057580292224884\n",
            "fc.bias grad norm: 0.029966164380311966\n",
            "[Batch 7000] Loss: 0.0209\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.07665708661079407\n",
            "c0 grad norm: 0.06143779680132866\n",
            "conv1.weight grad norm: 0.5097324848175049\n",
            "conv1.bias grad norm: 0.10176818072795868\n",
            "norm.weight grad norm: 0.08387932181358337\n",
            "norm.bias grad norm: 0.09329346567392349\n",
            "lstm.weight_ih_l0 grad norm: 0.31924253702163696\n",
            "lstm.weight_hh_l0 grad norm: 0.07654588669538498\n",
            "lstm.bias_ih_l0 grad norm: 0.029418818652629852\n",
            "lstm.bias_hh_l0 grad norm: 0.029418818652629852\n",
            "fc.weight grad norm: 0.42490774393081665\n",
            "fc.bias grad norm: 0.24285832047462463\n",
            "[Batch 8000] Loss: 0.0988\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.040129318833351135\n",
            "c0 grad norm: 0.04243912175297737\n",
            "conv1.weight grad norm: 0.3031552731990814\n",
            "conv1.bias grad norm: 0.08987369388341904\n",
            "norm.weight grad norm: 0.040254250168800354\n",
            "norm.bias grad norm: 0.048365429043769836\n",
            "lstm.weight_ih_l0 grad norm: 0.3754955530166626\n",
            "lstm.weight_hh_l0 grad norm: 0.11808580905199051\n",
            "lstm.bias_ih_l0 grad norm: 0.03545074164867401\n",
            "lstm.bias_hh_l0 grad norm: 0.03545074164867401\n",
            "fc.weight grad norm: 0.40090247988700867\n",
            "fc.bias grad norm: 0.10698512941598892\n",
            "[Batch 9000] Loss: 0.0313\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 89\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.017898960039019585\n",
            "c0 grad norm: 0.019876157864928246\n",
            "conv1.weight grad norm: 0.19493842124938965\n",
            "conv1.bias grad norm: 0.05712675303220749\n",
            "norm.weight grad norm: 0.03100760281085968\n",
            "norm.bias grad norm: 0.0376223549246788\n",
            "lstm.weight_ih_l0 grad norm: 0.2296861708164215\n",
            "lstm.weight_hh_l0 grad norm: 0.05628018453717232\n",
            "lstm.bias_ih_l0 grad norm: 0.022107452154159546\n",
            "lstm.bias_hh_l0 grad norm: 0.022107452154159546\n",
            "fc.weight grad norm: 0.19322967529296875\n",
            "fc.bias grad norm: 0.09947366267442703\n",
            "[Batch 0] Loss: 0.0185\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.07487992197275162\n",
            "c0 grad norm: 0.11465831845998764\n",
            "conv1.weight grad norm: 0.645943284034729\n",
            "conv1.bias grad norm: 0.16599158942699432\n",
            "norm.weight grad norm: 0.059103649109601974\n",
            "norm.bias grad norm: 0.06426931172609329\n",
            "lstm.weight_ih_l0 grad norm: 0.28859636187553406\n",
            "lstm.weight_hh_l0 grad norm: 0.06439844518899918\n",
            "lstm.bias_ih_l0 grad norm: 0.027963243424892426\n",
            "lstm.bias_hh_l0 grad norm: 0.027963243424892426\n",
            "fc.weight grad norm: 0.32941117882728577\n",
            "fc.bias grad norm: 0.10803476721048355\n",
            "[Batch 1000] Loss: 0.0653\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.023380160331726074\n",
            "c0 grad norm: 0.08165863156318665\n",
            "conv1.weight grad norm: 0.3098830580711365\n",
            "conv1.bias grad norm: 0.06195858493447304\n",
            "norm.weight grad norm: 0.0428885854780674\n",
            "norm.bias grad norm: 0.03777201101183891\n",
            "lstm.weight_ih_l0 grad norm: 0.2766103148460388\n",
            "lstm.weight_hh_l0 grad norm: 0.07013152539730072\n",
            "lstm.bias_ih_l0 grad norm: 0.02615932747721672\n",
            "lstm.bias_hh_l0 grad norm: 0.02615932747721672\n",
            "fc.weight grad norm: 0.36503419280052185\n",
            "fc.bias grad norm: 0.0914578065276146\n",
            "[Batch 2000] Loss: 0.0461\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.011393086984753609\n",
            "c0 grad norm: 0.013490804471075535\n",
            "conv1.weight grad norm: 0.1171564906835556\n",
            "conv1.bias grad norm: 0.025688478723168373\n",
            "norm.weight grad norm: 0.015787040814757347\n",
            "norm.bias grad norm: 0.01971433125436306\n",
            "lstm.weight_ih_l0 grad norm: 0.12052051723003387\n",
            "lstm.weight_hh_l0 grad norm: 0.036803748458623886\n",
            "lstm.bias_ih_l0 grad norm: 0.011734060943126678\n",
            "lstm.bias_hh_l0 grad norm: 0.011734060943126678\n",
            "fc.weight grad norm: 0.3129357397556305\n",
            "fc.bias grad norm: 0.11508551239967346\n",
            "[Batch 3000] Loss: 0.0456\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.04565636068582535\n",
            "c0 grad norm: 0.1254929155111313\n",
            "conv1.weight grad norm: 0.5909180045127869\n",
            "conv1.bias grad norm: 0.09813479334115982\n",
            "norm.weight grad norm: 0.09748627245426178\n",
            "norm.bias grad norm: 0.09158909320831299\n",
            "lstm.weight_ih_l0 grad norm: 0.5763194561004639\n",
            "lstm.weight_hh_l0 grad norm: 0.17505431175231934\n",
            "lstm.bias_ih_l0 grad norm: 0.05256388336420059\n",
            "lstm.bias_hh_l0 grad norm: 0.05256388336420059\n",
            "fc.weight grad norm: 0.4996598958969116\n",
            "fc.bias grad norm: 0.12107736617326736\n",
            "[Batch 4000] Loss: 0.1093\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.03530200570821762\n",
            "c0 grad norm: 0.06977876275777817\n",
            "conv1.weight grad norm: 0.39768460392951965\n",
            "conv1.bias grad norm: 0.0562649630010128\n",
            "norm.weight grad norm: 0.0426597036421299\n",
            "norm.bias grad norm: 0.047056879848241806\n",
            "lstm.weight_ih_l0 grad norm: 0.2937556207180023\n",
            "lstm.weight_hh_l0 grad norm: 0.08258029818534851\n",
            "lstm.bias_ih_l0 grad norm: 0.025692304596304893\n",
            "lstm.bias_hh_l0 grad norm: 0.025692304596304893\n",
            "fc.weight grad norm: 0.5292835831642151\n",
            "fc.bias grad norm: 0.10679662227630615\n",
            "[Batch 5000] Loss: 0.0591\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.011287537403404713\n",
            "c0 grad norm: 0.04196108877658844\n",
            "conv1.weight grad norm: 0.23214343190193176\n",
            "conv1.bias grad norm: 0.037240274250507355\n",
            "norm.weight grad norm: 0.033142197877168655\n",
            "norm.bias grad norm: 0.030491597950458527\n",
            "lstm.weight_ih_l0 grad norm: 0.18627148866653442\n",
            "lstm.weight_hh_l0 grad norm: 0.05966075137257576\n",
            "lstm.bias_ih_l0 grad norm: 0.017448455095291138\n",
            "lstm.bias_hh_l0 grad norm: 0.017448455095291138\n",
            "fc.weight grad norm: 0.31268635392189026\n",
            "fc.bias grad norm: 0.10675501078367233\n",
            "[Batch 6000] Loss: 0.0419\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.010676253587007523\n",
            "c0 grad norm: 0.034736547619104385\n",
            "conv1.weight grad norm: 0.19508692622184753\n",
            "conv1.bias grad norm: 0.03221183270215988\n",
            "norm.weight grad norm: 0.031201999634504318\n",
            "norm.bias grad norm: 0.02740216627717018\n",
            "lstm.weight_ih_l0 grad norm: 0.15540240705013275\n",
            "lstm.weight_hh_l0 grad norm: 0.04722042381763458\n",
            "lstm.bias_ih_l0 grad norm: 0.015321575105190277\n",
            "lstm.bias_hh_l0 grad norm: 0.015321575105190277\n",
            "fc.weight grad norm: 0.4690432548522949\n",
            "fc.bias grad norm: 0.14519521594047546\n",
            "[Batch 7000] Loss: 0.0573\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.05091031640768051\n",
            "c0 grad norm: 0.07373420894145966\n",
            "conv1.weight grad norm: 0.4088382124900818\n",
            "conv1.bias grad norm: 0.11474727839231491\n",
            "norm.weight grad norm: 0.056609828025102615\n",
            "norm.bias grad norm: 0.07842673361301422\n",
            "lstm.weight_ih_l0 grad norm: 0.30318132042884827\n",
            "lstm.weight_hh_l0 grad norm: 0.08194837719202042\n",
            "lstm.bias_ih_l0 grad norm: 0.03113308921456337\n",
            "lstm.bias_hh_l0 grad norm: 0.03113308921456337\n",
            "fc.weight grad norm: 0.43332618474960327\n",
            "fc.bias grad norm: 0.044948920607566833\n",
            "[Batch 8000] Loss: 0.0624\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.019880790263414383\n",
            "c0 grad norm: 0.016541212797164917\n",
            "conv1.weight grad norm: 0.17290832102298737\n",
            "conv1.bias grad norm: 0.028973419219255447\n",
            "norm.weight grad norm: 0.02330750599503517\n",
            "norm.bias grad norm: 0.022686487063765526\n",
            "lstm.weight_ih_l0 grad norm: 0.1589733213186264\n",
            "lstm.weight_hh_l0 grad norm: 0.04564480111002922\n",
            "lstm.bias_ih_l0 grad norm: 0.014960930682718754\n",
            "lstm.bias_hh_l0 grad norm: 0.014960930682718754\n",
            "fc.weight grad norm: 0.08844903111457825\n",
            "fc.bias grad norm: 0.019955165684223175\n",
            "[Batch 9000] Loss: 0.0233\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 90\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03386679291725159\n",
            "c0 grad norm: 0.11034005135297775\n",
            "conv1.weight grad norm: 0.43616411089897156\n",
            "conv1.bias grad norm: 0.06860566884279251\n",
            "norm.weight grad norm: 0.06146126613020897\n",
            "norm.bias grad norm: 0.05609983578324318\n",
            "lstm.weight_ih_l0 grad norm: 0.34121373295783997\n",
            "lstm.weight_hh_l0 grad norm: 0.09632349014282227\n",
            "lstm.bias_ih_l0 grad norm: 0.030095022171735764\n",
            "lstm.bias_hh_l0 grad norm: 0.030095022171735764\n",
            "fc.weight grad norm: 0.46308818459510803\n",
            "fc.bias grad norm: 0.2445317655801773\n",
            "[Batch 0] Loss: 0.0870\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.061938971281051636\n",
            "c0 grad norm: 0.13823753595352173\n",
            "conv1.weight grad norm: 0.8518857955932617\n",
            "conv1.bias grad norm: 0.2315886914730072\n",
            "norm.weight grad norm: 0.1095898374915123\n",
            "norm.bias grad norm: 0.1393730491399765\n",
            "lstm.weight_ih_l0 grad norm: 0.6209340691566467\n",
            "lstm.weight_hh_l0 grad norm: 0.16133816540241241\n",
            "lstm.bias_ih_l0 grad norm: 0.06211381405591965\n",
            "lstm.bias_hh_l0 grad norm: 0.06211381405591965\n",
            "fc.weight grad norm: 0.7322723865509033\n",
            "fc.bias grad norm: 0.29478734731674194\n",
            "[Batch 1000] Loss: 0.1372\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.09220416098833084\n",
            "c0 grad norm: 0.13960783183574677\n",
            "conv1.weight grad norm: 0.6330339908599854\n",
            "conv1.bias grad norm: 0.22341352701187134\n",
            "norm.weight grad norm: 0.08152419328689575\n",
            "norm.bias grad norm: 0.10365517437458038\n",
            "lstm.weight_ih_l0 grad norm: 0.533263623714447\n",
            "lstm.weight_hh_l0 grad norm: 0.10970702767372131\n",
            "lstm.bias_ih_l0 grad norm: 0.05324781313538551\n",
            "lstm.bias_hh_l0 grad norm: 0.05324781313538551\n",
            "fc.weight grad norm: 0.44874247908592224\n",
            "fc.bias grad norm: 0.2279028445482254\n",
            "[Batch 2000] Loss: 0.1077\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.040665335953235626\n",
            "c0 grad norm: 0.04035608842968941\n",
            "conv1.weight grad norm: 0.37077730894088745\n",
            "conv1.bias grad norm: 0.0827411413192749\n",
            "norm.weight grad norm: 0.04622035101056099\n",
            "norm.bias grad norm: 0.06842607259750366\n",
            "lstm.weight_ih_l0 grad norm: 0.27433377504348755\n",
            "lstm.weight_hh_l0 grad norm: 0.07201769202947617\n",
            "lstm.bias_ih_l0 grad norm: 0.0278128944337368\n",
            "lstm.bias_hh_l0 grad norm: 0.0278128944337368\n",
            "fc.weight grad norm: 0.3830975592136383\n",
            "fc.bias grad norm: 0.020023992285132408\n",
            "[Batch 3000] Loss: 0.0397\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.018421871587634087\n",
            "c0 grad norm: 0.04267757013440132\n",
            "conv1.weight grad norm: 0.28088900446891785\n",
            "conv1.bias grad norm: 0.05129556730389595\n",
            "norm.weight grad norm: 0.03331479802727699\n",
            "norm.bias grad norm: 0.037631556391716\n",
            "lstm.weight_ih_l0 grad norm: 0.20693263411521912\n",
            "lstm.weight_hh_l0 grad norm: 0.053426776081323624\n",
            "lstm.bias_ih_l0 grad norm: 0.01991160959005356\n",
            "lstm.bias_hh_l0 grad norm: 0.01991160959005356\n",
            "fc.weight grad norm: 0.24325142800807953\n",
            "fc.bias grad norm: 0.10430241376161575\n",
            "[Batch 4000] Loss: 0.0444\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.021365992724895477\n",
            "c0 grad norm: 0.02835884690284729\n",
            "conv1.weight grad norm: 0.1810080111026764\n",
            "conv1.bias grad norm: 0.030822256579995155\n",
            "norm.weight grad norm: 0.027323191985487938\n",
            "norm.bias grad norm: 0.026187516748905182\n",
            "lstm.weight_ih_l0 grad norm: 0.14375939965248108\n",
            "lstm.weight_hh_l0 grad norm: 0.034402452409267426\n",
            "lstm.bias_ih_l0 grad norm: 0.012591861188411713\n",
            "lstm.bias_hh_l0 grad norm: 0.012591861188411713\n",
            "fc.weight grad norm: 0.25981906056404114\n",
            "fc.bias grad norm: 0.12830579280853271\n",
            "[Batch 5000] Loss: 0.0231\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.027543814852833748\n",
            "c0 grad norm: 0.046745575964450836\n",
            "conv1.weight grad norm: 0.29520413279533386\n",
            "conv1.bias grad norm: 0.09364473819732666\n",
            "norm.weight grad norm: 0.038078129291534424\n",
            "norm.bias grad norm: 0.04718853160738945\n",
            "lstm.weight_ih_l0 grad norm: 0.2662753462791443\n",
            "lstm.weight_hh_l0 grad norm: 0.07374364137649536\n",
            "lstm.bias_ih_l0 grad norm: 0.02519998326897621\n",
            "lstm.bias_hh_l0 grad norm: 0.02519998326897621\n",
            "fc.weight grad norm: 0.3409036099910736\n",
            "fc.bias grad norm: 0.04793372377753258\n",
            "[Batch 6000] Loss: 0.0476\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.05551121011376381\n",
            "c0 grad norm: 0.0792728066444397\n",
            "conv1.weight grad norm: 0.8818825483322144\n",
            "conv1.bias grad norm: 0.18903473019599915\n",
            "norm.weight grad norm: 0.102019302546978\n",
            "norm.bias grad norm: 0.1363682746887207\n",
            "lstm.weight_ih_l0 grad norm: 0.529155969619751\n",
            "lstm.weight_hh_l0 grad norm: 0.16448873281478882\n",
            "lstm.bias_ih_l0 grad norm: 0.05343971028923988\n",
            "lstm.bias_hh_l0 grad norm: 0.05343971028923988\n",
            "fc.weight grad norm: 0.7814117074012756\n",
            "fc.bias grad norm: 0.23296615481376648\n",
            "[Batch 7000] Loss: 0.1332\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.009321478195488453\n",
            "c0 grad norm: 0.023891033604741096\n",
            "conv1.weight grad norm: 0.09989812970161438\n",
            "conv1.bias grad norm: 0.02181483991444111\n",
            "norm.weight grad norm: 0.012103687971830368\n",
            "norm.bias grad norm: 0.013661852106451988\n",
            "lstm.weight_ih_l0 grad norm: 0.08397865295410156\n",
            "lstm.weight_hh_l0 grad norm: 0.02584330551326275\n",
            "lstm.bias_ih_l0 grad norm: 0.0080792885273695\n",
            "lstm.bias_hh_l0 grad norm: 0.0080792885273695\n",
            "fc.weight grad norm: 0.1354813426733017\n",
            "fc.bias grad norm: 0.05502144992351532\n",
            "[Batch 8000] Loss: 0.0082\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.09517388790845871\n",
            "c0 grad norm: 0.0777125358581543\n",
            "conv1.weight grad norm: 1.0328518152236938\n",
            "conv1.bias grad norm: 0.22219650447368622\n",
            "norm.weight grad norm: 0.12368031591176987\n",
            "norm.bias grad norm: 0.16055546700954437\n",
            "lstm.weight_ih_l0 grad norm: 0.6886497735977173\n",
            "lstm.weight_hh_l0 grad norm: 0.15550918877124786\n",
            "lstm.bias_ih_l0 grad norm: 0.06811846792697906\n",
            "lstm.bias_hh_l0 grad norm: 0.06811846792697906\n",
            "fc.weight grad norm: 0.43751487135887146\n",
            "fc.bias grad norm: 0.2239852100610733\n",
            "[Batch 9000] Loss: 0.1929\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 91\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.03324007987976074\n",
            "c0 grad norm: 0.03871522843837738\n",
            "conv1.weight grad norm: 0.4656589925289154\n",
            "conv1.bias grad norm: 0.06885266304016113\n",
            "norm.weight grad norm: 0.07196492701768875\n",
            "norm.bias grad norm: 0.061303723603487015\n",
            "lstm.weight_ih_l0 grad norm: 0.3683193624019623\n",
            "lstm.weight_hh_l0 grad norm: 0.10808520019054413\n",
            "lstm.bias_ih_l0 grad norm: 0.03491903096437454\n",
            "lstm.bias_hh_l0 grad norm: 0.03491903096437454\n",
            "fc.weight grad norm: 0.33297795057296753\n",
            "fc.bias grad norm: 0.13602952659130096\n",
            "[Batch 0] Loss: 0.1183\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.018636345863342285\n",
            "c0 grad norm: 0.03572273999452591\n",
            "conv1.weight grad norm: 0.3537687659263611\n",
            "conv1.bias grad norm: 0.1197449117898941\n",
            "norm.weight grad norm: 0.03874414041638374\n",
            "norm.bias grad norm: 0.06309836357831955\n",
            "lstm.weight_ih_l0 grad norm: 0.25250229239463806\n",
            "lstm.weight_hh_l0 grad norm: 0.06841813772916794\n",
            "lstm.bias_ih_l0 grad norm: 0.024399850517511368\n",
            "lstm.bias_hh_l0 grad norm: 0.024399850517511368\n",
            "fc.weight grad norm: 0.5342115163803101\n",
            "fc.bias grad norm: 0.12314596772193909\n",
            "[Batch 1000] Loss: 0.0867\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.016912436112761497\n",
            "c0 grad norm: 0.019603366032242775\n",
            "conv1.weight grad norm: 0.20429690182209015\n",
            "conv1.bias grad norm: 0.0641656368970871\n",
            "norm.weight grad norm: 0.020041098818182945\n",
            "norm.bias grad norm: 0.02720191702246666\n",
            "lstm.weight_ih_l0 grad norm: 0.14124181866645813\n",
            "lstm.weight_hh_l0 grad norm: 0.0400456041097641\n",
            "lstm.bias_ih_l0 grad norm: 0.014737791381776333\n",
            "lstm.bias_hh_l0 grad norm: 0.014737791381776333\n",
            "fc.weight grad norm: 0.15159353613853455\n",
            "fc.bias grad norm: 0.05344395339488983\n",
            "[Batch 2000] Loss: 0.0101\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.02359282597899437\n",
            "c0 grad norm: 0.017212873324751854\n",
            "conv1.weight grad norm: 0.1578371375799179\n",
            "conv1.bias grad norm: 0.02168409526348114\n",
            "norm.weight grad norm: 0.01978781446814537\n",
            "norm.bias grad norm: 0.019263189285993576\n",
            "lstm.weight_ih_l0 grad norm: 0.12687157094478607\n",
            "lstm.weight_hh_l0 grad norm: 0.03273414075374603\n",
            "lstm.bias_ih_l0 grad norm: 0.011950450018048286\n",
            "lstm.bias_hh_l0 grad norm: 0.011950450018048286\n",
            "fc.weight grad norm: 0.17271211743354797\n",
            "fc.bias grad norm: 0.04543590918183327\n",
            "[Batch 3000] Loss: 0.0169\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.005644215736538172\n",
            "c0 grad norm: 0.014908055774867535\n",
            "conv1.weight grad norm: 0.08179139345884323\n",
            "conv1.bias grad norm: 0.017376990988850594\n",
            "norm.weight grad norm: 0.012584361247718334\n",
            "norm.bias grad norm: 0.011235153302550316\n",
            "lstm.weight_ih_l0 grad norm: 0.055830951780080795\n",
            "lstm.weight_hh_l0 grad norm: 0.01608029380440712\n",
            "lstm.bias_ih_l0 grad norm: 0.005475843790918589\n",
            "lstm.bias_hh_l0 grad norm: 0.005475843790918589\n",
            "fc.weight grad norm: 0.09660615026950836\n",
            "fc.bias grad norm: 0.03078356944024563\n",
            "[Batch 4000] Loss: 0.0030\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.015014194883406162\n",
            "c0 grad norm: 0.017141273245215416\n",
            "conv1.weight grad norm: 0.14057251811027527\n",
            "conv1.bias grad norm: 0.029011109843850136\n",
            "norm.weight grad norm: 0.019014976918697357\n",
            "norm.bias grad norm: 0.02002822794020176\n",
            "lstm.weight_ih_l0 grad norm: 0.10298136621713638\n",
            "lstm.weight_hh_l0 grad norm: 0.033149536699056625\n",
            "lstm.bias_ih_l0 grad norm: 0.010144766420125961\n",
            "lstm.bias_hh_l0 grad norm: 0.010144766420125961\n",
            "fc.weight grad norm: 0.14866723120212555\n",
            "fc.bias grad norm: 0.062162138521671295\n",
            "[Batch 5000] Loss: 0.0110\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.056359268724918365\n",
            "c0 grad norm: 0.05098143592476845\n",
            "conv1.weight grad norm: 0.6820019483566284\n",
            "conv1.bias grad norm: 0.15093132853507996\n",
            "norm.weight grad norm: 0.07719986140727997\n",
            "norm.bias grad norm: 0.08844205737113953\n",
            "lstm.weight_ih_l0 grad norm: 0.4116990864276886\n",
            "lstm.weight_hh_l0 grad norm: 0.11730733513832092\n",
            "lstm.bias_ih_l0 grad norm: 0.04000312462449074\n",
            "lstm.bias_hh_l0 grad norm: 0.04000312462449074\n",
            "fc.weight grad norm: 0.49213409423828125\n",
            "fc.bias grad norm: 0.19083131849765778\n",
            "[Batch 6000] Loss: 0.0639\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.03713664412498474\n",
            "c0 grad norm: 0.08272170275449753\n",
            "conv1.weight grad norm: 0.39104709029197693\n",
            "conv1.bias grad norm: 0.0919119119644165\n",
            "norm.weight grad norm: 0.05496322363615036\n",
            "norm.bias grad norm: 0.07796121388673782\n",
            "lstm.weight_ih_l0 grad norm: 0.38020139932632446\n",
            "lstm.weight_hh_l0 grad norm: 0.11044136434793472\n",
            "lstm.bias_ih_l0 grad norm: 0.03744647651910782\n",
            "lstm.bias_hh_l0 grad norm: 0.03744647651910782\n",
            "fc.weight grad norm: 0.33141982555389404\n",
            "fc.bias grad norm: 0.11163990199565887\n",
            "[Batch 7000] Loss: 0.0551\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.038253508508205414\n",
            "c0 grad norm: 0.07655690610408783\n",
            "conv1.weight grad norm: 0.33095866441726685\n",
            "conv1.bias grad norm: 0.058452293276786804\n",
            "norm.weight grad norm: 0.04018324241042137\n",
            "norm.bias grad norm: 0.04797406122088432\n",
            "lstm.weight_ih_l0 grad norm: 0.2634761929512024\n",
            "lstm.weight_hh_l0 grad norm: 0.05957387015223503\n",
            "lstm.bias_ih_l0 grad norm: 0.024743888527154922\n",
            "lstm.bias_hh_l0 grad norm: 0.024743888527154922\n",
            "fc.weight grad norm: 0.577214241027832\n",
            "fc.bias grad norm: 0.13469016551971436\n",
            "[Batch 8000] Loss: 0.0832\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.026114387437701225\n",
            "c0 grad norm: 0.03290528431534767\n",
            "conv1.weight grad norm: 0.3720220923423767\n",
            "conv1.bias grad norm: 0.07375214993953705\n",
            "norm.weight grad norm: 0.04049842432141304\n",
            "norm.bias grad norm: 0.04707358777523041\n",
            "lstm.weight_ih_l0 grad norm: 0.24444280564785004\n",
            "lstm.weight_hh_l0 grad norm: 0.08193007111549377\n",
            "lstm.bias_ih_l0 grad norm: 0.019492993131279945\n",
            "lstm.bias_hh_l0 grad norm: 0.019492993131279945\n",
            "fc.weight grad norm: 0.25327447056770325\n",
            "fc.bias grad norm: 0.03922669216990471\n",
            "[Batch 9000] Loss: 0.0299\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 92\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.04520156979560852\n",
            "c0 grad norm: 0.04965757578611374\n",
            "conv1.weight grad norm: 0.5829893350601196\n",
            "conv1.bias grad norm: 0.09890921413898468\n",
            "norm.weight grad norm: 0.06383419781923294\n",
            "norm.bias grad norm: 0.0811978206038475\n",
            "lstm.weight_ih_l0 grad norm: 0.4185609519481659\n",
            "lstm.weight_hh_l0 grad norm: 0.13272731006145477\n",
            "lstm.bias_ih_l0 grad norm: 0.04019690304994583\n",
            "lstm.bias_hh_l0 grad norm: 0.04019690304994583\n",
            "fc.weight grad norm: 0.4575834572315216\n",
            "fc.bias grad norm: 0.1698712259531021\n",
            "[Batch 0] Loss: 0.0714\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.025102131068706512\n",
            "c0 grad norm: 0.02167366072535515\n",
            "conv1.weight grad norm: 0.2837062180042267\n",
            "conv1.bias grad norm: 0.08571556210517883\n",
            "norm.weight grad norm: 0.03235341235995293\n",
            "norm.bias grad norm: 0.043593768030405045\n",
            "lstm.weight_ih_l0 grad norm: 0.17919906973838806\n",
            "lstm.weight_hh_l0 grad norm: 0.04647500440478325\n",
            "lstm.bias_ih_l0 grad norm: 0.017605431377887726\n",
            "lstm.bias_hh_l0 grad norm: 0.017605431377887726\n",
            "fc.weight grad norm: 0.25672343373298645\n",
            "fc.bias grad norm: 0.05831768736243248\n",
            "[Batch 1000] Loss: 0.0199\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.039042238146066666\n",
            "c0 grad norm: 0.07484517991542816\n",
            "conv1.weight grad norm: 0.5105186700820923\n",
            "conv1.bias grad norm: 0.08373568207025528\n",
            "norm.weight grad norm: 0.0739627555012703\n",
            "norm.bias grad norm: 0.07320147007703781\n",
            "lstm.weight_ih_l0 grad norm: 0.33561381697654724\n",
            "lstm.weight_hh_l0 grad norm: 0.08380400389432907\n",
            "lstm.bias_ih_l0 grad norm: 0.03110761009156704\n",
            "lstm.bias_hh_l0 grad norm: 0.03110761009156704\n",
            "fc.weight grad norm: 0.45107024908065796\n",
            "fc.bias grad norm: 0.084865503013134\n",
            "[Batch 2000] Loss: 0.0818\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.020119817927479744\n",
            "c0 grad norm: 0.06654201447963715\n",
            "conv1.weight grad norm: 0.28948190808296204\n",
            "conv1.bias grad norm: 0.05842360481619835\n",
            "norm.weight grad norm: 0.0594370998442173\n",
            "norm.bias grad norm: 0.06494611501693726\n",
            "lstm.weight_ih_l0 grad norm: 0.24256840348243713\n",
            "lstm.weight_hh_l0 grad norm: 0.06674060970544815\n",
            "lstm.bias_ih_l0 grad norm: 0.023649374023079872\n",
            "lstm.bias_hh_l0 grad norm: 0.023649374023079872\n",
            "fc.weight grad norm: 0.3887750804424286\n",
            "fc.bias grad norm: 0.09644390642642975\n",
            "[Batch 3000] Loss: 0.0384\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.025087425485253334\n",
            "c0 grad norm: 0.014116020873188972\n",
            "conv1.weight grad norm: 0.35428547859191895\n",
            "conv1.bias grad norm: 0.06396950036287308\n",
            "norm.weight grad norm: 0.04504283145070076\n",
            "norm.bias grad norm: 0.051920171827077866\n",
            "lstm.weight_ih_l0 grad norm: 0.23065249621868134\n",
            "lstm.weight_hh_l0 grad norm: 0.052568528801202774\n",
            "lstm.bias_ih_l0 grad norm: 0.02202271670103073\n",
            "lstm.bias_hh_l0 grad norm: 0.02202271670103073\n",
            "fc.weight grad norm: 0.439846932888031\n",
            "fc.bias grad norm: 0.18005375564098358\n",
            "[Batch 4000] Loss: 0.0594\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.023337241262197495\n",
            "c0 grad norm: 0.03866122290492058\n",
            "conv1.weight grad norm: 0.28539952635765076\n",
            "conv1.bias grad norm: 0.07334647327661514\n",
            "norm.weight grad norm: 0.03955705091357231\n",
            "norm.bias grad norm: 0.04857845604419708\n",
            "lstm.weight_ih_l0 grad norm: 0.24091434478759766\n",
            "lstm.weight_hh_l0 grad norm: 0.07503373175859451\n",
            "lstm.bias_ih_l0 grad norm: 0.023531930521130562\n",
            "lstm.bias_hh_l0 grad norm: 0.023531930521130562\n",
            "fc.weight grad norm: 0.7082864046096802\n",
            "fc.bias grad norm: 0.24912337958812714\n",
            "[Batch 5000] Loss: 0.1154\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.13000699877738953\n",
            "c0 grad norm: 0.09106160700321198\n",
            "conv1.weight grad norm: 0.8518403768539429\n",
            "conv1.bias grad norm: 0.2542927861213684\n",
            "norm.weight grad norm: 0.1435651183128357\n",
            "norm.bias grad norm: 0.18370667099952698\n",
            "lstm.weight_ih_l0 grad norm: 1.0654680728912354\n",
            "lstm.weight_hh_l0 grad norm: 0.32151660323143005\n",
            "lstm.bias_ih_l0 grad norm: 0.10467851161956787\n",
            "lstm.bias_hh_l0 grad norm: 0.10467851161956787\n",
            "fc.weight grad norm: 0.8978911638259888\n",
            "fc.bias grad norm: 0.2787174880504608\n",
            "[Batch 6000] Loss: 0.1818\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.011711910367012024\n",
            "c0 grad norm: 0.023542091250419617\n",
            "conv1.weight grad norm: 0.1618557572364807\n",
            "conv1.bias grad norm: 0.04872564971446991\n",
            "norm.weight grad norm: 0.018092064186930656\n",
            "norm.bias grad norm: 0.02458690106868744\n",
            "lstm.weight_ih_l0 grad norm: 0.14714576303958893\n",
            "lstm.weight_hh_l0 grad norm: 0.046064820140600204\n",
            "lstm.bias_ih_l0 grad norm: 0.013848762959241867\n",
            "lstm.bias_hh_l0 grad norm: 0.013848762959241867\n",
            "fc.weight grad norm: 0.1736973077058792\n",
            "fc.bias grad norm: 0.05083761736750603\n",
            "[Batch 7000] Loss: 0.0107\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.015480034984648228\n",
            "c0 grad norm: 0.034615837037563324\n",
            "conv1.weight grad norm: 0.15996554493904114\n",
            "conv1.bias grad norm: 0.04364559054374695\n",
            "norm.weight grad norm: 0.02748652547597885\n",
            "norm.bias grad norm: 0.029601506888866425\n",
            "lstm.weight_ih_l0 grad norm: 0.19001050293445587\n",
            "lstm.weight_hh_l0 grad norm: 0.04341763257980347\n",
            "lstm.bias_ih_l0 grad norm: 0.01910264976322651\n",
            "lstm.bias_hh_l0 grad norm: 0.01910264976322651\n",
            "fc.weight grad norm: 0.3310186266899109\n",
            "fc.bias grad norm: 0.11089056730270386\n",
            "[Batch 8000] Loss: 0.0408\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.02000115066766739\n",
            "c0 grad norm: 0.042027994990348816\n",
            "conv1.weight grad norm: 0.26805657148361206\n",
            "conv1.bias grad norm: 0.10197664797306061\n",
            "norm.weight grad norm: 0.04089845344424248\n",
            "norm.bias grad norm: 0.06028234586119652\n",
            "lstm.weight_ih_l0 grad norm: 0.2683413624763489\n",
            "lstm.weight_hh_l0 grad norm: 0.06166170537471771\n",
            "lstm.bias_ih_l0 grad norm: 0.02552065998315811\n",
            "lstm.bias_hh_l0 grad norm: 0.02552065998315811\n",
            "fc.weight grad norm: 0.2674224078655243\n",
            "fc.bias grad norm: 0.10699445754289627\n",
            "[Batch 9000] Loss: 0.0517\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 93\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.0056724888272583485\n",
            "c0 grad norm: 0.011863542720675468\n",
            "conv1.weight grad norm: 0.09399238973855972\n",
            "conv1.bias grad norm: 0.01555332075804472\n",
            "norm.weight grad norm: 0.009636202827095985\n",
            "norm.bias grad norm: 0.009958300739526749\n",
            "lstm.weight_ih_l0 grad norm: 0.07408452779054642\n",
            "lstm.weight_hh_l0 grad norm: 0.02448682114481926\n",
            "lstm.bias_ih_l0 grad norm: 0.006843371782451868\n",
            "lstm.bias_hh_l0 grad norm: 0.006843371782451868\n",
            "fc.weight grad norm: 0.16105225682258606\n",
            "fc.bias grad norm: 0.03420436009764671\n",
            "[Batch 0] Loss: 0.0118\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.008109345100820065\n",
            "c0 grad norm: 0.025094708427786827\n",
            "conv1.weight grad norm: 0.17599354684352875\n",
            "conv1.bias grad norm: 0.045615196228027344\n",
            "norm.weight grad norm: 0.02156180329620838\n",
            "norm.bias grad norm: 0.029083408415317535\n",
            "lstm.weight_ih_l0 grad norm: 0.17077969014644623\n",
            "lstm.weight_hh_l0 grad norm: 0.05210534855723381\n",
            "lstm.bias_ih_l0 grad norm: 0.01695561595261097\n",
            "lstm.bias_hh_l0 grad norm: 0.01695561595261097\n",
            "fc.weight grad norm: 0.2085789144039154\n",
            "fc.bias grad norm: 0.057384900748729706\n",
            "[Batch 1000] Loss: 0.0108\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.04201490059494972\n",
            "c0 grad norm: 0.06313662976026535\n",
            "conv1.weight grad norm: 0.3989499807357788\n",
            "conv1.bias grad norm: 0.09083294123411179\n",
            "norm.weight grad norm: 0.044906310737133026\n",
            "norm.bias grad norm: 0.05471740663051605\n",
            "lstm.weight_ih_l0 grad norm: 0.2662363648414612\n",
            "lstm.weight_hh_l0 grad norm: 0.05595158785581589\n",
            "lstm.bias_ih_l0 grad norm: 0.028513632714748383\n",
            "lstm.bias_hh_l0 grad norm: 0.028513632714748383\n",
            "fc.weight grad norm: 0.31369078159332275\n",
            "fc.bias grad norm: 0.15836429595947266\n",
            "[Batch 2000] Loss: 0.0410\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.07178246229887009\n",
            "c0 grad norm: 0.12987808883190155\n",
            "conv1.weight grad norm: 0.49936389923095703\n",
            "conv1.bias grad norm: 0.10846442729234695\n",
            "norm.weight grad norm: 0.06354845315217972\n",
            "norm.bias grad norm: 0.08279180526733398\n",
            "lstm.weight_ih_l0 grad norm: 0.477135568857193\n",
            "lstm.weight_hh_l0 grad norm: 0.14038920402526855\n",
            "lstm.bias_ih_l0 grad norm: 0.0464663952589035\n",
            "lstm.bias_hh_l0 grad norm: 0.0464663952589035\n",
            "fc.weight grad norm: 0.5287843942642212\n",
            "fc.bias grad norm: 0.2998606562614441\n",
            "[Batch 3000] Loss: 0.1290\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.02705545350909233\n",
            "c0 grad norm: 0.05765891820192337\n",
            "conv1.weight grad norm: 0.44596099853515625\n",
            "conv1.bias grad norm: 0.051007311791181564\n",
            "norm.weight grad norm: 0.04448675364255905\n",
            "norm.bias grad norm: 0.0502135343849659\n",
            "lstm.weight_ih_l0 grad norm: 0.2582041621208191\n",
            "lstm.weight_hh_l0 grad norm: 0.07231863588094711\n",
            "lstm.bias_ih_l0 grad norm: 0.024361547082662582\n",
            "lstm.bias_hh_l0 grad norm: 0.024361547082662582\n",
            "fc.weight grad norm: 0.3819788098335266\n",
            "fc.bias grad norm: 0.07625936716794968\n",
            "[Batch 4000] Loss: 0.0759\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.020303962752223015\n",
            "c0 grad norm: 0.041240397840738297\n",
            "conv1.weight grad norm: 0.18304459750652313\n",
            "conv1.bias grad norm: 0.050233807414770126\n",
            "norm.weight grad norm: 0.026233257725834846\n",
            "norm.bias grad norm: 0.03465065360069275\n",
            "lstm.weight_ih_l0 grad norm: 0.14153581857681274\n",
            "lstm.weight_hh_l0 grad norm: 0.04039554297924042\n",
            "lstm.bias_ih_l0 grad norm: 0.013493501581251621\n",
            "lstm.bias_hh_l0 grad norm: 0.013493501581251621\n",
            "fc.weight grad norm: 0.21576984226703644\n",
            "fc.bias grad norm: 0.06176319345831871\n",
            "[Batch 5000] Loss: 0.0198\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.048381008207798004\n",
            "c0 grad norm: 0.02581854723393917\n",
            "conv1.weight grad norm: 0.4204798638820648\n",
            "conv1.bias grad norm: 0.12542173266410828\n",
            "norm.weight grad norm: 0.06512659788131714\n",
            "norm.bias grad norm: 0.08072724938392639\n",
            "lstm.weight_ih_l0 grad norm: 0.35359635949134827\n",
            "lstm.weight_hh_l0 grad norm: 0.08226130902767181\n",
            "lstm.bias_ih_l0 grad norm: 0.0355200357735157\n",
            "lstm.bias_hh_l0 grad norm: 0.0355200357735157\n",
            "fc.weight grad norm: 0.5012447834014893\n",
            "fc.bias grad norm: 0.15822787582874298\n",
            "[Batch 6000] Loss: 0.0977\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.028820212930440903\n",
            "c0 grad norm: 0.017605602741241455\n",
            "conv1.weight grad norm: 0.2493419349193573\n",
            "conv1.bias grad norm: 0.029981616884469986\n",
            "norm.weight grad norm: 0.0327712744474411\n",
            "norm.bias grad norm: 0.03611335903406143\n",
            "lstm.weight_ih_l0 grad norm: 0.19263291358947754\n",
            "lstm.weight_hh_l0 grad norm: 0.04584797844290733\n",
            "lstm.bias_ih_l0 grad norm: 0.018964285030961037\n",
            "lstm.bias_hh_l0 grad norm: 0.018964285030961037\n",
            "fc.weight grad norm: 0.2459101676940918\n",
            "fc.bias grad norm: 0.06749570369720459\n",
            "[Batch 7000] Loss: 0.0215\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.019356289878487587\n",
            "c0 grad norm: 0.018676239997148514\n",
            "conv1.weight grad norm: 0.30920788645744324\n",
            "conv1.bias grad norm: 0.08858446776866913\n",
            "norm.weight grad norm: 0.04410741105675697\n",
            "norm.bias grad norm: 0.04961519315838814\n",
            "lstm.weight_ih_l0 grad norm: 0.2666710913181305\n",
            "lstm.weight_hh_l0 grad norm: 0.08356571942567825\n",
            "lstm.bias_ih_l0 grad norm: 0.025714676827192307\n",
            "lstm.bias_hh_l0 grad norm: 0.025714676827192307\n",
            "fc.weight grad norm: 0.19419480860233307\n",
            "fc.bias grad norm: 0.06746654212474823\n",
            "[Batch 8000] Loss: 0.0302\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.01620846427977085\n",
            "c0 grad norm: 0.022252090275287628\n",
            "conv1.weight grad norm: 0.268730103969574\n",
            "conv1.bias grad norm: 0.06927367299795151\n",
            "norm.weight grad norm: 0.04153520613908768\n",
            "norm.bias grad norm: 0.04775603488087654\n",
            "lstm.weight_ih_l0 grad norm: 0.22102512419223785\n",
            "lstm.weight_hh_l0 grad norm: 0.057631511241197586\n",
            "lstm.bias_ih_l0 grad norm: 0.021812548860907555\n",
            "lstm.bias_hh_l0 grad norm: 0.021812548860907555\n",
            "fc.weight grad norm: 0.25933656096458435\n",
            "fc.bias grad norm: 0.08258714526891708\n",
            "[Batch 9000] Loss: 0.0616\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 94\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.017597736790776253\n",
            "c0 grad norm: 0.025701778009533882\n",
            "conv1.weight grad norm: 0.207718163728714\n",
            "conv1.bias grad norm: 0.028273338451981544\n",
            "norm.weight grad norm: 0.018945559859275818\n",
            "norm.bias grad norm: 0.020478276535868645\n",
            "lstm.weight_ih_l0 grad norm: 0.12133538722991943\n",
            "lstm.weight_hh_l0 grad norm: 0.02850361168384552\n",
            "lstm.bias_ih_l0 grad norm: 0.011921721510589123\n",
            "lstm.bias_hh_l0 grad norm: 0.011921721510589123\n",
            "fc.weight grad norm: 0.11719373613595963\n",
            "fc.bias grad norm: 0.029088543727993965\n",
            "[Batch 0] Loss: 0.0109\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.030930470675230026\n",
            "c0 grad norm: 0.08291027694940567\n",
            "conv1.weight grad norm: 0.47162288427352905\n",
            "conv1.bias grad norm: 0.08337046205997467\n",
            "norm.weight grad norm: 0.055786363780498505\n",
            "norm.bias grad norm: 0.06733166426420212\n",
            "lstm.weight_ih_l0 grad norm: 0.33787575364112854\n",
            "lstm.weight_hh_l0 grad norm: 0.0813734233379364\n",
            "lstm.bias_ih_l0 grad norm: 0.032646212726831436\n",
            "lstm.bias_hh_l0 grad norm: 0.032646212726831436\n",
            "fc.weight grad norm: 0.5131157636642456\n",
            "fc.bias grad norm: 0.24999473989009857\n",
            "[Batch 1000] Loss: 0.1501\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.020601728931069374\n",
            "c0 grad norm: 0.07646529376506805\n",
            "conv1.weight grad norm: 0.4092748761177063\n",
            "conv1.bias grad norm: 0.07509404420852661\n",
            "norm.weight grad norm: 0.04517447203397751\n",
            "norm.bias grad norm: 0.055005669593811035\n",
            "lstm.weight_ih_l0 grad norm: 0.21572238206863403\n",
            "lstm.weight_hh_l0 grad norm: 0.058974165469408035\n",
            "lstm.bias_ih_l0 grad norm: 0.021047137677669525\n",
            "lstm.bias_hh_l0 grad norm: 0.021047137677669525\n",
            "fc.weight grad norm: 0.47277355194091797\n",
            "fc.bias grad norm: 0.22075067460536957\n",
            "[Batch 2000] Loss: 0.0989\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.021171744912862778\n",
            "c0 grad norm: 0.04730306938290596\n",
            "conv1.weight grad norm: 0.24351754784584045\n",
            "conv1.bias grad norm: 0.05220580846071243\n",
            "norm.weight grad norm: 0.02845909260213375\n",
            "norm.bias grad norm: 0.03535713627934456\n",
            "lstm.weight_ih_l0 grad norm: 0.17815563082695007\n",
            "lstm.weight_hh_l0 grad norm: 0.04647866636514664\n",
            "lstm.bias_ih_l0 grad norm: 0.017313215881586075\n",
            "lstm.bias_hh_l0 grad norm: 0.017313215881586075\n",
            "fc.weight grad norm: 0.35673439502716064\n",
            "fc.bias grad norm: 0.02435166761279106\n",
            "[Batch 3000] Loss: 0.0384\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.015058120712637901\n",
            "c0 grad norm: 0.035420577973127365\n",
            "conv1.weight grad norm: 0.23893655836582184\n",
            "conv1.bias grad norm: 0.05974119156599045\n",
            "norm.weight grad norm: 0.03308454528450966\n",
            "norm.bias grad norm: 0.038234859704971313\n",
            "lstm.weight_ih_l0 grad norm: 0.17804808914661407\n",
            "lstm.weight_hh_l0 grad norm: 0.048007670789957047\n",
            "lstm.bias_ih_l0 grad norm: 0.016238462179899216\n",
            "lstm.bias_hh_l0 grad norm: 0.016238462179899216\n",
            "fc.weight grad norm: 0.09251145273447037\n",
            "fc.bias grad norm: 0.021545255556702614\n",
            "[Batch 4000] Loss: 0.0250\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.02250819094479084\n",
            "c0 grad norm: 0.022533725947141647\n",
            "conv1.weight grad norm: 0.3797297179698944\n",
            "conv1.bias grad norm: 0.08877736330032349\n",
            "norm.weight grad norm: 0.038384970277547836\n",
            "norm.bias grad norm: 0.057513002306222916\n",
            "lstm.weight_ih_l0 grad norm: 0.22071810066699982\n",
            "lstm.weight_hh_l0 grad norm: 0.061660733073949814\n",
            "lstm.bias_ih_l0 grad norm: 0.021729949861764908\n",
            "lstm.bias_hh_l0 grad norm: 0.021729949861764908\n",
            "fc.weight grad norm: 0.4143032133579254\n",
            "fc.bias grad norm: 0.17338110506534576\n",
            "[Batch 5000] Loss: 0.0923\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.05358389392495155\n",
            "c0 grad norm: 0.06827545911073685\n",
            "conv1.weight grad norm: 0.7434644103050232\n",
            "conv1.bias grad norm: 0.18411074578762054\n",
            "norm.weight grad norm: 0.09539411962032318\n",
            "norm.bias grad norm: 0.09120236337184906\n",
            "lstm.weight_ih_l0 grad norm: 0.5534573793411255\n",
            "lstm.weight_hh_l0 grad norm: 0.08350318670272827\n",
            "lstm.bias_ih_l0 grad norm: 0.0507931262254715\n",
            "lstm.bias_hh_l0 grad norm: 0.0507931262254715\n",
            "fc.weight grad norm: 0.2880347967147827\n",
            "fc.bias grad norm: 0.0754459798336029\n",
            "[Batch 6000] Loss: 0.0809\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.02760123275220394\n",
            "c0 grad norm: 0.03302605077624321\n",
            "conv1.weight grad norm: 0.25424104928970337\n",
            "conv1.bias grad norm: 0.05030985176563263\n",
            "norm.weight grad norm: 0.03336247056722641\n",
            "norm.bias grad norm: 0.02801809087395668\n",
            "lstm.weight_ih_l0 grad norm: 0.1941819041967392\n",
            "lstm.weight_hh_l0 grad norm: 0.055922649800777435\n",
            "lstm.bias_ih_l0 grad norm: 0.018820833414793015\n",
            "lstm.bias_hh_l0 grad norm: 0.018820833414793015\n",
            "fc.weight grad norm: 0.1579311639070511\n",
            "fc.bias grad norm: 0.042381953448057175\n",
            "[Batch 7000] Loss: 0.0248\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.030470963567495346\n",
            "c0 grad norm: 0.03202074021100998\n",
            "conv1.weight grad norm: 0.28495416045188904\n",
            "conv1.bias grad norm: 0.037216123193502426\n",
            "norm.weight grad norm: 0.03687668219208717\n",
            "norm.bias grad norm: 0.042220134288072586\n",
            "lstm.weight_ih_l0 grad norm: 0.19681064784526825\n",
            "lstm.weight_hh_l0 grad norm: 0.05394343286752701\n",
            "lstm.bias_ih_l0 grad norm: 0.017082378268241882\n",
            "lstm.bias_hh_l0 grad norm: 0.017082378268241882\n",
            "fc.weight grad norm: 0.2103087306022644\n",
            "fc.bias grad norm: 0.0861625000834465\n",
            "[Batch 8000] Loss: 0.0259\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.036625586450099945\n",
            "c0 grad norm: 0.04118538275361061\n",
            "conv1.weight grad norm: 0.34814000129699707\n",
            "conv1.bias grad norm: 0.09940189868211746\n",
            "norm.weight grad norm: 0.04825077950954437\n",
            "norm.bias grad norm: 0.07923753559589386\n",
            "lstm.weight_ih_l0 grad norm: 0.3189041018486023\n",
            "lstm.weight_hh_l0 grad norm: 0.081112340092659\n",
            "lstm.bias_ih_l0 grad norm: 0.03049257956445217\n",
            "lstm.bias_hh_l0 grad norm: 0.03049257956445217\n",
            "fc.weight grad norm: 0.3490416705608368\n",
            "fc.bias grad norm: 0.14339736104011536\n",
            "[Batch 9000] Loss: 0.0608\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 95\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.02069271169602871\n",
            "c0 grad norm: 0.020879533141851425\n",
            "conv1.weight grad norm: 0.2069638967514038\n",
            "conv1.bias grad norm: 0.05322191119194031\n",
            "norm.weight grad norm: 0.02486134134232998\n",
            "norm.bias grad norm: 0.030177365988492966\n",
            "lstm.weight_ih_l0 grad norm: 0.1353524774312973\n",
            "lstm.weight_hh_l0 grad norm: 0.03134428337216377\n",
            "lstm.bias_ih_l0 grad norm: 0.013563771732151508\n",
            "lstm.bias_hh_l0 grad norm: 0.013563771732151508\n",
            "fc.weight grad norm: 0.28597313165664673\n",
            "fc.bias grad norm: 0.08670837432146072\n",
            "[Batch 0] Loss: 0.0246\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.0660656988620758\n",
            "c0 grad norm: 0.054029595106840134\n",
            "conv1.weight grad norm: 0.6764205694198608\n",
            "conv1.bias grad norm: 0.16250725090503693\n",
            "norm.weight grad norm: 0.07812023162841797\n",
            "norm.bias grad norm: 0.09794247895479202\n",
            "lstm.weight_ih_l0 grad norm: 0.446991503238678\n",
            "lstm.weight_hh_l0 grad norm: 0.09636235237121582\n",
            "lstm.bias_ih_l0 grad norm: 0.04177907854318619\n",
            "lstm.bias_hh_l0 grad norm: 0.04177907854318619\n",
            "fc.weight grad norm: 0.17688295245170593\n",
            "fc.bias grad norm: 0.0763845443725586\n",
            "[Batch 1000] Loss: 0.0245\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.025254923850297928\n",
            "c0 grad norm: 0.04513172805309296\n",
            "conv1.weight grad norm: 0.4632880985736847\n",
            "conv1.bias grad norm: 0.06340151280164719\n",
            "norm.weight grad norm: 0.058116670697927475\n",
            "norm.bias grad norm: 0.060841552913188934\n",
            "lstm.weight_ih_l0 grad norm: 0.32300806045532227\n",
            "lstm.weight_hh_l0 grad norm: 0.09960504621267319\n",
            "lstm.bias_ih_l0 grad norm: 0.031172487884759903\n",
            "lstm.bias_hh_l0 grad norm: 0.031172487884759903\n",
            "fc.weight grad norm: 0.32255810499191284\n",
            "fc.bias grad norm: 0.11687139421701431\n",
            "[Batch 2000] Loss: 0.0408\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.023373328149318695\n",
            "c0 grad norm: 0.04371345788240433\n",
            "conv1.weight grad norm: 0.3073367774486542\n",
            "conv1.bias grad norm: 0.051720425486564636\n",
            "norm.weight grad norm: 0.039535727351903915\n",
            "norm.bias grad norm: 0.04955260083079338\n",
            "lstm.weight_ih_l0 grad norm: 0.21680808067321777\n",
            "lstm.weight_hh_l0 grad norm: 0.0713881403207779\n",
            "lstm.bias_ih_l0 grad norm: 0.02048083208501339\n",
            "lstm.bias_hh_l0 grad norm: 0.02048083208501339\n",
            "fc.weight grad norm: 0.31461265683174133\n",
            "fc.bias grad norm: 0.10891201347112656\n",
            "[Batch 3000] Loss: 0.0405\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.11414185166358948\n",
            "c0 grad norm: 0.241912841796875\n",
            "conv1.weight grad norm: 1.1316450834274292\n",
            "conv1.bias grad norm: 0.23233510553836823\n",
            "norm.weight grad norm: 0.14361023902893066\n",
            "norm.bias grad norm: 0.16961346566677094\n",
            "lstm.weight_ih_l0 grad norm: 1.2065701484680176\n",
            "lstm.weight_hh_l0 grad norm: 0.3130110502243042\n",
            "lstm.bias_ih_l0 grad norm: 0.11972113698720932\n",
            "lstm.bias_hh_l0 grad norm: 0.11972113698720932\n",
            "fc.weight grad norm: 0.9688118100166321\n",
            "fc.bias grad norm: 0.20858658850193024\n",
            "[Batch 4000] Loss: 0.4214\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.055604301393032074\n",
            "c0 grad norm: 0.05297952890396118\n",
            "conv1.weight grad norm: 0.7474613189697266\n",
            "conv1.bias grad norm: 0.18683123588562012\n",
            "norm.weight grad norm: 0.10402064770460129\n",
            "norm.bias grad norm: 0.1158730685710907\n",
            "lstm.weight_ih_l0 grad norm: 0.5403656363487244\n",
            "lstm.weight_hh_l0 grad norm: 0.1763666719198227\n",
            "lstm.bias_ih_l0 grad norm: 0.04912664741277695\n",
            "lstm.bias_hh_l0 grad norm: 0.04912664741277695\n",
            "fc.weight grad norm: 0.3215823471546173\n",
            "fc.bias grad norm: 0.03944416716694832\n",
            "[Batch 5000] Loss: 0.1049\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.01930522359907627\n",
            "c0 grad norm: 0.026457229629158974\n",
            "conv1.weight grad norm: 0.2432011365890503\n",
            "conv1.bias grad norm: 0.04745420441031456\n",
            "norm.weight grad norm: 0.03708559647202492\n",
            "norm.bias grad norm: 0.0377708300948143\n",
            "lstm.weight_ih_l0 grad norm: 0.18058578670024872\n",
            "lstm.weight_hh_l0 grad norm: 0.047189000993967056\n",
            "lstm.bias_ih_l0 grad norm: 0.01699790731072426\n",
            "lstm.bias_hh_l0 grad norm: 0.01699790731072426\n",
            "fc.weight grad norm: 0.35519614815711975\n",
            "fc.bias grad norm: 0.09253525733947754\n",
            "[Batch 6000] Loss: 0.0581\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.040415991097688675\n",
            "c0 grad norm: 0.0831766128540039\n",
            "conv1.weight grad norm: 0.5771448016166687\n",
            "conv1.bias grad norm: 0.13723653554916382\n",
            "norm.weight grad norm: 0.08995640277862549\n",
            "norm.bias grad norm: 0.10200037807226181\n",
            "lstm.weight_ih_l0 grad norm: 0.462655633687973\n",
            "lstm.weight_hh_l0 grad norm: 0.1478564441204071\n",
            "lstm.bias_ih_l0 grad norm: 0.043074507266283035\n",
            "lstm.bias_hh_l0 grad norm: 0.043074507266283035\n",
            "fc.weight grad norm: 0.37968599796295166\n",
            "fc.bias grad norm: 0.059595461934804916\n",
            "[Batch 7000] Loss: 0.1075\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.027914760634303093\n",
            "c0 grad norm: 0.06329165399074554\n",
            "conv1.weight grad norm: 0.3550155758857727\n",
            "conv1.bias grad norm: 0.0716407299041748\n",
            "norm.weight grad norm: 0.047400783747434616\n",
            "norm.bias grad norm: 0.05476671829819679\n",
            "lstm.weight_ih_l0 grad norm: 0.2876691222190857\n",
            "lstm.weight_hh_l0 grad norm: 0.0840989202260971\n",
            "lstm.bias_ih_l0 grad norm: 0.027805114164948463\n",
            "lstm.bias_hh_l0 grad norm: 0.027805114164948463\n",
            "fc.weight grad norm: 0.3409045338630676\n",
            "fc.bias grad norm: 0.030189819633960724\n",
            "[Batch 8000] Loss: 0.0798\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.03349203243851662\n",
            "c0 grad norm: 0.0609135702252388\n",
            "conv1.weight grad norm: 0.4194367229938507\n",
            "conv1.bias grad norm: 0.09171386063098907\n",
            "norm.weight grad norm: 0.05950940400362015\n",
            "norm.bias grad norm: 0.06423141062259674\n",
            "lstm.weight_ih_l0 grad norm: 0.2523370683193207\n",
            "lstm.weight_hh_l0 grad norm: 0.06291519105434418\n",
            "lstm.bias_ih_l0 grad norm: 0.02544422820210457\n",
            "lstm.bias_hh_l0 grad norm: 0.02544422820210457\n",
            "fc.weight grad norm: 0.2803516983985901\n",
            "fc.bias grad norm: 0.1008446216583252\n",
            "[Batch 9000] Loss: 0.0261\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 96\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.008219297043979168\n",
            "c0 grad norm: 0.013246187940239906\n",
            "conv1.weight grad norm: 0.059668466448783875\n",
            "conv1.bias grad norm: 0.013933343812823296\n",
            "norm.weight grad norm: 0.009457290172576904\n",
            "norm.bias grad norm: 0.008481280878186226\n",
            "lstm.weight_ih_l0 grad norm: 0.060735784471035004\n",
            "lstm.weight_hh_l0 grad norm: 0.018946047872304916\n",
            "lstm.bias_ih_l0 grad norm: 0.005890649743378162\n",
            "lstm.bias_hh_l0 grad norm: 0.005890649743378162\n",
            "fc.weight grad norm: 0.12885823845863342\n",
            "fc.bias grad norm: 0.05138285085558891\n",
            "[Batch 0] Loss: 0.0047\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.013787394389510155\n",
            "c0 grad norm: 0.03340910002589226\n",
            "conv1.weight grad norm: 0.1902780830860138\n",
            "conv1.bias grad norm: 0.05537257343530655\n",
            "norm.weight grad norm: 0.029293859377503395\n",
            "norm.bias grad norm: 0.03262900933623314\n",
            "lstm.weight_ih_l0 grad norm: 0.1708907037973404\n",
            "lstm.weight_hh_l0 grad norm: 0.047661192715168\n",
            "lstm.bias_ih_l0 grad norm: 0.016157664358615875\n",
            "lstm.bias_hh_l0 grad norm: 0.016157664358615875\n",
            "fc.weight grad norm: 0.1786523163318634\n",
            "fc.bias grad norm: 0.013963805511593819\n",
            "[Batch 1000] Loss: 0.0194\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.03217102959752083\n",
            "c0 grad norm: 0.043152254074811935\n",
            "conv1.weight grad norm: 0.2045176774263382\n",
            "conv1.bias grad norm: 0.03867635130882263\n",
            "norm.weight grad norm: 0.030876679345965385\n",
            "norm.bias grad norm: 0.034271933138370514\n",
            "lstm.weight_ih_l0 grad norm: 0.2726095914840698\n",
            "lstm.weight_hh_l0 grad norm: 0.04819218069314957\n",
            "lstm.bias_ih_l0 grad norm: 0.026984570547938347\n",
            "lstm.bias_hh_l0 grad norm: 0.026984570547938347\n",
            "fc.weight grad norm: 0.15029171109199524\n",
            "fc.bias grad norm: 0.03692298382520676\n",
            "[Batch 2000] Loss: 0.0317\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.02650727517902851\n",
            "c0 grad norm: 0.0672602504491806\n",
            "conv1.weight grad norm: 0.32164761424064636\n",
            "conv1.bias grad norm: 0.0672638788819313\n",
            "norm.weight grad norm: 0.03781428560614586\n",
            "norm.bias grad norm: 0.03825664147734642\n",
            "lstm.weight_ih_l0 grad norm: 0.2048686146736145\n",
            "lstm.weight_hh_l0 grad norm: 0.04995053634047508\n",
            "lstm.bias_ih_l0 grad norm: 0.01995037868618965\n",
            "lstm.bias_hh_l0 grad norm: 0.01995037868618965\n",
            "fc.weight grad norm: 0.18450917303562164\n",
            "fc.bias grad norm: 0.07253862917423248\n",
            "[Batch 3000] Loss: 0.0232\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.05359267815947533\n",
            "c0 grad norm: 0.11039254814386368\n",
            "conv1.weight grad norm: 0.6568626761436462\n",
            "conv1.bias grad norm: 0.2095392495393753\n",
            "norm.weight grad norm: 0.09905125200748444\n",
            "norm.bias grad norm: 0.12596142292022705\n",
            "lstm.weight_ih_l0 grad norm: 0.5916032195091248\n",
            "lstm.weight_hh_l0 grad norm: 0.18296493589878082\n",
            "lstm.bias_ih_l0 grad norm: 0.05424393340945244\n",
            "lstm.bias_hh_l0 grad norm: 0.05424393340945244\n",
            "fc.weight grad norm: 0.797672688961029\n",
            "fc.bias grad norm: 0.22779783606529236\n",
            "[Batch 4000] Loss: 0.2557\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.011463083326816559\n",
            "c0 grad norm: 0.0169359240680933\n",
            "conv1.weight grad norm: 0.16321083903312683\n",
            "conv1.bias grad norm: 0.029812533408403397\n",
            "norm.weight grad norm: 0.025524910539388657\n",
            "norm.bias grad norm: 0.02085738442838192\n",
            "lstm.weight_ih_l0 grad norm: 0.1413678377866745\n",
            "lstm.weight_hh_l0 grad norm: 0.03464788943529129\n",
            "lstm.bias_ih_l0 grad norm: 0.012833785265684128\n",
            "lstm.bias_hh_l0 grad norm: 0.012833785265684128\n",
            "fc.weight grad norm: 0.13094177842140198\n",
            "fc.bias grad norm: 0.030611703172326088\n",
            "[Batch 5000] Loss: 0.0237\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.013184807263314724\n",
            "c0 grad norm: 0.023864317685365677\n",
            "conv1.weight grad norm: 0.12044774740934372\n",
            "conv1.bias grad norm: 0.020528404042124748\n",
            "norm.weight grad norm: 0.016072921454906464\n",
            "norm.bias grad norm: 0.016344865784049034\n",
            "lstm.weight_ih_l0 grad norm: 0.07686680555343628\n",
            "lstm.weight_hh_l0 grad norm: 0.02148006111383438\n",
            "lstm.bias_ih_l0 grad norm: 0.007507420144975185\n",
            "lstm.bias_hh_l0 grad norm: 0.007507420144975185\n",
            "fc.weight grad norm: 0.16996169090270996\n",
            "fc.bias grad norm: 0.0596732459962368\n",
            "[Batch 6000] Loss: 0.0168\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.023488657549023628\n",
            "c0 grad norm: 0.02482137270271778\n",
            "conv1.weight grad norm: 0.2705790400505066\n",
            "conv1.bias grad norm: 0.05576612055301666\n",
            "norm.weight grad norm: 0.03256776183843613\n",
            "norm.bias grad norm: 0.04302292689681053\n",
            "lstm.weight_ih_l0 grad norm: 0.20071671903133392\n",
            "lstm.weight_hh_l0 grad norm: 0.05649517476558685\n",
            "lstm.bias_ih_l0 grad norm: 0.0197183508425951\n",
            "lstm.bias_hh_l0 grad norm: 0.0197183508425951\n",
            "fc.weight grad norm: 0.3150838017463684\n",
            "fc.bias grad norm: 0.07267805933952332\n",
            "[Batch 7000] Loss: 0.0411\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.030740680173039436\n",
            "c0 grad norm: 0.02639973722398281\n",
            "conv1.weight grad norm: 0.3370843529701233\n",
            "conv1.bias grad norm: 0.09264352917671204\n",
            "norm.weight grad norm: 0.043796107172966\n",
            "norm.bias grad norm: 0.050536442548036575\n",
            "lstm.weight_ih_l0 grad norm: 0.2546389400959015\n",
            "lstm.weight_hh_l0 grad norm: 0.07307953387498856\n",
            "lstm.bias_ih_l0 grad norm: 0.02588154934346676\n",
            "lstm.bias_hh_l0 grad norm: 0.02588154934346676\n",
            "fc.weight grad norm: 0.35518187284469604\n",
            "fc.bias grad norm: 0.2016058713197708\n",
            "[Batch 8000] Loss: 0.0487\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.07678385078907013\n",
            "c0 grad norm: 0.07304075360298157\n",
            "conv1.weight grad norm: 1.0276381969451904\n",
            "conv1.bias grad norm: 0.25472667813301086\n",
            "norm.weight grad norm: 0.09981638193130493\n",
            "norm.bias grad norm: 0.1139468103647232\n",
            "lstm.weight_ih_l0 grad norm: 0.41695094108581543\n",
            "lstm.weight_hh_l0 grad norm: 0.08632386475801468\n",
            "lstm.bias_ih_l0 grad norm: 0.04025621712207794\n",
            "lstm.bias_hh_l0 grad norm: 0.04025621712207794\n",
            "fc.weight grad norm: 0.14649616181850433\n",
            "fc.bias grad norm: 0.03098933771252632\n",
            "[Batch 9000] Loss: 0.0557\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 97\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.06960192322731018\n",
            "c0 grad norm: 0.10918457061052322\n",
            "conv1.weight grad norm: 1.3682771921157837\n",
            "conv1.bias grad norm: 0.22872668504714966\n",
            "norm.weight grad norm: 0.16920660436153412\n",
            "norm.bias grad norm: 0.18493089079856873\n",
            "lstm.weight_ih_l0 grad norm: 0.9475163817405701\n",
            "lstm.weight_hh_l0 grad norm: 0.28684186935424805\n",
            "lstm.bias_ih_l0 grad norm: 0.08849667757749557\n",
            "lstm.bias_hh_l0 grad norm: 0.08849667757749557\n",
            "fc.weight grad norm: 0.5866430997848511\n",
            "fc.bias grad norm: 0.22132112085819244\n",
            "[Batch 0] Loss: 0.4300\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.05131646990776062\n",
            "c0 grad norm: 0.06645707041025162\n",
            "conv1.weight grad norm: 0.715914785861969\n",
            "conv1.bias grad norm: 0.19706471264362335\n",
            "norm.weight grad norm: 0.07891588658094406\n",
            "norm.bias grad norm: 0.1050243228673935\n",
            "lstm.weight_ih_l0 grad norm: 0.48996588587760925\n",
            "lstm.weight_hh_l0 grad norm: 0.14211015403270721\n",
            "lstm.bias_ih_l0 grad norm: 0.04898707941174507\n",
            "lstm.bias_hh_l0 grad norm: 0.04898707941174507\n",
            "fc.weight grad norm: 0.4707929790019989\n",
            "fc.bias grad norm: 0.16382847726345062\n",
            "[Batch 1000] Loss: 0.0731\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.02892410382628441\n",
            "c0 grad norm: 0.026333868503570557\n",
            "conv1.weight grad norm: 0.28434452414512634\n",
            "conv1.bias grad norm: 0.07664285600185394\n",
            "norm.weight grad norm: 0.042750582098960876\n",
            "norm.bias grad norm: 0.05827571824193001\n",
            "lstm.weight_ih_l0 grad norm: 0.20709392428398132\n",
            "lstm.weight_hh_l0 grad norm: 0.04294119402766228\n",
            "lstm.bias_ih_l0 grad norm: 0.020838286727666855\n",
            "lstm.bias_hh_l0 grad norm: 0.020838286727666855\n",
            "fc.weight grad norm: 0.3229537904262543\n",
            "fc.bias grad norm: 0.1630493849515915\n",
            "[Batch 2000] Loss: 0.0453\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.021557800471782684\n",
            "c0 grad norm: 0.0260933730751276\n",
            "conv1.weight grad norm: 0.2865018844604492\n",
            "conv1.bias grad norm: 0.06508695334196091\n",
            "norm.weight grad norm: 0.046264875680208206\n",
            "norm.bias grad norm: 0.05910049006342888\n",
            "lstm.weight_ih_l0 grad norm: 0.2162298709154129\n",
            "lstm.weight_hh_l0 grad norm: 0.061956435441970825\n",
            "lstm.bias_ih_l0 grad norm: 0.0209653340280056\n",
            "lstm.bias_hh_l0 grad norm: 0.0209653340280056\n",
            "fc.weight grad norm: 0.3849445879459381\n",
            "fc.bias grad norm: 0.05775994807481766\n",
            "[Batch 3000] Loss: 0.0687\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.04524269700050354\n",
            "c0 grad norm: 0.03331270441412926\n",
            "conv1.weight grad norm: 0.6282041668891907\n",
            "conv1.bias grad norm: 0.11656409502029419\n",
            "norm.weight grad norm: 0.07827796787023544\n",
            "norm.bias grad norm: 0.10183343291282654\n",
            "lstm.weight_ih_l0 grad norm: 0.3227415680885315\n",
            "lstm.weight_hh_l0 grad norm: 0.05733900144696236\n",
            "lstm.bias_ih_l0 grad norm: 0.031014271080493927\n",
            "lstm.bias_hh_l0 grad norm: 0.031014271080493927\n",
            "fc.weight grad norm: 0.21222230792045593\n",
            "fc.bias grad norm: 0.10845973342657089\n",
            "[Batch 4000] Loss: 0.0286\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.0322999581694603\n",
            "c0 grad norm: 0.03447229787707329\n",
            "conv1.weight grad norm: 0.3122301697731018\n",
            "conv1.bias grad norm: 0.07534082233905792\n",
            "norm.weight grad norm: 0.03657401725649834\n",
            "norm.bias grad norm: 0.04537731781601906\n",
            "lstm.weight_ih_l0 grad norm: 0.2720867693424225\n",
            "lstm.weight_hh_l0 grad norm: 0.05866587907075882\n",
            "lstm.bias_ih_l0 grad norm: 0.027092723175883293\n",
            "lstm.bias_hh_l0 grad norm: 0.027092723175883293\n",
            "fc.weight grad norm: 0.2928928732872009\n",
            "fc.bias grad norm: 0.1281827986240387\n",
            "[Batch 5000] Loss: 0.0476\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.02091895043849945\n",
            "c0 grad norm: 0.034617096185684204\n",
            "conv1.weight grad norm: 0.1763906180858612\n",
            "conv1.bias grad norm: 0.03990279510617256\n",
            "norm.weight grad norm: 0.028668439015746117\n",
            "norm.bias grad norm: 0.02506590262055397\n",
            "lstm.weight_ih_l0 grad norm: 0.14183248579502106\n",
            "lstm.weight_hh_l0 grad norm: 0.03646322339773178\n",
            "lstm.bias_ih_l0 grad norm: 0.013663996011018753\n",
            "lstm.bias_hh_l0 grad norm: 0.013663996011018753\n",
            "fc.weight grad norm: 0.17768573760986328\n",
            "fc.bias grad norm: 0.06886304169893265\n",
            "[Batch 6000] Loss: 0.0143\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.03277183696627617\n",
            "c0 grad norm: 0.07418854534626007\n",
            "conv1.weight grad norm: 0.33353328704833984\n",
            "conv1.bias grad norm: 0.09152254462242126\n",
            "norm.weight grad norm: 0.04453765228390694\n",
            "norm.bias grad norm: 0.06894528865814209\n",
            "lstm.weight_ih_l0 grad norm: 0.24180229008197784\n",
            "lstm.weight_hh_l0 grad norm: 0.06274014711380005\n",
            "lstm.bias_ih_l0 grad norm: 0.022294240072369576\n",
            "lstm.bias_hh_l0 grad norm: 0.022294240072369576\n",
            "fc.weight grad norm: 0.25919920206069946\n",
            "fc.bias grad norm: 0.12765884399414062\n",
            "[Batch 7000] Loss: 0.0351\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.0068515269085764885\n",
            "c0 grad norm: 0.017379943281412125\n",
            "conv1.weight grad norm: 0.10940004140138626\n",
            "conv1.bias grad norm: 0.026576338335871696\n",
            "norm.weight grad norm: 0.01242179237306118\n",
            "norm.bias grad norm: 0.014865626581013203\n",
            "lstm.weight_ih_l0 grad norm: 0.08128631860017776\n",
            "lstm.weight_hh_l0 grad norm: 0.02268088050186634\n",
            "lstm.bias_ih_l0 grad norm: 0.00752511341124773\n",
            "lstm.bias_hh_l0 grad norm: 0.00752511341124773\n",
            "fc.weight grad norm: 0.23815780878067017\n",
            "fc.bias grad norm: 0.06731362640857697\n",
            "[Batch 8000] Loss: 0.0378\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.032875873148441315\n",
            "c0 grad norm: 0.04364665225148201\n",
            "conv1.weight grad norm: 0.3592849373817444\n",
            "conv1.bias grad norm: 0.10483336448669434\n",
            "norm.weight grad norm: 0.04676731675863266\n",
            "norm.bias grad norm: 0.049880314618349075\n",
            "lstm.weight_ih_l0 grad norm: 0.28069502115249634\n",
            "lstm.weight_hh_l0 grad norm: 0.07649297267198563\n",
            "lstm.bias_ih_l0 grad norm: 0.027808483690023422\n",
            "lstm.bias_hh_l0 grad norm: 0.027808483690023422\n",
            "fc.weight grad norm: 0.23959116637706757\n",
            "fc.bias grad norm: 0.0883316695690155\n",
            "[Batch 9000] Loss: 0.0296\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 98\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.05485455319285393\n",
            "c0 grad norm: 0.0472077839076519\n",
            "conv1.weight grad norm: 0.6042565107345581\n",
            "conv1.bias grad norm: 0.07736220955848694\n",
            "norm.weight grad norm: 0.07698903232812881\n",
            "norm.bias grad norm: 0.07918573915958405\n",
            "lstm.weight_ih_l0 grad norm: 0.3106442093849182\n",
            "lstm.weight_hh_l0 grad norm: 0.05729646980762482\n",
            "lstm.bias_ih_l0 grad norm: 0.029580622911453247\n",
            "lstm.bias_hh_l0 grad norm: 0.029580622911453247\n",
            "fc.weight grad norm: 0.25209662318229675\n",
            "fc.bias grad norm: 0.11346638202667236\n",
            "[Batch 0] Loss: 0.0386\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.11369690299034119\n",
            "c0 grad norm: 0.07488325238227844\n",
            "conv1.weight grad norm: 1.3913476467132568\n",
            "conv1.bias grad norm: 0.23880095779895782\n",
            "norm.weight grad norm: 0.2020646184682846\n",
            "norm.bias grad norm: 0.17234700918197632\n",
            "lstm.weight_ih_l0 grad norm: 1.0157928466796875\n",
            "lstm.weight_hh_l0 grad norm: 0.18790358304977417\n",
            "lstm.bias_ih_l0 grad norm: 0.09972445666790009\n",
            "lstm.bias_hh_l0 grad norm: 0.09972445666790009\n",
            "fc.weight grad norm: 0.3648993670940399\n",
            "fc.bias grad norm: 0.17132222652435303\n",
            "[Batch 1000] Loss: 0.1184\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.020276237279176712\n",
            "c0 grad norm: 0.018316278234124184\n",
            "conv1.weight grad norm: 0.24349555373191833\n",
            "conv1.bias grad norm: 0.04272906854748726\n",
            "norm.weight grad norm: 0.025121033191680908\n",
            "norm.bias grad norm: 0.03309198096394539\n",
            "lstm.weight_ih_l0 grad norm: 0.16261231899261475\n",
            "lstm.weight_hh_l0 grad norm: 0.035210516303777695\n",
            "lstm.bias_ih_l0 grad norm: 0.01509078685194254\n",
            "lstm.bias_hh_l0 grad norm: 0.01509078685194254\n",
            "fc.weight grad norm: 0.0855209231376648\n",
            "fc.bias grad norm: 0.026027526706457138\n",
            "[Batch 2000] Loss: 0.0036\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.08578209578990936\n",
            "c0 grad norm: 0.04836760461330414\n",
            "conv1.weight grad norm: 0.6865535378456116\n",
            "conv1.bias grad norm: 0.18827690184116364\n",
            "norm.weight grad norm: 0.10458362102508545\n",
            "norm.bias grad norm: 0.11401806771755219\n",
            "lstm.weight_ih_l0 grad norm: 0.5936391353607178\n",
            "lstm.weight_hh_l0 grad norm: 0.12185521423816681\n",
            "lstm.bias_ih_l0 grad norm: 0.05674457550048828\n",
            "lstm.bias_hh_l0 grad norm: 0.05674457550048828\n",
            "fc.weight grad norm: 0.36969810724258423\n",
            "fc.bias grad norm: 0.1330975592136383\n",
            "[Batch 3000] Loss: 0.0493\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.022064702585339546\n",
            "c0 grad norm: 0.02406122535467148\n",
            "conv1.weight grad norm: 0.21246463060379028\n",
            "conv1.bias grad norm: 0.061266787350177765\n",
            "norm.weight grad norm: 0.03628548979759216\n",
            "norm.bias grad norm: 0.046709317713975906\n",
            "lstm.weight_ih_l0 grad norm: 0.18354426324367523\n",
            "lstm.weight_hh_l0 grad norm: 0.04522606357932091\n",
            "lstm.bias_ih_l0 grad norm: 0.018191814422607422\n",
            "lstm.bias_hh_l0 grad norm: 0.018191814422607422\n",
            "fc.weight grad norm: 0.2640722692012787\n",
            "fc.bias grad norm: 0.07458008825778961\n",
            "[Batch 4000] Loss: 0.0224\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.025958998128771782\n",
            "c0 grad norm: 0.0429411418735981\n",
            "conv1.weight grad norm: 0.6197366118431091\n",
            "conv1.bias grad norm: 0.0836273580789566\n",
            "norm.weight grad norm: 0.05810200423002243\n",
            "norm.bias grad norm: 0.06637884676456451\n",
            "lstm.weight_ih_l0 grad norm: 0.35023966431617737\n",
            "lstm.weight_hh_l0 grad norm: 0.11127910017967224\n",
            "lstm.bias_ih_l0 grad norm: 0.03313172608613968\n",
            "lstm.bias_hh_l0 grad norm: 0.03313172608613968\n",
            "fc.weight grad norm: 0.30573731660842896\n",
            "fc.bias grad norm: 0.05008877441287041\n",
            "[Batch 5000] Loss: 0.0500\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.01935482583940029\n",
            "c0 grad norm: 0.03292913734912872\n",
            "conv1.weight grad norm: 0.20265711843967438\n",
            "conv1.bias grad norm: 0.028385132551193237\n",
            "norm.weight grad norm: 0.03155623376369476\n",
            "norm.bias grad norm: 0.031975533813238144\n",
            "lstm.weight_ih_l0 grad norm: 0.16522568464279175\n",
            "lstm.weight_hh_l0 grad norm: 0.05130051076412201\n",
            "lstm.bias_ih_l0 grad norm: 0.015374286100268364\n",
            "lstm.bias_hh_l0 grad norm: 0.015374286100268364\n",
            "fc.weight grad norm: 0.24202467501163483\n",
            "fc.bias grad norm: 0.06705580651760101\n",
            "[Batch 6000] Loss: 0.0338\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.024083612486720085\n",
            "c0 grad norm: 0.03201054781675339\n",
            "conv1.weight grad norm: 0.25653669238090515\n",
            "conv1.bias grad norm: 0.06662464886903763\n",
            "norm.weight grad norm: 0.03008842095732689\n",
            "norm.bias grad norm: 0.04262486845254898\n",
            "lstm.weight_ih_l0 grad norm: 0.1932942420244217\n",
            "lstm.weight_hh_l0 grad norm: 0.061559226363897324\n",
            "lstm.bias_ih_l0 grad norm: 0.01950562559068203\n",
            "lstm.bias_hh_l0 grad norm: 0.01950562559068203\n",
            "fc.weight grad norm: 0.3428831100463867\n",
            "fc.bias grad norm: 0.09333223849534988\n",
            "[Batch 7000] Loss: 0.0143\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.004608131479471922\n",
            "c0 grad norm: 0.006889856420457363\n",
            "conv1.weight grad norm: 0.08039315789937973\n",
            "conv1.bias grad norm: 0.02356308326125145\n",
            "norm.weight grad norm: 0.009680480696260929\n",
            "norm.bias grad norm: 0.010983610525727272\n",
            "lstm.weight_ih_l0 grad norm: 0.049542397260665894\n",
            "lstm.weight_hh_l0 grad norm: 0.015107468701899052\n",
            "lstm.bias_ih_l0 grad norm: 0.004974548239260912\n",
            "lstm.bias_hh_l0 grad norm: 0.004974548239260912\n",
            "fc.weight grad norm: 0.12190642207860947\n",
            "fc.bias grad norm: 0.06059813126921654\n",
            "[Batch 8000] Loss: 0.0058\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.087240070104599\n",
            "c0 grad norm: 0.11036139726638794\n",
            "conv1.weight grad norm: 1.1872104406356812\n",
            "conv1.bias grad norm: 0.2396792769432068\n",
            "norm.weight grad norm: 0.10514828562736511\n",
            "norm.bias grad norm: 0.10046808421611786\n",
            "lstm.weight_ih_l0 grad norm: 0.45836055278778076\n",
            "lstm.weight_hh_l0 grad norm: 0.10538537800312042\n",
            "lstm.bias_ih_l0 grad norm: 0.04914936050772667\n",
            "lstm.bias_hh_l0 grad norm: 0.04914936050772667\n",
            "fc.weight grad norm: 0.4128902852535248\n",
            "fc.bias grad norm: 0.15109916031360626\n",
            "[Batch 9000] Loss: 0.0784\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 99\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.014544556848704815\n",
            "c0 grad norm: 0.017231682315468788\n",
            "conv1.weight grad norm: 0.10474037379026413\n",
            "conv1.bias grad norm: 0.03142734244465828\n",
            "norm.weight grad norm: 0.017750557512044907\n",
            "norm.bias grad norm: 0.019349947571754456\n",
            "lstm.weight_ih_l0 grad norm: 0.09233922511339188\n",
            "lstm.weight_hh_l0 grad norm: 0.022058065980672836\n",
            "lstm.bias_ih_l0 grad norm: 0.008894267491996288\n",
            "lstm.bias_hh_l0 grad norm: 0.008894267491996288\n",
            "fc.weight grad norm: 0.1257336437702179\n",
            "fc.bias grad norm: 0.06297526508569717\n",
            "[Batch 0] Loss: 0.0071\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.008570149540901184\n",
            "c0 grad norm: 0.020982438698410988\n",
            "conv1.weight grad norm: 0.11304448544979095\n",
            "conv1.bias grad norm: 0.024833127856254578\n",
            "norm.weight grad norm: 0.01761244796216488\n",
            "norm.bias grad norm: 0.02081415429711342\n",
            "lstm.weight_ih_l0 grad norm: 0.09259378910064697\n",
            "lstm.weight_hh_l0 grad norm: 0.02238648012280464\n",
            "lstm.bias_ih_l0 grad norm: 0.00895484909415245\n",
            "lstm.bias_hh_l0 grad norm: 0.00895484909415245\n",
            "fc.weight grad norm: 0.185122549533844\n",
            "fc.bias grad norm: 0.08077175170183182\n",
            "[Batch 1000] Loss: 0.0132\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.036106277257204056\n",
            "c0 grad norm: 0.04074832424521446\n",
            "conv1.weight grad norm: 0.362513929605484\n",
            "conv1.bias grad norm: 0.06066000089049339\n",
            "norm.weight grad norm: 0.04367538169026375\n",
            "norm.bias grad norm: 0.05401155725121498\n",
            "lstm.weight_ih_l0 grad norm: 0.28842785954475403\n",
            "lstm.weight_hh_l0 grad norm: 0.08258530497550964\n",
            "lstm.bias_ih_l0 grad norm: 0.0280362106859684\n",
            "lstm.bias_hh_l0 grad norm: 0.0280362106859684\n",
            "fc.weight grad norm: 0.3026430904865265\n",
            "fc.bias grad norm: 0.039837803691625595\n",
            "[Batch 2000] Loss: 0.0629\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.03309761360287666\n",
            "c0 grad norm: 0.06682945787906647\n",
            "conv1.weight grad norm: 0.4792216122150421\n",
            "conv1.bias grad norm: 0.11092417687177658\n",
            "norm.weight grad norm: 0.05655496567487717\n",
            "norm.bias grad norm: 0.07881774008274078\n",
            "lstm.weight_ih_l0 grad norm: 0.3050045371055603\n",
            "lstm.weight_hh_l0 grad norm: 0.07040607929229736\n",
            "lstm.bias_ih_l0 grad norm: 0.029456134885549545\n",
            "lstm.bias_hh_l0 grad norm: 0.029456134885549545\n",
            "fc.weight grad norm: 0.4242438077926636\n",
            "fc.bias grad norm: 0.07492003589868546\n",
            "[Batch 3000] Loss: 0.0447\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.07815000414848328\n",
            "c0 grad norm: 0.07923796772956848\n",
            "conv1.weight grad norm: 0.742996335029602\n",
            "conv1.bias grad norm: 0.11409415304660797\n",
            "norm.weight grad norm: 0.09256208688020706\n",
            "norm.bias grad norm: 0.07922597229480743\n",
            "lstm.weight_ih_l0 grad norm: 0.5643577575683594\n",
            "lstm.weight_hh_l0 grad norm: 0.15768708288669586\n",
            "lstm.bias_ih_l0 grad norm: 0.039125967770814896\n",
            "lstm.bias_hh_l0 grad norm: 0.039125967770814896\n",
            "fc.weight grad norm: 0.31413257122039795\n",
            "fc.bias grad norm: 0.05899932608008385\n",
            "[Batch 4000] Loss: 0.0527\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.018324431031942368\n",
            "c0 grad norm: 0.02175668254494667\n",
            "conv1.weight grad norm: 0.2133249044418335\n",
            "conv1.bias grad norm: 0.020770853385329247\n",
            "norm.weight grad norm: 0.027688035741448402\n",
            "norm.bias grad norm: 0.019001111388206482\n",
            "lstm.weight_ih_l0 grad norm: 0.1580965369939804\n",
            "lstm.weight_hh_l0 grad norm: 0.0356721468269825\n",
            "lstm.bias_ih_l0 grad norm: 0.015001870691776276\n",
            "lstm.bias_hh_l0 grad norm: 0.015001870691776276\n",
            "fc.weight grad norm: 0.21999673545360565\n",
            "fc.bias grad norm: 0.0567564032971859\n",
            "[Batch 5000] Loss: 0.0303\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.021668704226613045\n",
            "c0 grad norm: 0.04136519879102707\n",
            "conv1.weight grad norm: 0.30776721239089966\n",
            "conv1.bias grad norm: 0.07716154307126999\n",
            "norm.weight grad norm: 0.03698958456516266\n",
            "norm.bias grad norm: 0.0464143231511116\n",
            "lstm.weight_ih_l0 grad norm: 0.21305912733078003\n",
            "lstm.weight_hh_l0 grad norm: 0.06686609983444214\n",
            "lstm.bias_ih_l0 grad norm: 0.02050524391233921\n",
            "lstm.bias_hh_l0 grad norm: 0.02050524391233921\n",
            "fc.weight grad norm: 0.18202733993530273\n",
            "fc.bias grad norm: 0.06382305175065994\n",
            "[Batch 6000] Loss: 0.0195\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.03651432693004608\n",
            "c0 grad norm: 0.021237105131149292\n",
            "conv1.weight grad norm: 0.22664764523506165\n",
            "conv1.bias grad norm: 0.035775378346443176\n",
            "norm.weight grad norm: 0.02750604785978794\n",
            "norm.bias grad norm: 0.031126119196414948\n",
            "lstm.weight_ih_l0 grad norm: 0.17740808427333832\n",
            "lstm.weight_hh_l0 grad norm: 0.03940987586975098\n",
            "lstm.bias_ih_l0 grad norm: 0.015166266821324825\n",
            "lstm.bias_hh_l0 grad norm: 0.015166266821324825\n",
            "fc.weight grad norm: 0.11186183989048004\n",
            "fc.bias grad norm: 0.033688660711050034\n",
            "[Batch 7000] Loss: 0.0452\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.018331242725253105\n",
            "c0 grad norm: 0.03214075416326523\n",
            "conv1.weight grad norm: 0.24878890812397003\n",
            "conv1.bias grad norm: 0.051236413419246674\n",
            "norm.weight grad norm: 0.04657936468720436\n",
            "norm.bias grad norm: 0.04573250934481621\n",
            "lstm.weight_ih_l0 grad norm: 0.32012638449668884\n",
            "lstm.weight_hh_l0 grad norm: 0.1252329796552658\n",
            "lstm.bias_ih_l0 grad norm: 0.03325508162379265\n",
            "lstm.bias_hh_l0 grad norm: 0.03325508162379265\n",
            "fc.weight grad norm: 0.4072543680667877\n",
            "fc.bias grad norm: 0.17415624856948853\n",
            "[Batch 8000] Loss: 0.0587\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.04523376375436783\n",
            "c0 grad norm: 0.13816720247268677\n",
            "conv1.weight grad norm: 0.8199415802955627\n",
            "conv1.bias grad norm: 0.25458770990371704\n",
            "norm.weight grad norm: 0.10184690356254578\n",
            "norm.bias grad norm: 0.14550074934959412\n",
            "lstm.weight_ih_l0 grad norm: 0.8855596780776978\n",
            "lstm.weight_hh_l0 grad norm: 0.28856736421585083\n",
            "lstm.bias_ih_l0 grad norm: 0.08487089723348618\n",
            "lstm.bias_hh_l0 grad norm: 0.08487089723348618\n",
            "fc.weight grad norm: 0.9113646149635315\n",
            "fc.bias grad norm: 0.23289147019386292\n",
            "[Batch 9000] Loss: 0.2921\n",
            "Total Epoch Loss: 642.5318\n",
            "\n",
            "Epoch 100\n",
            "\n",
            "[Batch 0] Gradient Norms:\n",
            "h0 grad norm: 0.01330588199198246\n",
            "c0 grad norm: 0.02411024458706379\n",
            "conv1.weight grad norm: 0.2206958532333374\n",
            "conv1.bias grad norm: 0.04285672679543495\n",
            "norm.weight grad norm: 0.04063860699534416\n",
            "norm.bias grad norm: 0.03732872009277344\n",
            "lstm.weight_ih_l0 grad norm: 0.17115551233291626\n",
            "lstm.weight_hh_l0 grad norm: 0.050902921706438065\n",
            "lstm.bias_ih_l0 grad norm: 0.01727503165602684\n",
            "lstm.bias_hh_l0 grad norm: 0.01727503165602684\n",
            "fc.weight grad norm: 0.19473183155059814\n",
            "fc.bias grad norm: 0.04535295441746712\n",
            "[Batch 0] Loss: 0.0341\n",
            "\n",
            "[Batch 1000] Gradient Norms:\n",
            "h0 grad norm: 0.020764486864209175\n",
            "c0 grad norm: 0.028858136385679245\n",
            "conv1.weight grad norm: 0.291041761636734\n",
            "conv1.bias grad norm: 0.07374970614910126\n",
            "norm.weight grad norm: 0.03224870562553406\n",
            "norm.bias grad norm: 0.03497486189007759\n",
            "lstm.weight_ih_l0 grad norm: 0.15684452652931213\n",
            "lstm.weight_hh_l0 grad norm: 0.05184648558497429\n",
            "lstm.bias_ih_l0 grad norm: 0.015577987767755985\n",
            "lstm.bias_hh_l0 grad norm: 0.015577987767755985\n",
            "fc.weight grad norm: 0.2542625367641449\n",
            "fc.bias grad norm: 0.07465065270662308\n",
            "[Batch 1000] Loss: 0.0148\n",
            "\n",
            "[Batch 2000] Gradient Norms:\n",
            "h0 grad norm: 0.024881230667233467\n",
            "c0 grad norm: 0.08151722699403763\n",
            "conv1.weight grad norm: 0.6262962818145752\n",
            "conv1.bias grad norm: 0.09243753552436829\n",
            "norm.weight grad norm: 0.07781834900379181\n",
            "norm.bias grad norm: 0.08458542823791504\n",
            "lstm.weight_ih_l0 grad norm: 0.432218998670578\n",
            "lstm.weight_hh_l0 grad norm: 0.1331140547990799\n",
            "lstm.bias_ih_l0 grad norm: 0.04134921357035637\n",
            "lstm.bias_hh_l0 grad norm: 0.04134921357035637\n",
            "fc.weight grad norm: 0.3721372187137604\n",
            "fc.bias grad norm: 0.13769739866256714\n",
            "[Batch 2000] Loss: 0.0709\n",
            "\n",
            "[Batch 3000] Gradient Norms:\n",
            "h0 grad norm: 0.05557352676987648\n",
            "c0 grad norm: 0.04136497527360916\n",
            "conv1.weight grad norm: 0.642848551273346\n",
            "conv1.bias grad norm: 0.17652074992656708\n",
            "norm.weight grad norm: 0.0816023200750351\n",
            "norm.bias grad norm: 0.11023477464914322\n",
            "lstm.weight_ih_l0 grad norm: 0.4850519895553589\n",
            "lstm.weight_hh_l0 grad norm: 0.15202946960926056\n",
            "lstm.bias_ih_l0 grad norm: 0.04668070375919342\n",
            "lstm.bias_hh_l0 grad norm: 0.04668070375919342\n",
            "fc.weight grad norm: 0.32044947147369385\n",
            "fc.bias grad norm: 0.06974005699157715\n",
            "[Batch 3000] Loss: 0.0307\n",
            "\n",
            "[Batch 4000] Gradient Norms:\n",
            "h0 grad norm: 0.2015742063522339\n",
            "c0 grad norm: 0.0976591557264328\n",
            "conv1.weight grad norm: 1.2944270372390747\n",
            "conv1.bias grad norm: 0.2672134041786194\n",
            "norm.weight grad norm: 0.18833547830581665\n",
            "norm.bias grad norm: 0.23387014865875244\n",
            "lstm.weight_ih_l0 grad norm: 1.0304089784622192\n",
            "lstm.weight_hh_l0 grad norm: 0.1668936014175415\n",
            "lstm.bias_ih_l0 grad norm: 0.09819167107343674\n",
            "lstm.bias_hh_l0 grad norm: 0.09819167107343674\n",
            "fc.weight grad norm: 0.4452457129955292\n",
            "fc.bias grad norm: 0.1667543202638626\n",
            "[Batch 4000] Loss: 0.2055\n",
            "\n",
            "[Batch 5000] Gradient Norms:\n",
            "h0 grad norm: 0.04121638089418411\n",
            "c0 grad norm: 0.12437064945697784\n",
            "conv1.weight grad norm: 0.43601328134536743\n",
            "conv1.bias grad norm: 0.13748201727867126\n",
            "norm.weight grad norm: 0.08259172737598419\n",
            "norm.bias grad norm: 0.10720065981149673\n",
            "lstm.weight_ih_l0 grad norm: 0.5294590592384338\n",
            "lstm.weight_hh_l0 grad norm: 0.1771637499332428\n",
            "lstm.bias_ih_l0 grad norm: 0.0514911524951458\n",
            "lstm.bias_hh_l0 grad norm: 0.0514911524951458\n",
            "fc.weight grad norm: 0.6611384749412537\n",
            "fc.bias grad norm: 0.222708061337471\n",
            "[Batch 5000] Loss: 0.1732\n",
            "\n",
            "[Batch 6000] Gradient Norms:\n",
            "h0 grad norm: 0.053065937012434006\n",
            "c0 grad norm: 0.07333466410636902\n",
            "conv1.weight grad norm: 0.5748264789581299\n",
            "conv1.bias grad norm: 0.1312747597694397\n",
            "norm.weight grad norm: 0.08662812411785126\n",
            "norm.bias grad norm: 0.1128508523106575\n",
            "lstm.weight_ih_l0 grad norm: 0.7398495078086853\n",
            "lstm.weight_hh_l0 grad norm: 0.1907811313867569\n",
            "lstm.bias_ih_l0 grad norm: 0.07179044187068939\n",
            "lstm.bias_hh_l0 grad norm: 0.07179044187068939\n",
            "fc.weight grad norm: 0.5930286645889282\n",
            "fc.bias grad norm: 0.09072143584489822\n",
            "[Batch 6000] Loss: 0.0876\n",
            "\n",
            "[Batch 7000] Gradient Norms:\n",
            "h0 grad norm: 0.01858430914580822\n",
            "c0 grad norm: 0.0445295013487339\n",
            "conv1.weight grad norm: 0.3780060410499573\n",
            "conv1.bias grad norm: 0.06626560539007187\n",
            "norm.weight grad norm: 0.04674552381038666\n",
            "norm.bias grad norm: 0.04540000483393669\n",
            "lstm.weight_ih_l0 grad norm: 0.26992350816726685\n",
            "lstm.weight_hh_l0 grad norm: 0.0838516503572464\n",
            "lstm.bias_ih_l0 grad norm: 0.025474781170487404\n",
            "lstm.bias_hh_l0 grad norm: 0.025474781170487404\n",
            "fc.weight grad norm: 0.33939212560653687\n",
            "fc.bias grad norm: 0.09004093706607819\n",
            "[Batch 7000] Loss: 0.0412\n",
            "\n",
            "[Batch 8000] Gradient Norms:\n",
            "h0 grad norm: 0.022257452830672264\n",
            "c0 grad norm: 0.059981781989336014\n",
            "conv1.weight grad norm: 0.39013731479644775\n",
            "conv1.bias grad norm: 0.08667541295289993\n",
            "norm.weight grad norm: 0.06385350972414017\n",
            "norm.bias grad norm: 0.07810500264167786\n",
            "lstm.weight_ih_l0 grad norm: 0.2841000556945801\n",
            "lstm.weight_hh_l0 grad norm: 0.09409426152706146\n",
            "lstm.bias_ih_l0 grad norm: 0.027408966794610023\n",
            "lstm.bias_hh_l0 grad norm: 0.027408966794610023\n",
            "fc.weight grad norm: 0.42059969902038574\n",
            "fc.bias grad norm: 0.1415027230978012\n",
            "[Batch 8000] Loss: 0.0384\n",
            "\n",
            "[Batch 9000] Gradient Norms:\n",
            "h0 grad norm: 0.027730220928788185\n",
            "c0 grad norm: 0.045204222202301025\n",
            "conv1.weight grad norm: 0.3187878429889679\n",
            "conv1.bias grad norm: 0.07302352041006088\n",
            "norm.weight grad norm: 0.03667675703763962\n",
            "norm.bias grad norm: 0.06137480214238167\n",
            "lstm.weight_ih_l0 grad norm: 0.24638472497463226\n",
            "lstm.weight_hh_l0 grad norm: 0.06726552546024323\n",
            "lstm.bias_ih_l0 grad norm: 0.022670025005936623\n",
            "lstm.bias_hh_l0 grad norm: 0.022670025005936623\n",
            "fc.weight grad norm: 0.3480769693851471\n",
            "fc.bias grad norm: 0.08132081478834152\n",
            "[Batch 9000] Loss: 0.0523\n",
            "Total Epoch Loss: 642.5318\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation Loop**"
      ],
      "metadata": {
        "id": "KQZnmQwHKgtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "NHST5l_jKncM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        preds = model(batch_X)\n",
        "        all_preds.append(preds)\n",
        "        all_targets.append(batch_y)"
      ],
      "metadata": {
        "id": "SMC0mH6mKlXy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
        "all_targets = torch.cat(all_targets, dim=0).cpu().numpy()"
      ],
      "metadata": {
        "id": "LPJ5NNNkKsJj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(all_targets, all_preds)\n",
        "mae = mean_absolute_error(all_targets, all_preds)\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev97wbzbKxAE",
        "outputId": "e8e412cd-1f14-4a3a-c8bb-0c06ebbb3906"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.2819\n",
            "MAE: 0.3844\n",
            "R² Score: 0.7958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Model**"
      ],
      "metadata": {
        "id": "eV3uq9_aMCjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'convlstm.pth')"
      ],
      "metadata": {
        "id": "M_b_jbDWKy73"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}